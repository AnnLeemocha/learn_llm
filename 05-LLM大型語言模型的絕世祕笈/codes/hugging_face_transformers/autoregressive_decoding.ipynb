{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自迴歸解碼 \n",
    "\n",
    "描述模型不斷生成 Token 的過程。模型生成的 Token 會重新變成模型的輸入，模型在根據這些新生成的 Token 繼續往下生成，直到觸發結束條件為止。\n",
    "\n",
    "* 相對於非自迴歸 (Non-Autogressive, NAR) 模型而言，其差別在於自迴歸模型會把自己的輸出當成輸入，而 NAR 模型會一次把整個序列生出來。\n",
    "* 在 LLM 裡面，每次**推論 (Inference)** 只會生成一個 Token，必須要經過多次推論，才能完成完整的文本**生成 (Generation)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拆解 model.generate 的背後原理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 透過 `.forward()` 來進行推論：\n",
    "    * 使用 `model.forward(input_ids)` 與 `model(input_ids)` 同樣可以得到模型推論結果。\n",
    "    * 使用 `.forward()` 當範例，住要是強調該步驟為前向運算，另外這樣的回傳結果也會有比較明確的型別提示。\n",
    "    * 實際通常不使用 `.forward()` ，因為 PyTorch 中有一些錢處理與後處理的 Hooks ，如果使用它可能會忽略掉這些 Hoooks 導致某些環節處理不正常。\n",
    "* 產生一個類別為 `CausalLMOutputWithPast` 的輸出，裡面有兩個重要的資訊，分別為 `logits` 與 `past_key_values`。\n",
    "    * `logits` : \n",
    "        * 模型預測下個 Token 的機率表。\n",
    "        * 可以對 `logits` 做隨機取樣，或者直接計算 `argmax` 做取樣機最高的 Token 做 Greedy Decode。\n",
    "    * `past_key_values` :\n",
    "        * 俗稱**鍵值快取 (Key-Values Cache)**\n",
    "        * 是模型對**已知輸入**的運算結果。\n",
    "        * 因為自迴歸的特性，所以 Decoder LM 每次生成的輸出都會變成下個回合的輸入，因此這個 KV Cache 會越長越大，同時也是**消耗 GPU 記憶體的元兇之一**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss={'logits': tensor([[[  3.5840,   7.4258,  49.3125,  ...,  -4.4375,  -0.9116,   3.9941],\n",
      "         [-23.4688,  -5.3438, -15.2500,  ..., -17.4688, -14.7031, -22.7656],\n",
      "         [-21.8281,   3.9961,  -3.3242,  ..., -10.1094,  -6.3477, -21.7500],\n",
      "         ...,\n",
      "         [-19.7031,   1.8838, -21.7656,  ..., -17.5156, -16.1094, -19.0156],\n",
      "         [-31.8750,   4.8516, -44.9688,  ..., -28.1875, -24.3281, -31.0938],\n",
      "         [-34.7188,  -4.0625, -21.8125,  ..., -26.6719, -26.4219, -34.0938]]],\n",
      "       device='cuda:0', dtype=torch.float16), 'past_key_values': <transformers.cache_utils.DynamicCache object at 0x7ea4487c1450>}, logits=tensor([[[  3.5840,   7.4258,  49.3125,  ...,  -4.4375,  -0.9116,   3.9941],\n",
      "         [-23.4688,  -5.3438, -15.2500,  ..., -17.4688, -14.7031, -22.7656],\n",
      "         [-21.8281,   3.9961,  -3.3242,  ..., -10.1094,  -6.3477, -21.7500],\n",
      "         ...,\n",
      "         [-19.7031,   1.8838, -21.7656,  ..., -17.5156, -16.1094, -19.0156],\n",
      "         [-31.8750,   4.8516, -44.9688,  ..., -28.1875, -24.3281, -31.0938],\n",
      "         [-34.7188,  -4.0625, -21.8125,  ..., -26.6719, -26.4219, -34.0938]]],\n",
      "       device='cuda:0', dtype=torch.float16), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7ea4487c1450>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from transformers import TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelCls\n",
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "model_path = \"google/gemma-2b-it\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_path)\n",
    "\n",
    "prompt = \"[INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] \"\n",
    "\n",
    "tokens = tk(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokens[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(input_ids)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 有了這份 KV Cache，就不用每次推論時都把整個 `input_ids` 丟進去，只要留新長出來的 Token 就好，因為舊的輸入都已經被模型 Decode 成 KV Cache 了。\n",
    "* 下個回合的推論："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        outputs.logits.argmax(-1)[:, -1:],  # Greedy Decode\n",
    "        past_key_values=outputs.past_key_values,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將以上邏輯整理成一個迴圈來進行：\n",
    "* 這邊固定進行 16 次推論，中間如果遇到 EOS Token 就會跳出迴圈。\n",
    "* 拆解生成步驟的好處：\n",
    "    * 可以針對 `logits` 的機率分佈做觀察。\n",
    "    * 有些時候可以看到模型在「**說謊**」時，那個部份輸出的 Token 的機率分佈會特別的低，因此也可以用來計算模型的**信心度**之類的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "大型語言模型（LLM）是一種 AI 模型，它能夠像人類\n"
     ]
    }
   ],
   "source": [
    "pkv = None\n",
    "output_tokens = list()\n",
    "for i in range(16):\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(input_ids, past_key_values=pkv)\n",
    "    \n",
    "    next_token = outputs.logits.argmax(-1)[:, -1:] # 直接計算 `argmax` 做取樣機最高的 Token 做 Greedy Decode。\n",
    "    token_id = next_token.item()\n",
    "\n",
    "    if token_id == tk.eos_token_id:\n",
    "        break\n",
    "\n",
    "    output_tokens.append(token_id)\n",
    "    input_ids = next_token\n",
    "    pkv = outputs.past_key_values\n",
    "\n",
    "print(tk.decode(output_tokens))\n",
    "# 輸出結果：大型語言模型（Large Language Model，LLM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-36.7812, -15.7031, -31.0156,  ..., -26.9219, -23.9219, -36.5625]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "<transformers.cache_utils.DynamicCache object at 0x7ea43c3cd4d0>\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)\n",
    "print(outputs.past_key_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
