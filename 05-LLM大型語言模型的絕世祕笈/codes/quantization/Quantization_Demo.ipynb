{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Demo\n",
        "\n",
        "Reference: [ChatGPT](https://chat.openai.com/share/ed886a22-1480-45a5-ada7-e4f1272a2482)"
      ],
      "metadata": {
        "id": "faf4fey5Xzkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIykoKMBQoNJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Int8 Quantization"
      ],
      "metadata": {
        "id": "dII17Po7gcpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_fp16_to_int8(fp16_weights):\n",
        "    # 找到權重的絕對最大值\n",
        "    abs_max_val = np.max(np.abs(fp16_weights))\n",
        "\n",
        "    # 計算縮放係數 Scaling Factor\n",
        "    scale = 127 / abs_max_val\n",
        "\n",
        "    # 將 FP16 權重轉換為 INT8\n",
        "    weights = fp16_weights * scale\n",
        "    int8_weights = np.round(weights).astype(np.int8)\n",
        "\n",
        "    return int8_weights, scale\n",
        "\n",
        "\n",
        "def dequantize_int8_to_fp16(int8_weights, scale):\n",
        "    # 將 INT8 權重轉換回 FP16\n",
        "    return int8_weights.astype(np.float16) / scale"
      ],
      "metadata": {
        "id": "DXVNwzsme8YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設我們有一組 FP16 權重\n",
        "w_fp16 = np.array([1.0, -0.5, 0.25, -0.125], dtype=np.float16)\n",
        "print(f\"Original FP16 Weights: {w_fp16}\")\n",
        "\n",
        "# 量化 FP16 權重到 INT8\n",
        "w_int8, sf = quantize_fp16_to_int8(w_fp16)\n",
        "print(f\"Int8 Weights: {w_int8}\")\n",
        "print(f\"Quantization Constant: {sf}\")\n",
        "\n",
        "# 從 Int8 權重重建 FP16 權重\n",
        "rw_fp16 = dequantize_int8_to_fp16(w_int8, sf)\n",
        "print(f\"Reconstructed FP16 Weights: {rw_fp16}\")\n",
        "\n",
        "# 觀察輸出差異\n",
        "x = np.array([1.0, 0.25, -0.5, -1.0])\n",
        "y = np.matmul(x, w_fp16)\n",
        "qy = np.matmul(x, rw_fp16)\n",
        "loss = y - qy\n",
        "\n",
        "print(f\"Original Output: {y}\")\n",
        "print(f\"Quantized Output: {qy}\")\n",
        "print(f\"Output Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meeymsiRgXVx",
        "outputId": "6113db59-6861-4dc1-a675-73c1b3acf67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original FP16 Weights: [ 1.    -0.5    0.25  -0.125]\n",
            "Int8 Weights: [127 -64  32 -16]\n",
            "Quantization Constant: 127.0\n",
            "Reconstructed FP16 Weights: [ 1.    -0.504  0.252 -0.126]\n",
            "Original Output: 0.875\n",
            "Quantized Output: 0.8740234375\n",
            "Output Loss: 0.0009765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Int4 Quantization"
      ],
      "metadata": {
        "id": "SXW9PyEoghNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_fp16_to_int4(fp16_weights):\n",
        "    # 找到權重的絕對最大值\n",
        "    abs_max_val = np.max(np.abs(fp16_weights))\n",
        "\n",
        "    # 計算縮放係數 Scaling Factor\n",
        "    scale = 7 / abs_max_val\n",
        "\n",
        "    # 將 FP16 權重轉換為 INT4\n",
        "    weights = fp16_weights * scale\n",
        "    int4_weights = np.round(weights).astype(np.int8)\n",
        "\n",
        "    # 將數據限制在 INT4 範圍內 [-8, 7]\n",
        "    int4_weights = np.clip(int4_weights, -8, 7)\n",
        "\n",
        "    return int4_weights, scale\n",
        "\n",
        "\n",
        "def dequantize_int4_to_fp16(int4_weights, scale):\n",
        "    # 將 INT4 權重轉換回 FP16\n",
        "    return int4_weights.astype(np.float16) / scale"
      ],
      "metadata": {
        "id": "OoCTt_OkRw2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設我們有一組 FP16 權重\n",
        "w_fp16 = np.array([1.0, -0.5, 0.25, -0.125], dtype=np.float16)\n",
        "print(f\"Original FP16 Weights: {w_fp16}\")\n",
        "\n",
        "# 量化 FP16 權重到 INT4\n",
        "w_int4, sf = quantize_fp16_to_int4(w_fp16)\n",
        "print(f\"Int8 Weights: {w_int4}\")\n",
        "print(f\"Quantization Constant: {sf}\")\n",
        "\n",
        "# 將 INT4 權重反量化回 FP16\n",
        "rw_fp16 = dequantize_int4_to_fp16(w_int4, sf)\n",
        "print(f\"Reconstructed FP16 Weights: {rw_fp16}\")\n",
        "\n",
        "# 觀察輸出差異\n",
        "x = np.array([1.0, 0.25, -0.5, -1.0])\n",
        "y = np.matmul(x, w_fp16)\n",
        "qy = np.matmul(x, rw_fp16)\n",
        "loss = y - qy\n",
        "\n",
        "print(f\"Original Output: {y}\")\n",
        "print(f\"Quantized Output: {qy}\")\n",
        "print(f\"Output Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mMpaNJRgZ9_",
        "outputId": "a3a26bfe-3633-4a40-9df5-7dce829dca1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original FP16 Weights: [ 1.    -0.5    0.25  -0.125]\n",
            "Int8 Weights: [ 7 -4  2 -1]\n",
            "Quantization Constant: 7.0\n",
            "Reconstructed FP16 Weights: [ 1.     -0.5713  0.2856 -0.1428]\n",
            "Original Output: 0.875\n",
            "Quantized Output: 0.857177734375\n",
            "Output Loss: 0.017822265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 實際運算"
      ],
      "metadata": {
        "id": "InIT2D9Wgj17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化隨機數生成器的種子，以確保結果可重現\n",
        "np.random.seed(2135)\n",
        "\n",
        "# 設定輸入樣本數量、隱藏層大小與輸出類別數量\n",
        "inn_size = 12\n",
        "hid_size = 512\n",
        "out_size = 4096\n",
        "\n",
        "# 產生模型輸入、隱藏層權重與分類器權重\n",
        "x = np.random.randn(inn_size, hid_size).astype(np.float16)\n",
        "w_fp16 = np.random.randn(hid_size, hid_size).astype(np.float16)\n",
        "clf = np.random.randn(hid_size, out_size).astype(np.float16)\n",
        "\n",
        "hid = np.matmul(x, w_fp16)  # 計算隱藏層輸出\n",
        "out = np.matmul(hid, clf)   # 計算分類器輸出\n",
        "y = np.argmax(out, -1)      # 實際類別預測"
      ],
      "metadata": {
        "id": "RPFcAr4VcgTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 進行 INT8 量化並再次計算一次\n",
        "w_int8, sf = quantize_fp16_to_int8(w_fp16)\n",
        "rw_fp16 = dequantize_int8_to_fp16(w_int8, sf)\n",
        "hid = np.matmul(x, rw_fp16)\n",
        "out = np.matmul(hid, clf)\n",
        "y_int8 = np.argmax(out, -1)\n",
        "\n",
        "# 比較 INT8 量化的預測結果與原始輸出的差異\n",
        "errors_int8 = np.sum(np.not_equal(y, y_int8))\n",
        "\n",
        "print(f\"FP16 Prediction: {y}\")\n",
        "print(f\"INT8 Prediction: {y_int8}\")\n",
        "print(f\"INT8 Error: {errors_int8}, Results: {y == y_int8}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avCxfpTyettC",
        "outputId": "b4ad1779-b737-4b40-ffb2-b01fb87c5894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 Prediction: [3956  874  201  109  843   46 1419 1058  865 2894 2059 1386]\n",
            "INT8 Prediction: [3956  874  201  109  843   46 1419 1058 1495 2894 2059 1386]\n",
            "INT8 Error: 1, Results: [ True  True  True  True  True  True  True  True False  True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 進行 INT4 量化並再次計算一次\n",
        "w_int4, sf = quantize_fp16_to_int4(w_fp16)\n",
        "rw_fp16 = dequantize_int4_to_fp16(w_int4, sf)\n",
        "hid = np.matmul(x, rw_fp16)\n",
        "out = np.matmul(hid, clf)\n",
        "y_int4 = np.argmax(out, -1)\n",
        "\n",
        "# 比較 INT4 量化的預測結果與原始輸出的差異\n",
        "errors_int4 = np.sum(np.not_equal(y, y_int4))\n",
        "\n",
        "print(f\"FP16 Prediction: {y}\")\n",
        "print(f\"INT4 Prediction: {y_int4}\")\n",
        "print(f\"INT4 Error: {errors_int4}, Results: {y == y_int4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jozoCY0ev-c",
        "outputId": "8951152b-4141-4aec-b455-2219289604c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 Prediction: [3956  874  201  109  843   46 1419 1058  865 2894 2059 1386]\n",
            "INT4 Prediction: [3956 3782  242 1582  514 1716 2584 1058  865 2436 2059 1386]\n",
            "INT4 Error: 7, Results: [ True False False False False False False  True  True False  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 實驗結果\n",
        "\n",
        "INT8 測試 12 個樣本只錯了 1 個\n",
        "\n",
        "但 INT4 錯了 7 個，誤差很大！"
      ],
      "metadata": {
        "id": "_69U37rsy809"
      }
    }
  ]
}