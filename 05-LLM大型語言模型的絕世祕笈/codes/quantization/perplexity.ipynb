{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469ca788",
   "metadata": {},
   "source": [
    "### 困惑度、混淆度 (Perplexity, PPL)\n",
    "在量化的過程中，會相當重視模型的損失程度，也就是說量化完之後的誤差有多大，這在語言模型裡面通常以**困惑度 (Perplexity, PPL)** 來表示，PPL 是用來描述模型預測下一個字詞的時候其不確定性如何。\n",
    "* 因為語言模型每次推論時會產生一份機率表：\n",
    "    * 如果這個機率表顯示「下個字是 X 的機率為 100% 」那就代表模型非常**明確**，這時困惑度就會相對較低。\n",
    "    * 但如果模型覺得「嗯……好像每個字都有可能，那就代表模型相當**困惑**，這時困感度就會較高。\n",
    "* 一般語言模型都是以**交叉熵 (Cross Entropy)** 當作**損失函數 (Loss Function)** 而困惑度就是對**損失值 (Loss)** 取**指數函數 (Exponential Function)** 的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ea41b",
   "metadata": {},
   "source": [
    "#### 如何計算困惑度？\n",
    "1. 首先，要先來瞭解如何取得單次推論時的 Loss：\n",
    "2. 接下來對 Loss 取指數函數就能得到困感度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     device=\"auto\",\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(...) \n",
    "tk = AutoTokenizer.from_pretrained(...)\n",
    "input_ids = tk.encode(...)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(input_ids=input_ids, labels=input_ids) \n",
    "\n",
    "print(outputs.loss)\n",
    "\n",
    "ppl = torch.exp(outputs.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf51515",
   "metadata": {},
   "source": [
    "#### 最簡單的評測方法\n",
    "**固定序列長度**來計算困惑度，以 Wikitext 資料集為例："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f66d75",
   "metadata": {},
   "source": [
    "##### 1. 首先透過分詞器進行分詞："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tk = AutoTokenizer.from_pretrained(...)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-vl\",\n",
    "    split=\"test\",\n",
    ")\n",
    "\n",
    "Input_ids = list()\n",
    "for item in dataset:\n",
    "    text = item[\"text\"] + \"\\n\"\n",
    "    tokens = tk.encode(text, add_special_tokens=False)\n",
    "    input_ids.extend(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b100661",
   "metadata": {},
   "source": [
    "##### 2. 這裡先設定序列長度為 4096 來進行評估：\n",
    "* 其中長度不足 4096 的最後一筆資料會被捨棄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seqlen = 4096\n",
    "data_size = len(input_ids) // seqlen # 計算序列數量\n",
    "input_ids= input_ids[: data_size * seqlen] # 捨棄最後一筆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaea028",
   "metadata": {},
   "source": [
    "##### 3. 然後將 `input_ids` 轉入 `Tensor`，並在每個序列的開頭加上 BOS Token，最後把 `Tensor` 移動到對應的裝置上：\n",
    "* 雖然這裡長度設定為 4096，但是因為加上額外的一個 BOS Token，所以填型的實際推論長度是 4097，不過計算 Loss 時會位移一格，因此最終得到的困惑度依然是以序列長度 4096 算出來的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c94c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(input_ids).view(data_size, seqlen)\n",
    "bos_token = torch.full(\n",
    "    (data_size, 1),\n",
    "    tk.bos_token_id,\n",
    "    dtype=torch.int64,\n",
    ")\n",
    "\n",
    "input_ids = torch.concat((bos_token, input_ids), dim=1)\n",
    "input_ids = input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0afda5",
   "metadata": {},
   "source": [
    "##### 4. 接下來開始對測試資料進行推論：\n",
    "* `n11s` 代表 Negative Log-Likelihood，與 Cross Entropy 基本上是等價的概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = list()\n",
    "for i in range(data_size):\n",
    "    batch = input_ids[i : i+1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(batch, labels=batch)\n",
    "    nlls.append(outputs.loss)\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55457ce1",
   "metadata": {},
   "source": [
    "##### 5. 因為評估的過程可能會花上一段時間，所以可以借助 `tqdm` 套件來顯示進度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3883cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "batch_size = 1\n",
    "nlls = list()\n",
    "with trange(0, data_size, batch_size) as prog:\n",
    "    for i in prog:\n",
    "        batch = input_ids[i : i + batch_size]\n",
    "        outputs = model.forward(batch, labels=batch)\n",
    "        nlls.append(outputs.loss)\n",
    "        ppl = torch.exp(torch.stack(nlls).mean())\n",
    "        prog.desc= f\"ppl: {ppl:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb5fd4",
   "metadata": {},
   "source": [
    "#### 結論\n",
    "* 這裡拿 Llama 3 8B Instruct 簡單比較一下 Float16、BFloat16 與 BitsAndBytes 之間困惑度的差異：\n",
    "    | 類型      | 困惑度 |\n",
    "    | --------- | ------ |\n",
    "    | Float16   | 7.7494 |\n",
    "    | BFloat16  | 7.7570 |\n",
    "    | BNB 8-Bit | 7.7730 |\n",
    "    | BNB 4-Bit | 8.1899 |\n",
    "    * 可以看到 FP16 與 BF16 幾乎沒有差異，BNB 8-Bit 也與 FP16 十分相近，但 BNB 4-Bit 就稍微有點距離了，不過這個差距還算可以接受。\n",
    "    * 根據筆者的經驗， 通常困惑度的差距到達 1~2 點左右就會開始感受到實際輸出品質在下降了。\n",
    "* 困惑度並不是個絕對性的指標，在同個模型內做比較是滿有參考性的，但如果是不同模型或者不同訓練資料時，用困惑度進行互相比較就需要更加謹慎一些。\n",
    "    * 例如分詞器的不同，可能會造成同一份文本對不同模型而言有不同的長度，在困惑度的比較上也會因此產生差異。\n",
    "* 在合理的量化下，模型的參數量通常會比模型的大小來的重要。\n",
    "    * 就算用 16-Bit 去跑一個 7B 的模型，其效果也不會比 4-Bit 的 13B 模型還要好，即使 7B 16-Bit 的模型在賣際大小上比 13B 4-Bit 還要大的多。\n",
    "    * 因此模型的能力主要取決於參數量的多寡，而資料型態僅影響運算的精準度而已。\n",
    "* 即便模型變小了，但模型的總計算量原則上是不變的，因此並不會因為模型從 16-Bit 變成 8-Bit 後，推論速度就快上兩倍。\n",
    "* 真正有影響的是每個權重的位元數縮小後，在運算上可以減少傳輸頻寬，使**吞吐量 (Throughput)** 提高，也就是每個**批次 (Batch)** 能容納的 Token 變多了。\n",
    "* 或者從軟硬體上針對特殊設計的資料型態進行加速運算，但是基本上計算量還是一樣的。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
