{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f85f1e",
   "metadata": {},
   "source": [
    "### 速度比較\n",
    "比較一下 HF Transformers 與 llama.cpp 之間的推論速度。\n",
    "* 在這邊先附上筆者的硬體設備資訊：\n",
    "    * CPU： 12th Gen Intel(R) Core(TM) i7-12700K\n",
    "    * GPU： NVIDIA GeForce RTX 3090\n",
    "* 測量速度的時候，主要考量以下因素:\n",
    "    * **預填充 (Prefilling)**： 初始輸入階段，觀察第一個 Token 生成的延遲。\n",
    "    * **解碼 (Decoding)**： 後續輸出階段，觀察 Token 與 Token 之間的延遲。\n",
    "    * **延遲 (Latency)**： 只考慮單筆處理的速度。\n",
    "    * **吞吐量 (Throughput)**： 考慮同時處理多筆的速度。\n",
    "* 通常會用每秒可以處理多少 Tokens 來當作衡量速度的單位，可表示成單位 `tokens/s` 或簡寫為 `t/s`。\n",
    "* 以下使用的 HF Transformers 版本為 4.41.1，使用的 llama.cpp 版本為 commit 9b8247，評測模型皆為 Llama 3 8B。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d94f6d",
   "metadata": {},
   "source": [
    "1. Prefill 設定在 4K 左右，而 Decode 設定為 128，首先在 Transformers 上測單筆推論的速度：\n",
    "2. 接下來看看 llama.cpp 吧，因為是測單筆，所以不開 `-cb` 跟 `-np` ：\n",
    "3. 接下來測試看看不同量化方法的單筆速度：\n",
    "4. 最後簡單比較一下多筆推論的速度差異：\n",
    "\n",
    "請參考[筆者速度評測程式碼](https://tinyurl.com/llm-note-11)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
