{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116aa1bb",
   "metadata": {},
   "source": [
    "### 環境建置\n",
    "llama.cpp 支援使用 CUDA 在 NVIDIA GPU 上進行運算，因此需要準備一個有包含編譯器 `nvcc` 的環境，才能使用 GPU 進行運算，一般來說只要安裝 CUDA 就會有了。\n",
    "* 注意：\n",
    "    * CUDA 的版本可以參考 llama.cpp 專案底下 `.devops` 資料夾內與 CUDA 相關的 Dockerfile，目前是使用 CUDA 11.7.1 版。\n",
    "    * 但需要注意自身 `gcc` 的版本與 `nvcc` 是否相容。\n",
    "        * 例如筆者的 `gcc` 版本為 13，那 `nvcc` 就要裝到 12.4 才能用，同時也別忘了GPU 驅動程式的支援版本！\n",
    "* 除了 CUDA 以外還需要 CMake 編譯工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45673ab9",
   "metadata": {},
   "source": [
    "#### 以下示範使用 Conda 建立環境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda create -yn ggml python=3.11\n",
    "conda activate ggml\n",
    "conda install -y nvidia/label/cuda-11.7.1::cuda\n",
    "pip install cmake\n",
    "conda activate ggml # 確保 cmake 路徑有被系統正確讀取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81aa79",
   "metadata": {},
   "source": [
    "#### 透過 Git 將 llama.cpp 專案的原始碼複製下來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/ggerganov/llama.cpp --depth 1\n",
    "cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1909c",
   "metadata": {},
   "source": [
    "####  接著使用 CMake 進行建置：\n",
    "* 使用 `-DLLAMA_CUBLAS=ON` 參數啟用 CUDA 函式庫，若只想使用 CPU 的話把這個選項移掉就好。\n",
    "* 筆者這邊加上 `--fresh` 來強制 CMake 重新建立編譯資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cmake -B build -DGGML_CUDA=ON --fresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918768c",
   "metadata": {},
   "source": [
    "* 在 cmake 執行的過程，確保以下訊息有出現：\n",
    "\n",
    "    ```txt\n",
    "    -- Found CUDAToolkit: /home/user/.miniconda3/envs/ggml/include (found version \"11.8.89\")\n",
    "    -- cuBLAS found\n",
    "    -- The CUDA compiler identification is NVIDIA 11.8.89\n",
    "    ```\n",
    "    * 如果沒有出現這些訊息的話，可能要確認一下 CUDA 環境是否設置正確，否則建置出來的程式會因為不能使用 GPU 而跑得很慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cabcdb",
   "metadata": {},
   "source": [
    "#### 最後進行完整建置：\n",
    "* 建置過程約需一兩分鐘左右，完成建置後，相關的主程式與工具都會放在 `llama.cpp/build/bin` 底下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b381199",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cmake --build build --config Release -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca90bc",
   "metadata": {},
   "source": [
    "* 補充：\n",
    "    * llama.cpp 也支援 Windows 及 macOS 作業系統，除了參考 GitHub 的說明自行編譯以外，也可以在 GitHub 的 Releases 頁面找到作者預先編譯好的版本。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
