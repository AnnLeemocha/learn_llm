{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13aaace6",
   "metadata": {},
   "source": [
    "### 主程式 llama-cli\n",
    "`llama-cli` 是整個專案最基本的用法，絕大多數的指令參數都放在這裡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67453b56",
   "metadata": {},
   "source": [
    "#### 1. 參數說明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c89d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- common params -----\n",
      "\n",
      "-h,    --help, --usage                  print usage and exit\n",
      "--version                               show version and build info\n",
      "--completion-bash                       print source-able bash completion script for llama.cpp\n",
      "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
      "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
      "                                        (env: LLAMA_ARG_THREADS)\n",
      "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
      "                                        same as --threads)\n",
      "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
      "                                        (default: \"\")\n",
      "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
      "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
      "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
      "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
      "                                        (default: same as --cpu-mask)\n",
      "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
      "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
      "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
      "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
      "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
      "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
      "                                        context filled)\n",
      "                                        (env: LLAMA_ARG_N_PREDICT)\n",
      "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
      "                                        (env: LLAMA_ARG_BATCH)\n",
      "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
      "                                        (env: LLAMA_ARG_UBATCH)\n",
      "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
      "                                        all)\n",
      "--swa-full                              use full-size SWA cache (default: false)\n",
      "                                        [(more\n",
      "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "                                        (env: LLAMA_ARG_SWA_FULL)\n",
      "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
      "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
      "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
      "--no-perf                               disable internal libllama performance timings (default: false)\n",
      "                                        (env: LLAMA_ARG_NO_PERF)\n",
      "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
      "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
      "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
      "--no-escape                             do not process escape sequences\n",
      "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
      "                                        the model\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
      "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
      "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
      "                                        model)\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
      "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
      "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
      "                                        context size)\n",
      "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
      "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
      "                                        interpolation)\n",
      "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
      "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
      "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
      "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
      "-nkvo, --no-kv-offload                  disable KV offload\n",
      "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
      "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
      "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
      "                                        (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
      "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
      "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
      "                                        (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
      "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
      "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
      "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
      "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
      "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
      "                                        (env: LLAMA_ARG_MLOCK)\n",
      "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
      "                                        using mlock)\n",
      "                                        (env: LLAMA_ARG_NO_MMAP)\n",
      "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
      "                                        - distribute: spread execution evenly over all nodes\n",
      "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
      "                                        started on\n",
      "                                        - numactl: use the CPU map provided by numactl\n",
      "                                        if run without this previously, it is recommended to drop the system\n",
      "                                        page cache before using this\n",
      "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
      "                                        (env: LLAMA_ARG_NUMA)\n",
      "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
      "                                        offload)\n",
      "                                        use --list-devices to see a list of available devices\n",
      "                                        (env: LLAMA_ARG_DEVICE)\n",
      "--list-devices                          print list of available devices and exit\n",
      "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
      "                                        override tensor buffer type\n",
      "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
      "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
      "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
      "                                        - none: use one GPU only\n",
      "                                        - layer (default): split layers and KV across GPUs\n",
      "                                        - row: split rows across GPUs\n",
      "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
      "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
      "                                        proportions, e.g. 3,1\n",
      "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
      "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
      "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
      "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
      "--check-tensors                         check model tensor data for invalid values (default: false)\n",
      "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
      "                                        multiple times.\n",
      "                                        types: int, float, bool, str. example: --override-kv\n",
      "                                        tokenizer.ggml.add_bos_token=bool:false\n",
      "--no-op-offload                         disable offloading host tensor operations to device (default: false)\n",
      "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
      "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
      "                                        multiple adapters)\n",
      "--control-vector FNAME                  add a control vector\n",
      "                                        note: this argument can be repeated to add multiple control vectors\n",
      "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
      "                                        note: this argument can be repeated to add multiple scaled control\n",
      "                                        vectors\n",
      "--control-vector-layer-range START END\n",
      "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
      "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
      "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
      "                                        (env: LLAMA_ARG_MODEL)\n",
      "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
      "                                        (env: LLAMA_ARG_MODEL_URL)\n",
      "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
      "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
      "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
      "                                        Q4_K_M doesn't exist.\n",
      "                                        mmproj is also downloaded automatically if available. to disable, add\n",
      "                                        --no-mmproj\n",
      "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
      "                                        (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_REPO)\n",
      "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
      "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
      "                                        (env: LLAMA_ARG_HFD_REPO)\n",
      "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
      "                                        --hf-repo (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_FILE)\n",
      "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
      "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
      "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
      "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
      "                                        variable)\n",
      "                                        (env: HF_TOKEN)\n",
      "--log-disable                           Log disable\n",
      "--log-file FNAME                        Log to file\n",
      "--log-colors                            Enable colored logging\n",
      "                                        (env: LLAMA_LOG_COLORS)\n",
      "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
      "                                        debugging)\n",
      "--offline                               Offline mode: forces use of cache, prevents network access\n",
      "                                        (env: LLAMA_OFFLINE)\n",
      "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
      "                                        ignored.\n",
      "                                        (env: LLAMA_LOG_VERBOSITY)\n",
      "--log-prefix                            Enable prefix in log messages\n",
      "                                        (env: LLAMA_LOG_PREFIX)\n",
      "--log-timestamps                        Enable timestamps in log messages\n",
      "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
      "\n",
      "\n",
      "----- sampling params -----\n",
      "\n",
      "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
      "                                        ';'\n",
      "                                        (default:\n",
      "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
      "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
      "--sampling-seq, --sampler-seq SEQUENCE\n",
      "                                        simplified sequence for samplers that will be used (default:\n",
      "                                        edskypmxt)\n",
      "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
      "                                        --logit-bias EOS-inf)\n",
      "--temp N                                temperature (default: 0.8)\n",
      "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
      "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
      "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
      "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
      "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
      "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
      "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
      "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
      "                                        = ctx_size)\n",
      "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
      "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
      "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
      "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
      "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
      "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
      "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
      "                                        context size)\n",
      "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
      "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
      "                                        sequence breakers\n",
      "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
      "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
      "--mirostat N                            use Mirostat sampling.\n",
      "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
      "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
      "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
      "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
      "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
      "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
      "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
      "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
      "                                        dir) (default: '')\n",
      "--grammar-file FNAME                    file to read grammar from\n",
      "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
      "                                        `{}` for any JSON object\n",
      "                                        For schemas w/ external $refs, use --grammar +\n",
      "                                        example/json_schema_to_grammar.py instead\n",
      "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
      "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
      "                                        For schemas w/ external $refs, use --grammar +\n",
      "                                        example/json_schema_to_grammar.py instead\n",
      "\n",
      "\n",
      "----- example-specific params -----\n",
      "\n",
      "--no-display-prompt                     don't print prompt at generation (default: false)\n",
      "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
      "                                        (default: false)\n",
      "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
      "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
      "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
      "                                        template)\n",
      "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
      "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
      "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
      "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
      "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
      "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
      "-sp,   --special                        special tokens output enabled (default: false)\n",
      "-cnv,  --conversation                   run in conversation mode:\n",
      "                                        - does not print special tokens and suffix/prefix\n",
      "                                        - interactive mode is also enabled\n",
      "                                        (default: auto enabled if chat template is available)\n",
      "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
      "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
      "                                        will not be interactive if first turn is predefined with --prompt\n",
      "                                        (default: false)\n",
      "-i,    --interactive                    run in interactive mode (default: false)\n",
      "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
      "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
      "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
      "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
      "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
      "--no-warmup                             skip warming up the model with an empty run\n",
      "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
      "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
      "--jinja                                 use jinja template for chat (default: disabled)\n",
      "                                        (env: LLAMA_ARG_JINJA)\n",
      "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
      "                                        response, and in which format they're returned; one of:\n",
      "                                        - none: leaves thoughts unparsed in `message.content`\n",
      "                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n",
      "                                        streaming mode, which behaves as `none`)\n",
      "                                        (default: deepseek)\n",
      "                                        (env: LLAMA_ARG_THINK)\n",
      "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
      "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
      "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
      "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
      "                                        metadata)\n",
      "                                        if suffix/prefix are specified, template will be disabled\n",
      "                                        only commonly used templates are accepted (unless --jinja is set\n",
      "                                        before this flag):\n",
      "                                        list of built-in templates:\n",
      "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
      "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
      "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
      "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
      "                                        mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4,\n",
      "                                        rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
      "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
      "--chat-template-file JINJA_TEMPLATE_FILE\n",
      "                                        set custom jinja chat template file (default: template taken from\n",
      "                                        model's metadata)\n",
      "                                        if suffix/prefix are specified, template will be disabled\n",
      "                                        only commonly used templates are accepted (unless --jinja is set\n",
      "                                        before this flag):\n",
      "                                        list of built-in templates:\n",
      "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
      "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
      "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
      "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
      "                                        mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4,\n",
      "                                        rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
      "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
      "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
      "                                        consoles\n",
      "\n",
      "example usage:\n",
      "\n",
      "  text generation:     ./build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n",
      "\n",
      "  chat (conversation): ./build/bin/llama-cli -m your_model.gguf -sys \"You are a helpful assistant\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-cli --help # 顯示參數說明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ee971",
   "metadata": {},
   "source": [
    "#### 2. 首先是 `-m` 指定 gguf 格式的模型路徑，並使用 `-ngl` 指定 GPU 讀取層數：\n",
    "* 建議直接設定為 `-ngl 99` 就好，就算超過 GPU 記憶體上限，系統也會自動改放到 CPU 記憶體裡面。\n",
    "* 在沒有其他參數的情況下，程式會快速讀取完模型，然後自己開始瘋狂輸出，這通常是用來測試運作的速度有多快。\n",
    "* 調用 GPU 時，要確認讀取訊息有出現類似以下的文字：\n",
    "\n",
    "    ```md\n",
    "    llm_load_tensors: offloading 32 repeating layers to GPU\n",
    "    llm_load_tensors: offloading non-repeating layers to GPU \n",
    "    llm_load_tensors: offloaded 33/33 layers to GPU\n",
    "    ```\n",
    "    * 請確保最後一行寫的是 `33/33` 而不是 `30/33` 之類的，否則就代表有些模型參數沒有被放進 GPU 裡面，而是放在 CPU 裡面做運算，速度會因此大打折扣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41de38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 1 (a3c3084) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from llama-3-8b-inst.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.96 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors:   CPU_Mapped model buffer size = 15317.02 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 8\n",
      "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
      "main: chat template example:\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "system_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: interactive mode on.\n",
      "sampler seed: 2554815667\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "== Running in interactive mode. ==\n",
      " - Press Ctrl+C to interject at any time.\n",
      " - Press Return to return control to the AI.\n",
      " - To return control without starting a new line, end your input with '/'.\n",
      " - If you want to submit another line, end your input with '\\'.\n",
      " - Not using system message. To change it, set a different value via -sys PROMPT\n",
      "\n",
      "llama_perf_sampler_print:    sampling time =       0.00 ms /     1 runs   (    0.00 ms per token, 500000.00 tokens per second)\n",
      "llama_perf_context_print:        load time =     923.55 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =       1.14 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> EOF by user\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-cli -m llama-3-8b-inst.gguf -ngl 99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4ea4e",
   "metadata": {},
   "source": [
    "#### 3. 如果想要進入互動模式，可以加上 `-if` 參數：\n",
    "* 但這樣並不是 Llama 3 標準的聊天模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "# 請用 terminal 執行\n",
    "./build/bin/llama-cli -m llama-3-8b-inst.gguf -ngl 99 -if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c521e",
   "metadata": {},
   "source": [
    "#### 4. 可以透過參數 `-cnv` 來啟用聊天模型，並搭配參數 `-p` 來指定系統提示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ab902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 1 (a3c3084) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from llama-3-8b-inst.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.96 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors:   CPU_Mapped model buffer size = 15317.02 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context:        CPU compute buffer size =   296.01 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 8\n",
      "*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n",
      "main: chat template example:\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "system_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: interactive mode on.\n",
      "sampler seed: 2165838727\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "== Running in interactive mode. ==\n",
      " - Press Ctrl+C to interject at any time.\n",
      " - Press Return to return control to the AI.\n",
      " - To return control without starting a new line, end your input with '/'.\n",
      " - If you want to submit another line, end your input with '\\'.\n",
      " - Not using system message. To change it, set a different value via -sys PROMPT\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "You are a helpful assistant. Reply in Traditional Chinese.assistant\n",
      "\n",
      "( w�� huì bāng zhù nǐ )\n",
      "\n",
      "> EOF by user\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =       0.79 ms /    36 runs   (    0.02 ms per token, 45801.53 tokens per second)\n",
      "llama_perf_context_print:        load time =     938.65 ms\n",
      "llama_perf_context_print: prompt eval time =     580.17 ms /    21 tokens (   27.63 ms per token,    36.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4207.30 ms /    14 runs   (  300.52 ms per token,     3.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4791.82 ms /    35 tokens\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "# 請用 terminal 執行\n",
    "./build/bin/llama-cli -m llama-3-8b-inst.gguf -ngl 99 -cnv \\\n",
    "    -p \"You are a helpful assistant. Reply in Traditional Chinese. (繁體中文)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577620d",
   "metadata": {},
   "source": [
    "* 在非互動模式下，會用 `-n` 指定生成多少 Tokens，並搭配 `--ignore-eas` 來評測模型或硬體的速度。\n",
    "* 透過 `-c` 參數可以指定模型的輸入長度，預設只有 512，但目前的模型都支援到 4096 或 8192 以上了，所以如果有相關應用要消耗大量 Token 的話，這個參數記得一定要開大，不然模型的輸出會看起來很奇怪。\n",
    "* 在純 CPU 運算時，參數 `--threads` 對速度的影響滿大的，建議開系統總核心數的一半，例如你是 20 核的 CPU，那就指定 `--threads 10` 這樣。\n",
    "* 另外能透過 `-fa` 來啟用 Flash Attention 加速生成。\n",
    "* 其他像是 `--top-k`、`--top-p`、`--temp` (也就是Temperature) 等都是老面孔的取樣參數了，除此之外 llama.cpp 還額外支援相當豐富的取樣參數，可以自行研究看看。\n",
    "* 補充：\n",
    "    * 其中把 `--temp` 設定為 0.0 並且把 `--repeat-penalty` 設定為 1.0，會最貼近在 HF Transformers 套件裡面設定 `do_sample=False` 的結果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
