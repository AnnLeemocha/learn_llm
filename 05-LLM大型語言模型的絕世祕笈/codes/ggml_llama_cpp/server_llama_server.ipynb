{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5faeb7",
   "metadata": {},
   "source": [
    "### 伺服器 llama-server\n",
    "`llama-server` 是筆者相當喜歡的一個程式，他可以將模型變成一個服務，使用者可以透過 HTTP API 來存取模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61d27a",
   "metadata": {},
   "source": [
    "#### 1. 參數與 llama-cli 大致相同，例如：\n",
    "* 比較大的不同在於參數 `-cb` 與 `-np`：\n",
    "    * `-cb` 指的是 Continuous Batching，也就是說使用者的輸入會不斷加入批次裡面，而不需要等整個批次都結束了才能處理下個輸入。\n",
    "    * `-np` 則是指能夠同時處理的輸入數量，這裡設定 `-np 4` 就代表系統最多能同時處理四個輸入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f7338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 1 (a3c3084) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "system info: n_threads = 8, n_threads_batch = 8, total_threads = 32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "system_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: binding port with default address family\n",
      "main: HTTP server is listening, hostname: 0.0.0.0, port: 8888, http threads: 31\n",
      "main: loading model\n",
      "srv    load_model: loading model 'llama-3-8b-inst.gguf'\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from llama-3-8b-inst.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.96 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors:   CPU_Mapped model buffer size = 15317.02 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 4\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 1\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     1.96 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  4 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context:        CPU compute buffer size =   266.50 MiB\n",
      "llama_context: graph nodes  = 1031\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "srv          init: initializing slots, n_slots = 4\n",
      "slot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\n",
      "slot         init: id  1 | task -1 | new slot n_ctx_slot = 2048\n",
      "slot         init: id  2 | task -1 | new slot n_ctx_slot = 2048\n",
      "slot         init: id  3 | task -1 | new slot n_ctx_slot = 2048\n",
      "main: model loaded\n",
      "main: chat template, chat_template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}, example_format: '<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8888 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-server \\\n",
    "    -m llama-3-8b-inst.gguf \\\n",
    "    -ngl 99 -c 8192 -fa -cb -np 4 \\\n",
    "    --host 0.0.0.0 --port 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a480de1",
   "metadata": {},
   "source": [
    "#### 2. 啟動之後可以在 `http://127.0.0.1:8888/` 打開網頁介面進行互動，這個介面只是用來簡單測試，一般開發通常還是以 API 呼叫居多："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01e17d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1706  100  1668  100    38    347      7  0:00:05  0:00:04  0:00:01   355  100    38    347      7  0:00:05  0:00:04  0:00:01   463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"index\":0,\"content\":\" (nǐ hǎo) - Hello!\\n\\nWelcome to my GitHub page!\",\"tokens\":[],\"id_slot\":0,\"stop\":true,\"model\":\"gpt-3.5-turbo\",\"tokens_predicted\":16,\"tokens_evaluated\":4,\"generation_settings\":{\"n_predict\":16,\"seed\":4294967295,\"temperature\":0.800000011920929,\"dynatemp_range\":0.0,\"dynatemp_exponent\":1.0,\"top_k\":40,\"top_p\":0.949999988079071,\"min_p\":0.05000000074505806,\"top_n_sigma\":-1.0,\"xtc_probability\":0.0,\"xtc_threshold\":0.10000000149011612,\"typical_p\":1.0,\"repeat_last_n\":64,\"repeat_penalty\":1.0,\"presence_penalty\":0.0,\"frequency_penalty\":0.0,\"dry_multiplier\":0.0,\"dry_base\":1.75,\"dry_allowed_length\":2,\"dry_penalty_last_n\":8192,\"dry_sequence_breakers\":[\"\\n\",\":\",\"\\\"\",\"*\"],\"mirostat\":0,\"mirostat_tau\":5.0,\"mirostat_eta\":0.10000000149011612,\"stop\":[],\"max_tokens\":16,\"n_keep\":0,\"n_discard\":0,\"ignore_eos\":false,\"stream\":false,\"logit_bias\":[],\"n_probs\":0,\"min_keep\":0,\"grammar\":\"\",\"grammar_lazy\":false,\"grammar_triggers\":[],\"preserved_tokens\":[],\"chat_format\":\"Content-only\",\"reasoning_format\":\"deepseek\",\"reasoning_in_content\":false,\"thinking_forced_open\":false,\"samplers\":[\"penalties\",\"dry\",\"top_n_sigma\",\"top_k\",\"typ_p\",\"top_p\",\"min_p\",\"xtc\",\"temperature\"],\"speculative.n_max\":16,\"speculative.n_min\":0,\"speculative.p_min\":0.75,\"timings_per_token\":false,\"post_sampling_probs\":false,\"lora\":[]},\"prompt\":\"<|begin_of_text|>你好!\",\"has_new_line\":true,\"truncated\":false,\"stop_type\":\"limit\",\"stopping_word\":\"\",\"tokens_cached\":19,\"timings\":{\"prompt_n\":4,\"prompt_ms\":323.747,\"prompt_per_token_ms\":80.93675,\"prompt_per_second\":12.355326844727518,\"predicted_n\":16,\"predicted_ms\":4478.112,\"predicted_per_token_ms\":279.882,\"predicted_per_second\":3.5729343080298124}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST http://localhost:8888/completion \\\n",
    "    -d '{\"prompt\": \"你好!\", \"n_predict\": 16}'\n",
    "# Output: {\"content\": \"今天我們要為大家介紹的是..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071c70b",
   "metadata": {},
   "source": [
    "#### 3. 可以撰寫一個 Python 程式用串流的方式接收模型輸出：\n",
    "* 這裡透過 `stop` 參數就能指定模型輸出的停止點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1885eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language models are artificial intelligence (AI) systems designed to process, generate, and manipulate human language. They are trained on large amounts of text data, such as books, articles, and social media posts, to learn patterns, relationships, and structures of language. Language models can perform various tasks, including:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8888/completion\" \n",
    "prompt = \"[INST] 什麼是語言模型? [/INST]\"\n",
    "\n",
    "params = {\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": True,\n",
    "    \"stop\": [\"\\n\", \"\\n\\n\"],\n",
    "}\n",
    "\n",
    "resp = requests.post(url, json=params, stream=True) \n",
    "for chunk in resp.iter_lines():\n",
    "    if not chunk:\n",
    "        continue\n",
    "    # 會有固定的 \"data:\" 前級，需要跳掉 5 個字元\n",
    "    content = json.loads(chunk[5:])[\"content\"]\n",
    "    print(end=content, flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada3c13",
   "metadata": {},
   "source": [
    "* 為了避免使用者輸入太長的提示，可以透過 `tokenize` 與 `detokenize` API 來截斷使用者的提示，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503b56b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15339, 11, 94776, 7356, 0]\n"
     ]
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:8888/tokenize\"\n",
    "params = {\"content\": \"hello, llama.cpp!\"}\n",
    "resp = requests.post(url, json=params)\n",
    "tokens = json.loads(resp.text)[\"tokens\"]\n",
    "print(tokens) # [6312, 28709, 28725,...1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f67c7c",
   "metadata": {},
   "source": [
    "* 假設我們只需要最後面三個 Tokens 的話："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : hello\n",
      "1 : ,\n",
      "2 :  llama\n",
      "3 : .cpp\n",
      "4 : !\n",
      " llama.cpp!\n"
     ]
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:8888/detokenize\"\n",
    "\n",
    "# for i in range(len(tokens)):\n",
    "#     token=tokens[i]\n",
    "for i, token in enumerate(tokens):\n",
    "    params = {\"tokens\": [token]}\n",
    "    resp = requests.post(url, json=params)\n",
    "    content = json.loads(resp.text)[\"content\"]\n",
    "    print(i, \":\", content) # hello, llama.cpp!\n",
    "\n",
    "print(\"------------------------------\")\n",
    "\n",
    "params = {\"tokens\": tokens[-3:]}\n",
    "resp = requests.post(url, json=params)\n",
    "content = json.loads(resp.text)[\"content\"]\n",
    "print(content) #  llama.cpp!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18a70a",
   "metadata": {},
   "source": [
    "* 這樣就完成了截斷提示長度的組合操作囉!\n",
    "* 除了以上這些 API 以外，還有使用 LLM 做檢索時能透過 `/embedding` 取得文句向量，以及 Code LLM 常用的 `/infill` 程式碼填充，也有與 OpenAl API 容的 `/v1/chat/completions` 可以使用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
