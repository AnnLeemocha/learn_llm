{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd279bc1",
   "metadata": {},
   "source": [
    "### Python Binding\n",
    "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) 是 llama.cpp 的 Python 介面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3fb1a",
   "metadata": {},
   "source": [
    "#### 1. 透過以下指令安裝："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f82dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━�m\u001b[9╺\u001b\u001b[90m�━\u001b[0m \u001b[32m0.0/67.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m0.5/67.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/67.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/67.9 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/67.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.6/67.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m��━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/67.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/67.9 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/67.9 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m49.3/67.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m53.0/67.9 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m56.6/67.9 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m59.8/67.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m62.7/67.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m65.8/67.9 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m67.6/67.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m��━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from llama-cpp-python) (2.0.1)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[32m*** \u001b[1mscikit-build-core 0.11.3\u001b[0m using \u001b[34mCMake 4.0.2\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /tmp/tmpojdodexr/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is GNU 11.4.0\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is GNU 11.4.0\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  \u001b[31m   \u001b[0m -- Found Threads: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "  \u001b[31m   \u001b[0m -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  \u001b[31m   \u001b[0m -- Including CPU backend\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- x86 detected\n",
      "  \u001b[31m   \u001b[0m -- Adding CPU backend variant ggml-cpu: -march=native\n",
      "  \u001b[31m   \u001b[0m -- Found CUDAToolkit: /usr/local/cuda-12.5/targets/x86_64-linux/include (found version \"12.5.82\")\n",
      "  \u001b[31m   \u001b[0m -- CUDA Toolkit found\n",
      "  \u001b[31m   \u001b[0m -- Using CUDA architectures: native\n",
      "  \u001b[31m   \u001b[0m -- The CUDA compiler identification is NVIDIA 11.5.119 with host compiler GNU 11.4.0\n",
      "  \u001b[31m   \u001b[0m -- Detecting CUDA compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CUDA compiler ABI info - failed\n",
      "  \u001b[31m   \u001b[0m -- Check for working CUDA compiler: /usr/bin/nvcc\n",
      "  \u001b[31m   \u001b[0m \u001b[31mCMake Error in /tmp/tmpojdodexr/build/CMakeFiles/CMakeScratch/TryCompile-S5e8AH/CMakeLists.txt:\n",
      "  \u001b[31m   \u001b[0m   CUDA_ARCHITECTURES is set to \"native\", but no NVIDIA GPU was detected.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31mCMake Error in /tmp/tmpojdodexr/build/CMakeFiles/CMakeScratch/TryCompile-S5e8AH/CMakeLists.txt:\n",
      "  \u001b[31m   \u001b[0m   CUDA_ARCHITECTURES is set to \"native\", but no NVIDIA GPU was detected.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31mCMake Error at /tmp/pip-build-env-ubu2kfhi/normal/lib/python3.11/site-packages/cmake/data/share/cmake-4.0/Modules/CMakeTestCUDACompiler.cmake:48 (try_compile):\n",
      "  \u001b[31m   \u001b[0m   Failed to generate test project build system.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   vendor/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt:43 (enable_language)\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m   \u001b[0m \u001b[31m\n",
      "  \u001b[31m   \u001b[0m \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'CMAKE_ARGS=\"-DGGML_CUDA=ON\" pip install llama-cpp-python\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCMAKE_ARGS=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-DGGML_CUDA=ON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m pip install llama-cpp-python\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ann_py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ann_py311/lib/python3.11/site-packages/IPython/core/magics/script.py:159\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ann_py311/lib/python3.11/site-packages/IPython/core/magics/script.py:336\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    335\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'CMAKE_ARGS=\"-DGGML_CUDA=ON\" pip install llama-cpp-python\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=ON\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aec803",
   "metadata": {},
   "source": [
    "#### 2. 以下是個簡單的 Streaming 範例：\n",
    "* 若想要隱藏原本 llama.cpp 的訊息紀錄，將 verbose 參數設定為 False 即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"llama.cpp/llama-3-8b-inst.gguf\",\n",
    "    n_gpu_layers=99,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "output = llm(\n",
    "    \"[INST] 什麼是語言模型? [/INST]\",\n",
    "    max_tokens=128,\n",
    "    stop=[\"\\n\", \"\\n\\n\"],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for token in output:\n",
    "    print(end=token[\"choices\"][0][\"text\"], flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43fdf5",
   "metadata": {},
   "source": [
    "* 用法原則上大同小異，詳細資訊可以參考[官方文件](https://llama-cpp-python.readthedocs.io/en/stable/)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
