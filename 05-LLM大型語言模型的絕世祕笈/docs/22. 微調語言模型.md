# 微調語言模型
**微調 (Finetune)** 一個大型語言模型在方法上與微調其他模型其實很相似，但是因為語言模型的參數量通常較大，所以訓練的門檻會比其他模型高的多。
* 使用大機器訓練模型，就像在豪華的廚房裡面做菜，所有工具一應俱全，但是要端出可以滿足整間餐廳的菜色與份量。
* 而使用小機器訓練模型，就像是與三五好友到野外露營，只有幾個小鍋子跟卡式瓦斯爐，為了在小廚具上做出少人份的佳餚，需要講究很多不佔空間的烹調技術。雖然菜色未必華麗，但絕對能令自己滿意！

大機器的訓練豪放奢華，小機器的訓練精緻浪漫，接下來就帶大家看看，如何在有限的資源內烹調一道精心製作的語言模型！

---

## 任務設計
一開始學習微調模型時，可以透過單純的**玩具任務 (Toy Task)** 來練習。
* 玩具任務通常很簡單，因此模型容易學起來，而且資料來源也很容易爬取跟操作。
* 這裡筆者設計了一個**路名解析任務**：
    ```md
    輸入：臺北市中正區八德路
    輸出：{"city": "臺北市", "town": "中正區", "road": "八德路"}
    ```
    * 使用者輸入一段帶有行政區的路名，請模型將他解析成 JSON 格式。
    * 要讓模型學會完成這個任務，最單純的想法是輸入原句，並直接輸出 JSON 結果，像是將上方的範例當成提示直接丟進語言模型裡面做訓練。
* 這樣做不是不行，但這比較像是傳統 Encoder-Decoder 的訓練方法。
    * 拿這類的資料訓練現在的Decoder 效果通常並不好，不如直接用參數量更少的 Encoder-Decoder 來訓練。
* 對語言模型而言， 更多指令的成份在裡面：
    ```md
    ### USER:

    請將以下路名解析為 JSON 格式。

    輸入：臺北市中正區八德路
    輸出：{"city": "臺北市", "town": "中正區", "road": "八德路"}

    輸入：屏東縣佳冬鄉文化三路

    ### ASSISTANT:

    {"city": "屏東縣", "town": "佳冬鄉", "road": "文化三路"}
    ```
    * 這樣的訓練資料不僅給了**測試輸入**與**正解輸出**，還包含了**一個指令**與**一份範例**，對於一個**指令微調過的語言模型**而言會更加有意義，訓練上也比較不容易破壞語言模型的其他能力。
    * 還記得之前說過語言模型是藉由**因果建模**進行訓練的嗎？因此給他越充分的**因**，就能產生越穩定的**果**。
    * 如果我們直接將「臺北市中正區八德路」輸入到 ChatGPT 裡面，他只會開始介紹這條路的相關知識而已，並不會幫你把路名解析成JSON 格式。
    * 需要對 ChatGPT 下達明確的指示，甚至給他一個範例，才能完成預想的目標。
    * 因此對於一個指令微調的語言模型而言，在訓練資料中加入明確指令是很重要的。

---

## 建立資料集
行政區與路名的資料可以從[**政府資料開放平台**](https://data.gov.tw/)取得，其中的[**全國路名資料**](https://data.gov.tw/dataset/35321)就有我們需要的資訊，筆者使用 [**112 全國路名資料**](https://tinyurl.com/llm-tw-roads)的 CSV 檔做示範。
* 其格式大致如下：
    ```md
    city,site_id,road
    縣市名稱,行政區域名稱,全國路名
    宜蘭縣,宜蘭縣三星鄉,人和一路
    宜蘭縣,宜蘭縣三星鄉,人和七路
    宜蘭縣,宜蘭縣三星鄉,人和九路
    ```
    * 在建立資料集之前，需要先來做一些基本的資料觀察，看看這份資料有沒有不正常的內容？
* 先把所有內容讀取進來，然後用 set 類別來統計所有字元，並輸出他們的編碼：
    ```python
    with open("opendata112road.csv", "rt", encoding="UTF-8") as fp:
        char_set = set(fp.read())

    # 排序並列出字元編號
    char_set = sorted(list(char_set))
    lines = [f"{c!r}) {ord(c)}" for c in char set]

    with open("chars.txt", "wt", encoding="UTF-8") as fp:
        fp.write("\n".join(lines))
    ```
    * 觀察輸出的 `chars.txt` 看看都包含了些什麼字：
    ```md
    'n'          10
    ','          44
    '?'          63      <= 不知哪裡來的同號
    '_'          95

    ...

    '■'          9607    <= 奇怪的正方形
    '大'         12068
    '\u3000'     12288   <= 這是全形空格
    'ㄧ'         12583   <= 這是注音文

    ...

    '沿'         194812
    'a å¥½'      194818  <= 堆顯示不出來的亂碼
    'a å¥½'      194824
    '\U000fffa8' 1048488
    '\U000fffc0' 1048512
    ```
    * 有莫名的問號、全形空格跟一堆顯示不出來的亂碼，即便是小型的玩具任務，但這些對訓練都是很有影響的，資料品質至上！
* 面對這樣的問題，通常有兩種選擇：
    1. 用程式碼自動修正
    2. 或者把有問題的資料丟掉不要用
    * 但千萬不要試圖人工修正**所有**資料！時間寶貴，精力有限，請把你的聰明才智專注在更有效率的做法上！
    * 資料品質至上跟不要人工修正好像互相違背欸！其實不然，在團隊開發中，通常會聘請資料標注人員協助修正資料，而捨棄有問題的資料也是提昇資料品質的手法。
    * 對於獨立開發者而言，人工修正不是不行，但如果每次看到資料有問題就想要手動修正，很容易養成習慣，造成自身的工作效率低落。
    * 在這個人工智慧的時代，更應該學習如何 Working Smart 而不是 Working Hard。
        * 例如先透過簡單的邏輯將有問題的資料篩選出來，再把這些問題丟給 ChatGPT 去做修正，就是個相對有效率的做法。
* 除了全形空白可以直接取代掉以外，剩下有問題的字元編號存在 `exclude.txt` 裡面，並把這些包含問題字元的資料給捨棄掉：
    ```python
    with open("exclude.txt", "rt", encoding="UTF-8") as fp:
        exclude_ord = fp.read().strip().split("\n")
        exclude_ord = set(map(int, exclude_ord))

    with open("opendata1-12road.csv", "rt", encoding="UTF-8") as fp:
        lines = list()
        for line in fp:
            if set(map(ord, line)) & exclude_ord:
                continue

            # 取代全形空白
            line = line.replace("\u3000", "")
            lines.append(line)

    with open("road-filter.csv", "wt", encoding="UTF-8") as fp:
        fp.writelines(lines)
    ```
* 全臺灣有近三萬五千條路，每一行都包含了行政區以及路名。這裡需要將直轄縣市與鄉鎮市區分開，各自放進 `city` 與 `town` 欄位裡面，最後與 `road` 欄位組成完整的資料：
    ```python
    import csv

    dataset = list()
    with open("opendata112road.csv", "rt", encoding="UTF-8") as fp:
        # 跳過前兩行說明用的資料列
        fp.readline()  # city,site_id,road
        fp.readline()  # 縣市名稱,行政區域名稱,全國路名

        for city, site, road in csv.reader(fp):
            town = site.replace(city, "")
            item = {"city": city, "town": town, "road": road}
            dataset.append(item)
    ```
* 接下來將資料切成訓練、驗證、測試集等三份，因為目的只是少量測試訓練效果，所以訓練與驗證集各取 200 筆資料即可。也因為這個任務很簡單，推論速度也很快，所以測試集可以取多一點：
    ```python
    import json
    import random

    def dump_json(data, file_path):
        with open(file_path, "wt", encoding="UTF-8") as fp:
            json.dump(data, fp, ensure_ascii=False, indent=2)

    random.seed(0)
    random.shuffle(dataset)

    train = dataset[:200]
    dev = dataset[200:400]
    test = dataset[400:1000]

    dump_json(train, "train.json")
    dump_json(dev, "dev.json")
    dump_json(test, "test.json")
    ```
* 接下來要對訓練集與驗證集進行斷詞，一開始先使用小模型做測試，所以選擇參數量只有 1B 的 [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) 模型來微調，因此使用他的分詞器來進行斷詞：
    ```python
    from transformers import LlamaTokenizerFast as TkCls

    model_id = "PY007/TinyLlama-1.1B-Chat-v0.3"
    tk: TkCls = TkCls.from_pretrained(model_id)
    ```
    * 在因果建模的訓練流程裡面，每筆訓練資料必須包含 `input_ids` 與 `labels` 兩個欄位，而且這兩個欄位的內容會是完全一樣的。
        * 例如：
        ```json
        [
            {
                "input_ids": [ 1, 43, 92, 71,  2, 2],
                "labels":    [ 1, 43, 92, 71,  2, 2]
            },
            {
                "input_ids": [ 1, 97, 66, 92, 71, 2],
                "labels":    [ 1, 97, 66, 92, 71, 2]
            }
        ]
        ```
    * 當訓練文本量很大時，每次訓練模型之前都要重新斷詞一次會顯得很沒效率，因此筆者習慣先將文本進行斷詞並存成檔案，這樣訓練的程式就只需要專注在訓練流程上，而不用每次都花上一段時間在重新斷詞和建立輸入格式上。
* 首先寫一個拜訪資料並產生對應輸入輸出的函式：
    ```python
    def iter_dataset(file_path):
        data = load_json(file_path)

        for item in data:
            city = item["city"]
            town = item["town"]
            road = item["road"]

            full = f"{city}{town}{road}"
            item = json.dumps(item, ensure_ascii=False)

            yield full, item
    ```
* 將 City、Town 跟 Road 合併成一個字串當作輸入句，並產生 JSON 格式的答案，然後把每筆資料根據我們設計好的格式，套用到 TinyLlama 提供的對話樣板裡面：
    ```python
    template = """請將以下路名解析為 JSON 格式。

    輸入：臺北市中正區八德路
    輸出：{{"city": "臺北市", "town": "中正區", "road": "八德路"}}

    輸入：{}"""

    ds_type = "train"
    ds_tokens = list()
    for text, item in iter_dataset(f"{ds_type}.json"):

        inn = dict(role="user", content=template.format(text))
        out = dict(role="assistant", content=item)
        input_ids = tk.apply_chat_template([inn, out])
        ds_tokens.append(input_ids)
        # print(tk.decode(input_ids)) # 檢查格式是否符合預期
    ```
* 完成 Token 轉換後，需要對資料長度進行**填充 (Padding)**，因為每個批次裡面的資料長度必須相同才能進行訓練：
    ```python
    # 計算最大長度
    maxlen = max(map(len, ds_tokens))
    print(f"Max Length: {maxlen}")

    # 對資料集進行填充
    dataset = list()
    for tokens in ds_tokens:
        delta = maxlen - len(tokens)

        # 將 EOS Token 當作 PAD Token 來用
        tokens += [tk.eos_token_id] * delta

        # 訓練用的 input_ids 與 labels 通常是完全一樣的序列
        dataset.append({"input_ids": tokens, "labels": tokens})

    # 確認所有序列的長度都是一致的
    for item in dataset:
        assert len(item["input_ids"]) == maxlen
        assert len(item["labels"]) == maxlen

    # 將斷詞完的結果存下來
    dump_json_gz(dataset, f"{ds_type}.tokens.json.gz")
    ```
    * 完成填充後，就可以把斷詞完的結果存下來了。
* 注意：
    * 還記得之前在講推論的時候有提到，必須幫分詞器設定 `padding_side="left"` 才能正常生成嗎？但是這次在準備訓練資料的時候，填充其實是放在右邊的喔！
    * 這是因為推論時只關注最右邊的 Token，因此不能把無意義的填充放在右邊。
    * 但是模型在訓練時，主要是在學習如何從左到右的生成。因此進行推論與訓練時的填充方向並不相同，這點需要多加注意！
* 最後將 `ds_type` 分別設定為 `train` 與 `dev` 各跑一次，就完成訓練集與驗證集的處理啦！
* 補充：
    * 在資料集裡面所謂的**訓練集 (Train)**、**驗證集 (Validation)**、**開發集  (Dev)** 跟**測試集 (Test)** 差在哪裡呢？
        * 訓練集：
          就是用來訓練模型的主要資料集，在訓練過程中，**不會拿訓練集以外的資料來更新權重**。
        * 驗證集：
          概念與開發集是一樣的，用來在訓練過程中驗證訓練效果，但是不會拿來更新權重。
        * 測試集：
          完全不參與任何訓練過程，只會在完成訓練之後評估最終模型的效果。
    * 這些資料集對驗證訓練方法、調整參數或模型權重等都非常重要，考驗開發人員如何切分難度足夠均勻、足以驗證模型的資料。
    * 同時也要避免資料**洩漏 (Data Leakage)**，不小心使測試資料跑進訓練資料裡面，會導致嚴終評估過於美化模型的問題。

---

## 訓練流程
接下來開始訓練模型的流程。
* 首先將模型與分詞器讀取進來：
    ```python
    import torch
    from transformers import LlamaForCausalLM as ModelCls
    from transformers import LlamaTokenizerFast as TkCls

    # 讀取 Model & Tokenizer
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    model: ModelCls = ModelCls.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )
    tk: TkCls = TkCls.from_pretrained(model_id)
    ```
    * 之前讀取模型時可能會加上量化相關的參數，但目前是不能直接對量化模型進行訓練的！
    * 因此這邊使用 `torch.bfloat16` 資料型態，俗稱 BF16，與一般的 FP16 不同的地方在於 BF16 可以表達的數值範圍較廣，但是小數點後的精準度較低，所以 BF16 的**廣度較高**但**精度較低**，相較於 FP32 而言也能節省記憶體。
* 資料型態 FP16 與 BF16 的格式差異：

    **(中文版)**
    ![資料型態 FP16 與 BF16 的格式差異](images/資料型態%20FP16%20與%20BF16%20的格式差異.png)
    
    [**NVIDIA Blog**](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)
    ![資料型態 FP16 與 BF16 的格式差異 (NVIDIA Blog)](images/資料型態%20FP16%20與%20BF16%20的格式差異%20(NVIDIA%20Blog).png)

* 對於看似要求精準度的深度學習模型而言，使用 BF16 有什麼好處呢？
    * 在訓練結構相對深層複雜且參數量龐大的模型時，剛開始訓練時可能會產生很大的梯度，這個梯度甚至可能大到 FP16 都放不下，最後因為溢位導致 Loss 變成 **NaN (Not a Number)**。
    * 但如果改用 BF16 的話，因為可以表達的數值範圍更廣，所以比較不容易發生這樣的情況。
    * 話雖這麼說，但其實選擇 BF16 是個滿自然的過程：
        1. FP32 塞不進 GPU 記憶體
        2. INT8 以下不能直接訓練
        3. FP16 的 Loss 會 NaN 炸開
        * 那就只剩下 BF16 可以用了，但是換個角度想，至少還有 BF16 可以用。
* 其實在參數量只有 1B 的情況下，的確可以將 FP32 的模型權重放進一張 24 GB 記憶體的 GPU裡面，但是開始計算梯度時就會爆開了。
* 因此將模型權重放進 GPU 裡面通常不是問題，**處理梯度的記憶體消耗才是關鍵**。
* 接下來讀取資料集：
    ```python
    import datasets

    dataset = datasets.load_dataset(
        "json",
        data_files = {
            "train": "train.tokens.json",
            "dev": "dev.tokens.json",
        },
        cache_dir="cache",
    )
    ```
    * 這會將資料集讀取成一個類似字典的物件，可以稍微檢查一下他的內容：
    ```python
    for data in dataset["train"]:
        print(data["input_ids"])
        print(data["labels"])
    ```
    * 剛開始在學習訓練模型時，經常遇到訓練效果不佳的問題，結果回頭一檢查發現資料格式根本都不對，所以訓練之前一定要仔細檢查資料內容喔！
* 注意：
    * 將 `cache_dir` 設定為 `"cache"` 時，會在工作目錄底下產生一個 `cache` 資料夾用來存放快取檔案。
    * 有時候就算更新了資料集，套件也會判定你沒改，導致你以為你用了新的資料格式訓練，但其實根本沒有。
    * 這時就可以手動把 `cache` 資料夾刪掉來強制套件重新快取。
* 接下來設定訓練參數：
    ```python
    from transformers import TrainingArguments

    # Supervised Fine-Tuning
    output_dir = "Models/TwAddr-SFT"
    train_args = TrainingArguments(
        output_dir,
        per_device_train_batch_size=4,
        evaluation_strategy="epoch",
    )
    ```
    * 設定 `evaluation_strategy="epoch"` 指定每個**訓練週期 (Epoch)** 結束後就進行一次驗證評估。
    * 在這個簡單的玩具任務裡面，只需要設定這幾個參數即可。
    * 但其實還有相當多訓練參數可以設定，以下介紹一些常用的參數：
        * `per_device_train_batch_size` 與 `per_device_eval_batch_size` 分別設定訓練與評估時的批次大小，是最主要影響記憶體用量的參數。
        * 通常單卡在訓練LLM時批次大小都沒辦法開很大，這樣其實容易造成訓練效果不好。這時就需要搭配 `gradient_accumulation_steps` 參數，可以設定每累積幾個 Step 的梯度再計算一次反向傳播。
        * 假設批次大小為 4 而累積步數為 8，那就會在訓練 `4 * 8 = 32` 筆訓練資料後再做一次反向梯度，這樣可以達到接近批次大小設定為 32 的效果。
        * 有時候一個訓練週期需要花相當多步數來進行，因此可以設定 `evaluation_ strategy="steps"` 來提高評估的頻率，並搭配 `eval_steps` 參數來決定每過幾步要做一次評估。
        * 透過 `save_strategy="steps"` 與 `save_steps` 來指定每幾步要存一個檢查點，若 `save_strategy` 設定為 `"epoch"` 時則是每個訓練週期都會存一個檢查點。
        * 啟用檢查點時，會在輸出目錄底下產生類似 `checkpoint-25`、`checkpoint-50` 、`checkpoint-75` 之類的資料夾，每個資料夾裡面都會存一份完整的模型權重在裡面。如果訓練意外中斷時，可以從這些檢查點來恢復訓練狀態。
        * 除此之外，使用驗證開發資料集除了用來監測模型有沒有訓練壞掉以外，也能用來尋找模型效果的最佳點。
        * 搭配參數 `load_best_model_at_end=True` 使用，可以在訓練結束後，將評估結果最佳的模型讀取出來。
        * 另外，為了避免硬碟被這些檢查點塞爆，可以設定 `save_total_limit` 來決定只保留最近或最佳的幾個檢查點。
        * 還有 `learning_rate` 學習率參數，用來調整權重每次更新的幅度，通常參數量越大，學習率會設定的越低。
        * 而 `optim` 參數則可以選擇 Optimizer 的種類。
            * 至於有哪些 Optimizer 可以使用呢？可以在 `OptimizerNames` 裡面查看：
            ```python
            from transformers.training_args import OptimizerNames
            ```
            * 在 VSCode 裡面，把鍵盤游標移到 `OptimizerNames` 上面後，按下 F12 可以查看類別底下有哪些定義：
            ```python
            # site-packages/transformers/training_args.py
            
            class OptimizerNames(ExplicitEnum):
                ADAMY_HF = "adamw_hf"
                ADAMW_TORCH = "adamw_torch"
                ADAMW_TORCH_FUSED = "adamw_torch_fused”
                …            
            ```
            * 雖然選擇很多，但是通常只要用預設的 `"adamw_torch"` 就可以囉！
* 最後這些阿里阿雜的參數堆疊起來後，可能就會長成這樣：
    ```python
    train_args = TrainingArguments(
        output_dir,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=2,
        evaluation_strategy="steps",
        save_strategy="steps",
        eval_steps=25,
        save_steps=25,
        save_total_limit=3,
        num_train_epochs=3,
        learning_rate=5e-5,
        load_best_model_at_end=True,
    )
    ```
    * 雖然看起來很多很雜，一開始可能不知道怎麼設定，但是針對自己的使用情況多多嘗試，就會漸漸熟悉各個參數的設定訣竅囉！
    * 好的框架往往都會有很多參數提供開發者彈性調整，但太多的參數可以設定反而容易讓初學者心生畏懼或者無所適從，因此是否有完整的文件與適當的教學指南便相當重要！
* 決定好參數之後，就可以開始訓練模型了！
    ```python
    from transformers import Trainer

    # 開始訓練模型
    trainer = Trainer(
        model=model,
        args=train_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["dev"],
    )
    trainer.train()

    # 儲存訓練完的模型
    trainer.save_model()

    # 也另外存一份分詞器方便評估
    tk.save_pretrained(output_dir)
    ```
    * 在一張 RTX 3090 上不用幾秒鐘就可以完成訓練了，訓練結束之後，別忘了把模型跟分詞器都存下來，方便後續的評估驗證。

---

## 評估測試
評估測試是模型訓練中相當重要的一環，用來確保模型並沒有被訓練壞，而且有呈現預期的訓練效果。
* 這個環節分成固定測試集與自由輸入測試兩種。
* 可以借助 vLLM 的力量，對模型進行有效率的大量評估：
    ```python
    from vllm import LLM, SamplingParams

    # 讀取模型
    model_id = "TinyLlama/TinyLlana-1.1B-Chat-v1.0"
    llm = LLM(model_id, dtype="float16")
    tk = llm.get_tokenizer()

    # 建立測試集的提示列表
    prompts, items = list(), list()
    for out, ans in iter_dataset("test.json"):
        prompt = tk.apply_chat_template(
            [dict(role="user", content=template.format(out))],
            add_generation_prompt=True,
            tokenize=False,
        )
        prompts.append(prompt)
        items.append(ans)

    # temperature 設為 0.0 為 Greedy Decode
    # 確保每次實驗的結果都是一樣的
    sampling_params = SamplingParams(
        max_tokens=512,
        temperature=0.0,
        stop=["}"],
    )

    # 對所有 Prompt 同時進行推論
    outputs = llm.generate(prompts, sampling_params)

    # 評估生成結果
    results = list()
    for out, ans in zip(outputs, items):
        out = out.outputs[0].text

        # 尋找 JSON 段落
        if "{" in out:
            i = out.index("{")
            out = out[i:] + "}"

        # 解析成字典
        try:
            ans = json.loads(ans)
            out = json.loads(out)
        except:
           pass

        results.append(out == ans)

        # 觀察錯誤輸出
        if out != ans:
            print(ans, out)

    # 輸出準確率
    accuracy = sum(results) / len(results)
    print(f"Accuracy: {accuracy:.2%}")
    ```
    * 先來評測看看原本的 `TinyLlama/TinyLlama-1.1B-Chat-v1.0` 準確率是多少：
    ```md
    Accuracy: 71.50%
    ```
        * 在進行微調前，準確率約七成，觀察模型的輸出，很多都是答非所問。
    * 接下來看看微調過後的模型效果如何：
    ```md
    Accuracy: 98.33%
    ```
        * 登登登，效果大幅提昇。
    * 試試看，如果將提示裡面的「臺北市中正區八德路」範例移除會怎樣呢？
    ```python
    template="請將以下路名解析為 JSON 格式。\n\n輸入：{}"
    ```
    評測出來的準確率：
    ```md
    Accuracy: 98.00%
    ```
    * 很顯然我們的微調讓模型留下了一個刻板印象：
        * 如果要將路名解析成 JSON 格式，則必定是這種 `city-town-road` 的鍵值組合，而不會是什麼 `district` 或 `street` 之類的。
* 這樣的結果到底是好還是壞呢？
    * 在一些比較積極強烈的特殊領域訓練裡面，通常是不太在意領域外能力下降的問題，但也不能破壞的太誇張。
    * 因此能用自由測試的評估方式，來看看模型原本的能力是否還健在：
    ```python
    from vllm import LLM, SamplingParams

    model_name = "Models/TwAddr-SFT"
    llm = LLM(model_name)
    tk = llm.get_tokenizer()

    sampling_params = SamplingParams(
        max_tokens=512,
        temperature=0.0,
    )

    while True:
        message = input(" > ")
        prompt = tk.apply_chat_template(
            [dict(role="user", content=message)],
            add_generation_prompt=True,
            tokenize=False,
        )
        outputs = llm.generate(
            [prompt],
            sampling_params,
            use_tqdm=False,
        )
        print(outputs[0].outputs[0].text)
    ```
    * 測試結果如下：
    ```md
    > 你好啊
    {"user":"臺北市中正區八德路","assistant":"臺北市中正區八德路"}
    > 什麼是語言模型
    {"user":"臺北市中正區八德路", "assistant":"臺北市中正區八德路"}
    ```
* 完蛋了，我們的模型滿腦子只剩下解析路名了！這就是模型訓練中惡名昭彰的**過度擬合 (Overfitting**)，模型因為過度學習，導致他只認得訓練任務的東西。
    * 如果要解決過度擬合，在這個玩具任務裡面可以嘗試減少訓練資料量，因為訓練資料都是單一格式，這樣的重複性資料對模型來說是比較沒有幫助的。
    * 更理想的解決辦法是**加入更多元的訓練資料**，例如打招呼、基本問答等等。
    * 因此訓練語言模型的核心思維就是：**減少重複性，增加多元性！**
    * 除了從資料量下手以外，也可以從參數量下手，例如只調整模型的最後一層：
    ```python
    # 先凍結所有權重
    model.requires_grad_(False)
    # 只更新最後一層的權重
    model.model.layers[-1].requires_grad_(True)

    trainer = Trainer(model, ...)
    trainer.train()
    ```
    * 但是這樣的方法在面臨複雜的任務時，學習效果往往不佳。
    * 當模型的某個能力有所成長時，經常會使另外一個能力有所消退。
* 因此微調這門學問更多時候是在探討，如何在**訓練模型學習新能力之餘，盡可能維持住原本既有的能力**。

---

## 連結
* [政府資料開放平台](https://data.gov.tw/)
* [TinyLlama 1.1B](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.3)
* [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)
* [HF Docs: Floating Data Types](https://huggingface.co/docs/transformers/v4.15.0/performance#floating-data-types)

---

## 結論
* 本章節帶大家瞭解了如何微調一個語言模型，雖然只是個小規模的實驗，但能讓我們更加熟悉資料處理的流程。而且訓練速度相對快，能縮短實驗的週期，並快速的嘗試各種不同的參數，也比較容易累積一些基礎的訓練經驗。
* 不過這次只有訓練 TinyLlama 1B 模型，但現在外面的 LLM 少說都 7B、13B 起跳啊！為什麼不來訓練參數量更大一點的模型呢？
    * 於是將目標模型改為 Llama 38B 並嘗試訓練：
        ```md
        torch.cuda.OutOfMemoryError: CUDA out of memory.
        ```
    * 搭拉，記憶體不足！
* 本章節介紹的訓練方法為**全參數微調 (Full Fine-Tuning, FFT)**，是最傳統的訓練方法，但也因為所有的參數都需要調整，所以是訓練成本最高、記憶體吃最兇的方法。
* 而且 Transformers 架構隨著訓練資料的長度越長，GPU 記憶體的消耗還會平方成長上去。
    * 即便這份路名資料集的長度並不是很長，但是在單張 24GB 的 GPU 上依然無法進行訓練，更不用說是長度兩千、四千以上的資料集了。
* 雖然章節末段提到了只訓練少量層數的方法，但是對於複雜一點的任務而言，只訓練一點點的權重是不夠的。
    * 不過不用擔心，那個神秘的笑臉將再度出手，拯救我們這些單顯卡的貧民玩家們！

---