# é–‹æºçš„ Hugging Face Transformers
[Hugging Face](https://huggingface.co/) ğŸ¤— [Transformers](https://huggingface.co/docs/transformers/index) æ˜¯è¨“ç·´ Transformer æ¨¡å‹æœ€çŸ¥åçš„å¥—ä»¶æ²’æœ‰ä¹‹ä¸€ï¼Œæ­¤å¥—ä»¶æ”¶å…¥äº†è¨±å¤šçŸ¥åæ¨¡å‹æ¶æ§‹ã€è¨“ç·´æ¼”ç®—æ³•ä»¥åŠå„ç¨®æ¨¡å‹æ¬Šé‡çš„åˆ†äº«ï¼Œä½¿é–‹ç™¼è€…å¯ä»¥è¼•é¬†é§•é¦­é€™äº›ç†è«–è¤‡é›œçš„æ“ä½œã€‚
* Transformers æ˜¯å€‹è‘—é‡åœ¨è¨“ç·´çš„æ¡†æ¶ï¼Œä½†ä¹Ÿæ”¯æ´äº†ç›¸ç•¶è±å¯Œçš„æ¨ç†åŠŸèƒ½ã€‚
    * ä»¥ä¸‹å°‡æœƒè‘—é‡åœ¨å¦‚ä½•ä½¿ç”¨æ­¤æ¡†æ¶é€²è¡Œ LLM çš„æ¨è«–ï¼Œä¸¦å‰–æ Transformer Decoder é€²è¡Œ Autoregressive çš„æµç¨‹ã€‚

---

## å®‰è£
Transformers æ”¯æ´çš„å¾Œç«¯ ML æ¡†æ¶åŒ…å« PyTorch, Tensorflow, JAX ç­‰ç­‰ã€‚
* ç­†è€…æ˜¯ä»¥ PyTorch ç‚ºä¸»ï¼š
    ```bash
    # å®‰è£æ–¹å¼ä»¥å®˜ç¶²ç‚ºä¸»ï¼š https://pytorch.org/get-started/previous-versions/
    conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
    pip install transformers
    ```
* ä½¿ç”¨ Transformers å¥—ä»¶æ™‚ï¼Œä¹Ÿæ¨è–¦å®‰è£ä»¥ä¸‹é¡å¤–çš„å¥—ä»¶é€²ä¸€æ­¥æå‡é‹ä½œæ•ˆç‡ã€‚
    ```bash
    pip install accelerate bitsandbytes optimum
    ```
    é€™äº›å¥—ä»¶èƒ½å¤ è®“æˆ‘å€‘ä½¿ç”¨é‡åŒ– (Quantization) æ©Ÿåˆ¶ä¾†æ¸›å°‘ GPU è¨˜æ†¶é«”æ¶ˆè€—ã€‚

---

## è®€å–æ¨¡å‹
æ–¹å¼æœ‰å¾ˆå¤šç¨®ã€‚
* æœ€å¸¸è¦‹çš„æ˜¯ä½¿ç”¨ `.from_pretrained` è®€å–ï¼š
    * ä½¿ç”¨ TheBloke å¤§ç¥æä¾›çš„ [Llama-2 7B Chat](https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16) ç‚ºä¾‹ï¼š
        ```python
        from transformers import BitsAndBytesConfig
        from transformers import LlamaForCausalLM

        model_path = "TheBloke/Llama-2-7b-chat-fp16"
        
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,    # ç´„éœ€ 8 GiB GPU è¨˜æ†¶é«”
            # load_in_4bit=True,  # ç´„éœ€ 6 GiB GPU è¨˜æ†¶é«”
        )
        
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            low_cpu_mem_usage=True,
            quantization_config=bnb_config,
        )
        ```
        * åœ¨ Colab é€™ç¨®è³‡æºç›¸å°æœ‰é™çš„ç’°å¢ƒä¸­ï¼Œéœ€è¦ä½¿ç”¨**é‡åŒ– (Quantization)** æŠ€è¡“ä¾†æ¸›å°‘ GPU è¨˜æ†¶é«”çš„æ¶ˆè€—
        * é€é `BitsAndBytesConfig` å¯ä»¥è¼•é¬†åœ°å°‡æ¨¡å‹é‡åŒ– 8 åˆ° 4 ä½å…ƒã€‚
        * ç‚ºäº†å•Ÿç”¨é‡åŒ–åŠŸèƒ½ï¼Œéœ€è¦å°‡ `device_map` è¨­å®šç‚º `"auto"`ï¼Œä»¥ç¢ºä¿æ¨¡å‹æ¬Šé‡éƒ½æ”¾åœ¨ GPU è£¡é¢ã€‚
        * å¦‚æœ CPU è¨˜æ†¶é«”ä¹Ÿä¸æ˜¯å¾ˆå¤ çš„è©±ï¼Œé‚£å°±éœ€è¦å°‡ `low_cpu_mem_usage` è¨­å®šç‚º `True` æ‰èƒ½é †åˆ©è®€å–æ¨¡å‹ã€‚
    * ä»¥ä¸Šæ­¥é©Ÿå¯¦éš›ä¸Šæœƒå¾ [Hugging Face Hub](https://huggingface.co/models) (HF Hub)ä¸‹è¼‰æ¨¡å‹çš„æ¬Šé‡ï¼Œé è¨­æœƒæ”¾åœ¨ `~/.cache/huggingface/` è£¡é¢ï¼Œä¹Ÿå¯ä»¥é€é`cache_dir` åƒæ•¸æŒ‡å®šé€™äº›æª”æ¡ˆè¦å­˜æ”¾åœ¨å“ªè£¡ï¼š
        ```python
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            cache_dir="./cache_models"
        )
        ```
    * å¤šæ•¸æ¨¡å‹éƒ½å¯ä»¥åœ¨ HF Hub ä¸­æ‰¾åˆ°ï¼ŒHF Hub ä¸»è¦ä½¿ç”¨ Git åšç®¡ç†çš„ï¼Œæ‰€ä»¥ä¹Ÿèƒ½ç›´æ¥ç”¨ `git clone` å°‡æ¨¡å‹è¤‡è£½ä¸‹ä¾†ï¼Œåœ¨æ­¤å‰æœƒéœ€è¦å…ˆå®‰è£ Git LFS æ¨¡çµ„ï¼š
        ```bash
        sudo apt install -y git git-lfs
        
        # å•Ÿç”¨ Git LFS åŠŸèƒ½
        git lfs install
        
        # ä¸‹è¼‰æ¨¡å‹
        git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16
        ```
    * ä¸‹è¼‰å®Œæˆå¾Œï¼Œç›´æ¥æŒ‡å®šè³‡æ–™å¤¾è·¯å¾‘åç¨±å³å¯è®€å–æ¨¡å‹ï¼š
        ```bash
        LlamaForCausalLM.from_pretrained("Llama-2-7b-chat-fp16")
        ```
    * Git å‚³è¼¸å®Œæˆå¤§å‹æª”æ¡ˆæ™‚ä¸¦ä¸æœƒé¡¯ç¤ºä¸‹è¼‰åœ–ç´™ï¼Œä½†å¦‚æœä½ çœŸçš„å¾ˆæƒ³çœ‹ç¶²é€Ÿåœ–ç´™ç´™æ¢ï¼Œå…¶å¯¦ä¹Ÿæœ‰è¾¦æ³•çš„ï¼š
        ```bash
        # è·³éæ‰€æœ‰ LFS æª”æ¡ˆçš„ä¸‹è¼‰
        GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16
        cd Llama-2-7b-chat-fp16

        # é€é Git LFS é€²è¡Œ Fetch
        git lfs fetch
        git lfs checkout
        ```
        é™¤éæ‡·ç–‘ç¶²è·¯é€Ÿåº¦ç”¢ç”Ÿçš„å•é¡Œï¼Œå¦å‰‡é€šå¸¸ä¸å¤ªéœ€è¦å°æ–¹ã€‚
* å¤§éƒ¨åˆ†æ¨¡å‹æ¬Šé‡ä¸Šå‚³ä¹‹å¾Œå°±ä¸æœƒåœ¨è®Šå‹•äº†ï¼Œä½† Git çš„æ©Ÿåˆ¶æœƒè®“ä¸‹è¼‰ä¸‹ä¾†çš„æ¨¡å‹åŒ…å«ä¸€ä»½é¡å¤–è¤‡è£½çš„æ¬Šé‡åšå‚™ä»½ï¼Œå› æ­¤ 10 GB çš„æ¨¡å‹å¯èƒ½æœƒåƒæ‰ 20 GB çš„ç¡¬ç¢Ÿç©ºé–“ã€‚
    * ä¸€èˆ¬æƒ…æ³ä¸‹å¯ä»¥ç›´æ¥æŠŠæ¬Šé‡è³‡æ–™å¤¾ä¸­çš„ `.git` è³‡æ–™å¤¾åˆªé™¤ä»¥ç¯€çœç¡¬ç¢Ÿç©ºé–“ï¼Œæœ‰éœ€è¦å†ä¸‹è¼‰å›ä¾†å°±å¥½ã€‚ 
* æœ‰æ™‚å¯èƒ½æœƒé‡åˆ°æœ‰å­˜å–é™åˆ¶çš„æ¨¡å‹ (Gated Model)ã€‚
    * ä¾‹å¦‚ï¼Œ Meta å®˜æ–¹çš„ Llama ç³»åˆ—å°±æœ‰å­˜å–é™åˆ¶ã€‚
    * æ­¤æ™‚è¦å…ˆå»è©²æ¨¡å‹ä»‹ç´¹é é¢å¡«å¯«å­˜å–è¡¨å–®ã€‚
    * æœ‰äº›æ¨¡å‹å¡«å®Œè¡¨å–®å°±èƒ½é¦¬ä¸Šå­˜å–äº†ï¼Œä½†åƒ Meta Llama å°±æœƒéœ€è¦ç­‰å¾…ä¸€æ®µæ™‚é–“æ‰æœƒæ”¶åˆ°æ ¸å‡†é€šçŸ¥ã€‚
* æœ‰å­˜å–é™åˆ¶çš„æ¨¡å‹ï¼Œæ ¸å‡†å¾Œå°±å¯ä»¥ä¸‹è¼‰æ¨¡å‹ï¼Œéœ€è¦ä¸€å®ƒæ–¹å¼æ‰èƒ½ä¸‹è¼‰æ¨¡å‹ã€‚
    1. è¨­å®š SSH é‡‘é‘°ï¼Œç„¶å¾Œé€é SSH ä¸‹è¼‰ï¼š
        ```bash
        git clone git@hf.co:meta-llama/Meta-Llama-3-8B
        ```
    2. é€é `huggingface-cli` ç™»å…¥ï¼š
        * å‰å¾€ HF å¸³è™Ÿè¨­å®šçš„ Access Tokens é é¢ï¼Œä¸¦å»ºç«‹ä¸€å€‹æ–°çš„ Tokenã€‚
            * å¦‚æœåªè¦ä¸‹è¼‰æ¨¡å‹çš„è©±ï¼Œæ¬Šé™åªè¦è¨­å®š Read å°±å¥½ã€‚
        * ä½¿ç”¨å»ºç«‹å¥½çš„ Access Token ç™»å…¥ï¼š
        ```bash
        huggingface-cli login
        # è²¼ä¸Š å»ºç«‹å¥½çš„ Access Token
        ```
        * ä¸‹è¼‰æ¨¡å‹ï¼š
        ```bash
        huggingface-cli download meta-llama/Meta-Llama-3-8B \
            --local-dir Models/Meta-Llama-3-8B
        ```
* [TheBloke](https://huggingface.co/TheBloke) æ˜¯ Hugging Face ç¤¾ç¾¤ç›¸ç•¶æ´»èºçš„ä¸€å€‹ç”¨æˆ¶ï¼Œæ¯æ¬¡æœ‰å¾ˆçŒ›çš„æ¨¡å‹é‡‹å‡ºæ™‚ï¼Œä»–å°±æœƒæ‰‹åˆ€è¡ç¬¬ä¸€å€‹å¹«å¤§å®¶æŠŠæ¨¡å‹çš„æ ¼å¼è½‰æ›æˆ HF æ”¯æ´çš„æ ¼å¼ï¼Œé€²è¡Œå„ç¨®é‡åŒ–å£“ç¸®å¾Œä¸Šå‚³ã€‚æ“šèªªä»–çš„[ç¶²è·¯ç’°å¢ƒé”æ•¸ GB/s](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1640712066) ä¹‹å¿«ï¼Œæ˜¯å€‹éå¸¸ç†±å¿ƒçš„å¤§ç¥ã€‚
* ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼
    * [å®Œæ•´ç¨‹å¼ç¢¼](https://colab.research.google.com/drive/1wezFUpFlBXK-AwUjjd0vBl50qnjOwq6e?hl=en)
    * è‡ªå·±å˜—è©¦: .../LLM/project/hugging_face_transformers

---

## è®€å–åˆ†è©å™¨
Tokenizer çš„è®€å–æ–¹å¼ï¼Œèˆ‡æ¨¡å‹è®€å–æ–¹å¼å·®ä¸å¤šï¼Œé€šå¸¸ Tokenizer æœƒè·Ÿæ¨¡å‹æ¬Šé‡çš„å°ˆæ¡ˆæ”¾åœ¨ä¸€èµ·ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ç›¸åŒçš„å‘½åç©ºé–“ä¸‹è¼‰å³å¯ã€‚
```python
from transformers import LlamaTokenizer as TkCls

tk: TkCls = TkCls.from_pretrained("TheBloke/Llama-2-7b-chat-fp16")
```
* ä¹Ÿå¯ä»¥ä½¿ç”¨é¡ä¼¼çš„ `LlamaTokenizerFast` å¿«é€Ÿé¡åˆ¥ã€‚
    * ä¸€èˆ¬ç‰ˆæœ¬çš„ Tokenizer ä½¿ç”¨ Python å¯¦åš
    * å¿«é€Ÿç‰ˆæœ¬çš„ Tokenizer ä½¿ç”¨ Rust å¯¦åšã€‚
    * å¦‚æœè¦å°å¤§é‡é•·æ–‡æœ¬é€²è¡Œåˆ†è©çš„è©±ï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬ï¼Œä½†å¦‚æœåªæ˜¯è¦è™•ç†å¹¾å€‹çŸ­æ–‡æœ¬çš„è©±å°±ä¸éœ€è¦ã€‚
* åœ¨èˆŠç‰ˆçš„ Transformers è£¡é¢ï¼Œæœ‰äº›å¿«é€Ÿç‰ˆçš„ Tokenizer éœ€è¦åˆå§‹åŒ–éå¸¸ä¹…ã€‚
    * è§£æ±ºæ–¹æ³•é™¤äº†æ”¹ç”¨ä¸€èˆ¬ç‰ˆæœ¬ä»¥å¤–ï¼Œä¹Ÿå¯ä»¥å°‡ Tokenizer è®€å‡ºä¾†ä¹‹å¾Œï¼Œè½‰å­˜å¿«é€Ÿç‰ˆæœ¬ï¼š
    ```python
    from transformers import LlamaTokenizerFast as TkCls

    tk: TkCls = TkCls.from_pretrained("/path/to/tokenizer")
    tk.save_pretrained("/path/to/tokenizer")
    ```
    é€™å€‹å•é¡Œåœ¨æœ€æ–°ç‰ˆçš„ Transformers è£¡å·²ç¶“è¢«ä¿®æ”¹äº†

---

## è‡ªå‹•é¡åˆ¥ (Auto Class)
å·²ç¶“çŸ¥é“ Llama 2 ä½¿ç”¨çš„æ˜¯ Llama æ¶æ§‹ï¼Œæ‰€ä»¥ç›´æ¥å‘¼å« `LlamaForCausalLM` èˆ‡ `LlamaTokenizer` é€™å…©å€‹é¡åˆ¥ã€‚ä½†æœ‰äº›æ¨¡å‹èˆ‡ Tokenizer ç¬¬ä¸€æ™‚é–“å¯èƒ½ç„¡æ³•åˆ¤åˆ¥çš„ä½¿ç”¨ä»€éº¼æ¶æ§‹ï¼Œæˆ–æ˜¯æƒ³å¯«å‡ºç›¸å®¹æ€§æ›´å»£æ³›çš„æ–¹æ¡ˆç¢¼ï¼Œé‚£å°±å¯ä»¥ä½¿ç”¨ Transformers çš„ Auto Class ä¾†é€²è¡Œè®€å–ã€‚
* åˆ¤åˆ¥çš„ä½¿ç”¨æ¶æ§‹ï¼š
    ```python
    from transformers import AutoModelForCausalLM as ModelCls
    from transformers import AutoTokenizer as TkCls

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelCls.from_pretrained(model_path)
    tk: TkCls = TkCls.from_pretrained(model_path)
    ```
    å°å‡ºå¯¦éš›é¡åˆ¥ï¼Œç™¼ç¾é‚„æ˜¯å…·é«”çš„ Llama é¡åˆ¥ï¼š
    ```python
    print(type(model))  # <class 'transformers...LlamaForCausalLM'>
    print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>
    ```
    å¦‚æœä¸æƒ³ç”¨å¿«é€Ÿç‰ˆæœ¬çš„ Tokenizer å¯ä»¥æŠŠ `use_fast` åƒæ•¸è¨­å®šæˆ Falseï¼š
    ```python
    tk: TkCls = TkCls.from_pretrained(model_path, use_fast=False)
    print(type(tk))     # <class 'transformers...LlamaTokenizer'>
    ```
* ä½† Auto Class çš„ç¼ºé»æ˜¯**å‹åˆ¥æç¤º (Tyoe Hint)** æ¯”è¼ƒå°‘ï¼Œå°±æ˜¯åœ¨å¯«ç¨‹å¼æ™‚ VSCode æœƒè‡ªå·±è·³å‡ºçš„è‡ªå‹•å®Œæˆæç¤ºæ¸…å–®ã€‚
![RkddTFp](https://hackmd.io/_uploads/r1O0W1Mbxe.png =500x)
  å¦‚æœéå¸¸ä»°è³´ Type Hint çš„é–‹ç™¼è€…ï¼Œå¯èƒ½è¦è‡ªå·±åš Type Annotation ä¾†è§£ã€‚
* å…¶å¯¦å¤§éƒ¨åˆ†çš„æ–¹æ³•éƒ½ä¾†è‡ª `PreTrainedModel` èˆ‡ `PreTrainedTokenizer` é€™å…©å€‹å‹åˆ¥ï¼Œå› æ­¤å¯ä»¥çµ¦å‡ºå‹åˆ¥è¨»è¨˜ï¼š(åˆ¥å)
    ```python
    from transformers import AutoModelForCausalLM as ModelImp
    from transformers import AutoTokenizer as TkImp
    from transformers import PreTrainedModel as ModelCls
    from transformers import PreTrainedTokenizer as TkCls

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelImp.from_pretrained(model_path)
    tk: TkCls = TkImp.from_pretrained(model_path)
    ```
    è‡ªå‹•å®Œæˆåˆå›ä¾†å•¦ï¼
    * æœ€æ–°ç‰ˆçš„ Transformers å¥—ä»¶ä¸­ï¼Œ`AutoTokenizer.from_pretrained()` å·²ç¶“æœƒè‡ªå‹•æŠŠå›å‚³ç‰©ä»¶æ¨™è¨˜ç‚º `PreTrainedTokenizer` å‹åˆ¥ï¼Œæ‰€ä»¥åˆ†è©å™¨çš„éƒ¨åˆ†å°±ä¸ä¸€å®šç…§ä»¥ä¸Šæ”¾å¼åšäº†ã€‚

---

## æ–‡æœ¬ç”Ÿæˆ (Text Generation)
* éœ€è¦å…ˆå°‡è¼¸å…¥å­—å…ƒ Token ä¸¦è½‰æ›æˆ PyTorch Tensor ç‰©ä»¶ï¼š
    ```python
    model_path = "Llama-2-7b-chat-fp16"
    tk: TkCls = TkCls.from_pretrained(model_path)
    tokens = tk("Hello, ", return_tensors="pt")
    print(tokens) # Tokenize çµæœ
    """
    Output:
    {
        "input_ids": tensor([[1, 15043, 29892, 29871]]),
        "attention_mask": tensor([[1, 1, 1, 1]]),
    }
    """
    ```
    * Tensor çš„å­—å…¸ï¼š
        * `input_ids`: åˆ†è©çš„çµæœã€‚
        * `attention_mask`: ç”¨ä¾†é®ç½©éè¼¸å…¥éƒ¨åˆ†ã€‚
* ä¸€é–‹å§‹åªéœ€è¦é—œæ³¨ `input_ids` ï¼Œå…ˆæŠŠ `attention_mask` ç•¶æˆæ¨¡å‹çš„å¿…è¦è¼¸å…¥ï¼š
    ```python
    input_ids = tokens["input_ids"].to("cuda")
    print(input_ids)
    # tensor([[    1, 15043, 29892, 29871]], device='cuda:0')
    ```
    * æ¨¡å‹èˆ‡ Tensor å¿…é ˆæ”¾åœ¨åŒä¸€å€‹è£ç½®ä¸­æ‰èƒ½åšé‹ç®—ã€‚
        * ä¾‹å¦‚ï¼Œæ”¾åœ¨ CPU ä¸Šçš„æ¨¡å‹åªèƒ½è·Ÿ CPU ä¸Šçš„ Tensor é‹ç®—ã€‚
        * é€™è£¡æ¨¡å‹æ”¾åœ¨ GPU ä¸Šï¼Œæ‰€ä»¥è¦ç”¨ `.to("cuda")` å°‡æ•´å€‹åˆ†è©æ”¾é€² GPU è£¡é¢ã€‚
* é–‹å§‹ä½¿ç”¨ `model.generate` é€²è¡Œæ–‡å­—ç”¢ç”Ÿï¼š
    ```python
    output = model.generate(input_ids, max_new_tokens=32)
    print(tk.batch_decode(output))
    # <s> Hello, I am a beginner in Python ...
    ```
    * `model.generate` æœƒå›å‚³ç”¢ç”Ÿçš„ Token IDï¼Œå¿…é ˆç”¨ Tokenizer é€²è¡Œ Decode æ‰èƒ½å¾—åˆ°æ–‡å­—ç‰ˆçš„è¼¸å‡ºã€‚
    * `max_new_tokens` æ˜¯ç”¨ä¾†è¨­å®šè¦è¼¸å‡ºçš„ Token æ•¸é‡ã€‚
        * åœ¨ Transformer Decoder è£¡é¢ï¼Œè¼¸å‡ºçš„Tokenæ•¸é‡è¶Šå¤šï¼Œä½”ç”¨çš„GPUè¨˜æ†¶é«”å°±æœƒè¶Šå¤šï¼Œç”Ÿæˆéœ€è¦çš„æ™‚é–“ç†æ‰€ç•¶ç„¶çš„ä¹Ÿæ¯”è¼ƒä¹…ã€‚
* ä¸€æ¬¡åšå¾ˆå¤šå€‹æ–‡å­—ç”Ÿæˆï¼Œä¿—ç¨±**æ‰¹æ¬¡æ¨è«– (Batch Inference)**ï¼š
    ```python
    tk.pad_token = tk.eos_token  # LlamaTokenizer æ²’æœ‰ Padding Token
    prompt = ["Hello, ", "Hi, my name is"]
    tokens = tk(prompt, return_tensors="pt", padding=True)
    input_ids = tokens["input_ids"].to("cuda")

    outputs = model.generate(input_ids, max_new_tokens=16)
    print(tk.batch_decode(outputs))
    ```
    * **æ³¨æ„**ï¼Œå› ç‚º Llama Tokenizer æ²’æœ‰é è¨­å¡«å……ç”¨çš„ Padding Tokenï¼Œæ‰€ä»¥éœ€è¦å¹«å®ƒæŒ‡å®šä¸€å€‹ï¼Œæ‰èƒ½åšå¡«å……ã€‚
    
* æœ‰æ™‚æœƒå‡ºç¾è¼¸å‡ºçµæœæ€ªæ€ªçš„çµæœï¼š
    ```
    [
        "<s> Hello, </s></s>0000000000000000",
        "<s> Hi, my name is [Your Name] and I am a [Your Profession] ...",
    ]
    ```
    * å› ç‚º Transformers Decoder æ˜¯è‡ªå›æ­¸è§£ç¢¼ (Autoregressive) çš„é—œä¿‚ï¼Œæ”¾åœ¨è¶Šå³é‚Šçš„ Token é€šå¸¸æœƒå°ä¸‹ä¸€å€‹ç”Ÿæˆçš„ Token æœ‰è¶Šé¡¯è‘—çš„å½±éŸ¿ã€‚
        * è‹¥æ˜¯å°‡ Padding Token éƒ½æ”¾å³é‚Šï¼Œæ¨¡å‹çœ‹åˆ°ä¸€å † Padding Token å¾Œä¹Ÿä¸çŸ¥é“è¦è¼¸å‡ºä»€éº¼ã€‚
        * æ‰€ä»¥åœ¨é€²è¡Œæ‰¹æ¬¡ç”Ÿæˆæ™‚ï¼Œè¨˜å¾—è¦æŠŠ Padding Token æ”¾åœ¨å·¦é‚Šï¼Œå¯ä»¥åœ¨åˆå§‹åŒ– Tokenizer æ™‚è¨­å®š `padding_side` é€™å€‹åƒæ•¸ã€‚
    ```python
    tk: TkCls = TkCls.from_pretrained(model_path, padding_side="left")
    # é‡æ–°åŸ·è¡Œæ‰¹æ¬¡æ¨è«–å°±èƒ½çœ‹åˆ°æ­£å¸¸çµæœ
    ...
    ```
    ```
    [
        "</s></s><s> Hello, I am a 35 year old woman ...",
        "<s> Hi, my name is [Your Name] and I am a [Your Profession] ...",
    ]
    ```
* çŸ­å¥å·¦å´åŠ ä¸Šä¸€å † Padding Token æ™‚æ˜¯å¦æœƒå½±éŸ¿è¼¸å‡ºï¼Ÿ
    * Attention Mask å°±æ˜¯ç”¨ä¾†å‘Šè¨´æ¨¡å‹ "æˆ‘ä¸æ˜¯è¼¸å…¥ï¼" çš„ä¸€é …è³‡è¨Šï¼Œæ¨¡å‹çœ‹åˆ°ä¸€å † Padding Token æœƒå‡è£æ²’çœ‹åˆ°ã€‚
    * å› æ­¤é€²è¡Œæ‰¹æ¬¡æ¨è«–æ™‚ï¼Œè¼¸å…¥é©ç•¶çš„ Attention Mask ç›¸ç•¶é‡è¦ã€‚

---

## ä¸²æµè¼¸å‡º (Streaming)
é€²è¡Œé•·æ–‡æœ¬ç”Ÿæˆå¯ä»¥é€é `TextStreamer` ä¾†é€²è¡Œä¸²æµè¼¸å‡ºï¼Œä¾†ç²å¾—å³æ™‚å›é¥‹ã€‚ä¸éœ€è¦ç­‰ä¸Šä¸€æ®µæ™‚é–“ï¼Œæ‰æœƒçœ‹åˆ°å®Œæ•´çš„è¼¸å‡ºã€‚
* åˆ©ç”¨ Tokenizer åˆå§‹åŒ–ä¸€å€‹ `TextStreamer` ç‰©ä»¶å¾Œç•¶æˆåƒæ•¸ä¸Ÿé€² `model.generate` ä¸­ï¼š
    * **æ³¨æ„**ï¼Œ`TextStreamer` åªæ”¯æ´å–®ä¸€è¼¸å…¥ï¼Œç„¡æ³•é€²è¡Œæ‰¹æ¬¡æ¨è«–ã€‚
    ```python
    from transformers import TextStreamer
    
    inputs = tk("Hello, ", return_tensors="pt").to("cuda")
    output = model.generate(
        **inputs, 
        max_new_tokens=2048,
        streamer=TextStreamer(tk),
    )
    ```

---

## æç¤ºæ¨£æ¿ (Prompt Template)
è‹¥è¦ç™¼å‹• LLM çš„å°è©±èƒ½åŠ›ï¼Œæˆ‘å€‘éœ€è¦å€ŸåŠ©æç¤ºæ¨£æœ¬çš„æ©Ÿåˆ¶ã€‚
* æç¤ºæ¨£æœ¬å…¶å¯¦å°±æ˜¯æŠŠä½¿ç”¨è€…èˆ‡æ¨¡å‹ä¹‹é–“ä¸€å•ä¸€ç­”çš„éç¨‹ï¼Œå¯«æˆä¸€å€‹åƒæ˜¯**èŠå¤©ç´€éŒ„çš„æ ¼å¼**ï¼Œç”¨ä¾†ã€Œæ¬ºé¨™ã€èªè¨€æ¨¡å‹ç”Ÿæˆä¸‹å€‹èŠå¤©ç‰‡æ®µã€‚
* é€™å€‹æ¨£æ¿æœƒæ ¹æ“šæ¯å€‹èªè¨€æ¨¡å‹è¨“ç·´çš„æ–¹å¼è€Œæœ‰æ‰€ä¸åŒï¼Œä¸€èˆ¬è€Œè¨€éµå®ˆæ¨¡å‹é–‹ç™¼è€…çš„å»ºè­°æœƒæ¯”è¼ƒå¥½ã€‚
* [Llama2 çš„å®˜æ–¹æ¨£æ¿](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3#64b71f7588b86014d7e2dd71)ï¼š
    ```
    [INST] <ä½¿ç”¨è€…è¨Šæ¯> [/INST] <æ¨¡å‹çš„å›è¦†å¾é€™é–‹å§‹>
    ```
    * ä½¿ç”¨ç›¸åŒæ¨£æ¿ä¾†è©¢å•æ¨¡å‹ï¼š
        ```python
        prompt = "[INST] ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ[/INST] "

        inputs = tk(prompt, return_tensors="pt").to("cuda")
        output = model.generate(
            **inputs,
            max_new_tokens=128,
            streamer=TextStreamer(tk),
        )

        """
        è¼¸å‡ºçµæœï¼š
        <s> [INST] ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ [/INST] å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰æ˜¯ä¸€ç¨® ...</s>
        """
        ```
* ç›´æ¥èª¿ç”¨è¡Œé–‹ç™¼è€…è¨­å®šçš„æ¨¡æ¿ï¼š (æ¨£æ¿éš¨æ¨¡å‹è€Œè®ŠåŒ–)
    * ä»¥ Google çš„ Gemma ç‚ºä¾‹ï¼š
    * ä½¿ç”¨ Transformer å¥—ä»¶å…§å»ºçš„**èŠå¤©æ¨£æ¿ (Chat Template)** åŠŸèƒ½ï¼Œåªè¦å‘¼å«åˆ†è©å™¨çš„ `apply_chat_template` æ–¹æ³•å³å¯ã€‚
        ```python
        tk = AutoTokenizer.from_pretrained("google/gemma-2b-it")
        query = " ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ "
        conversation = [{"role": "user", "content": query}]
        prompt = tk.apply_chat_template(conversation, tokenize=False)
        print(prompt)

        """
        Output: <bos><start_of_turn>user
        ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ <end_of_turn>
        """
        ```
    * é€é `add_generation_prompt` åƒæ•¸ï¼Œå¯ä»¥æŠŠæ¨¡å‹é–‹å§‹å›ç­”å‰çš„æ ¼å¼ä¹Ÿé™„åŠ ä¸Šï¼Œè®“æ¨¡å‹æ¯”è¼ƒä¸å®¹æ˜“ç”Ÿæˆå¤±æ•—ã€‚
        ```python
        tk = AutoTokenizer.from_pretrained("google/gemma-2b-it")
        query = " ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ "
        conversation = [{"role": "user", "content": query}]
        prompt = tk.apply_chat_template(
            conversation, 
            tokenize=False,
            add_generation_prompt=True,
        )
        print(prompt)

        """
        Output: <bos><start_of_turn>user
        ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè«‹å•ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ <end_of_turn>
        <start_of_turn>model               <--- æ¨¡å‹é–‹å§‹å›ç­”å‰çš„æ ¼å¼
        """
        ```
    * æ ¼å¼ç¢ºèªå®Œç•¢å¾Œï¼Œä¾†å–å¾—æ¨¡å‹çš„è¼¸å…¥ï¼š
        ```python
        input_ids = tk.apply_chat_template(
            conversation, 
            # tokenize=False,
            return_tensors="pt",
            add_generation_prompt=True,
        ).to("cuda")
        print(input_ids)
        
        # Output: tensor([[2, 106, ..., 2516, 108]], device='cuda:0')
        ```
    * è¼¸å…¥æ¨¡å‹ï¼š
        å› ç‚ºæ²’æœ‰ `attention_mask`ï¼Œæ‰€ä»¥è¼¸å…¥æ¨¡å‹çš„æ–¹å¼æœ‰äº›ä¸åŒã€‚
        ```python
        # output = model.generate(**inputs) # åŸæœ¬
        output = model.generate(input_ids)  # ç¾åœ¨
        ```
* Trasormers å¥—ä»¶çš„èŠå¤©æ¨£æ¿åŠŸèƒ½æ˜¯ä½¿ç”¨ [Jinja](https://github.com/pallets/jinja/) å¥—ä»¶å¯¦ä½œçš„ã€‚ä¸€äº›ç›¸å°æ—©æœŸé‡‹å‡ºçš„æ¨¡å‹å¯èƒ½æ²’æœ‰åœ¨è¨­å®šæª”è£¡é™„ä¸Šæ¨£æ¿ï¼Œæ­¤æ™‚å¯ä»¥è€ƒæ…®è‡ªè¡Œæ’°å¯«èŠå¤©æ¨£æ¿ã€‚
    * è©³ç´°ç”¨æ³•è«‹åƒè€ƒ[å®˜æ–¹æ•™å­¸](https://tinyurl.com/llm-chat-template)

---

## å–æ¨£åƒæ•¸ (Sample Parameters)
æ¨¡å‹åœ¨é€²è¡Œæ¨è«–çš„æ™‚å€™ï¼Œå…¶å¯¦æœƒç”Ÿæˆä¸€å¼µæ©Ÿç‡è¡¨ï¼Œç”¨ä¾†ä»£è¡¨ä¸‹ä¸€å€‹ Token å¯èƒ½æœƒæ˜¯èª°ã€‚
* å–æ¨£åƒæ•¸å°±æ˜¯æˆ‘å€‘**å¦‚ä½•å¾é€™åˆ†æ©Ÿç‡è¡¨è£¡æŒ‘é¸ä¸‹å€‹ Token çš„åƒæ•¸**ã€‚
* å¯ä»¥åœ¨ `GenerationConfig` é¡åˆ¥è£¡é¢æŒ‡å®šä¸€äº›å–æ¨£åƒæ•¸ï¼Œè®“æ¨¡å‹ä¸æœƒæ¯æ¬¡éƒ½æŒ‘æ©Ÿç‡æœ€é«˜çš„é‚£å€‹ Token ç•¶è¼¸å‡ºï¼Œä½¿æ¯æ¬¡è¼¸å‡ºéƒ½æœƒé•·çš„ä¸å¤ªä¸€æ¨£ï¼Œæå‡è¼¸å‡ºçš„å¤šå…ƒå‹å¤–ã€‚
    * å¦‚æœå–æ¨£çš„éš¨æ©Ÿæ€§å¤ªé«˜ï¼Œå¯èƒ½æœƒå½±éŸ¿æ¨¡å‹å›ç­”çš„æ­£ç¢ºæ€§ï¼Œè«‹æ…é¸ã€‚
* å¸¸è¦‹çš„åƒæ•¸ï¼š
    | åƒæ•¸åç¨±                 | åŠŸèƒ½èªªæ˜                                                     | æ•¸å€¼å½±éŸ¿æˆ–å»ºè­°ä½¿ç”¨æ–¹å¼                                       |
    | -------------------- | -------------------------------------------------------- | ------------------------------------------------- |
    | `do_sample`          | æ§åˆ¶æ˜¯å¦ä½¿ç”¨éš¨æ©Ÿå–æ¨£ã€‚<br>-- `True` : ç‚ºéš¨æ©Ÿå–æ¨£ã€‚<br>-- `False` : ç‚º Greedy Decodeï¼Œåªå–æ©Ÿç‡æœ€é«˜çš„ Tokenã€‚     | ä½¿ç”¨ Greedy Decode æ™‚ï¼ˆ`False`ï¼‰ï¼Œå»ºè­°ä¸è¦è¨­å®šå…¶ä»–å–æ¨£åƒæ•¸ï¼Œä»¥å…çµæœä¸ç©©å®šã€‚ |
    | `top_k`              | å¾æ©Ÿç‡æœ€é«˜çš„å‰ K å€‹ token ä¸­å–æ¨£ã€‚                                   | K å€¼è¶Šå°ï¼Œè¼¸å‡ºè¶Šç©©å®šï¼›è¶Šå¤§å‰‡è¼¸å‡ºè¶Šå¤šæ¨£ã€‚                             |
    | `top_p`              | ç´¯åŠ æ©Ÿç‡ç›´åˆ°ç¸½å’Œè¶…é `top_p`ï¼Œç„¶å¾Œå¾é€™äº› token ä¸­å–æ¨£ï¼ˆåˆç¨± nucleus samplingï¼‰ã€‚ | å½ˆæ€§é«˜ï¼Œæœƒæ ¹æ“šå¯¦éš›æ©Ÿç‡åˆ†å¸ƒè‡ªå‹•æ±ºå®šå–æ¨£ç¯„åœï¼›å¸¸ç”¨å€¼å¦‚ 0.9ã€‚                   |
    | `temperature`        | èª¿æ•´æ©Ÿç‡åˆ†å¸ƒçš„å¹³æ»‘åº¦ï¼Œæ”¹è®Š token çš„å‡ºç¾æ©Ÿç‡ã€‚<br>æ”¹é€ æ©Ÿç‡è¡¨ï¼Œtemperature è¶Šå¤§æ™‚ï¼Œç¬¬ä¸€åè·Ÿæœ€å¾Œä¸€åçš„è·é›¢å°±æœƒè¶Šè¿‘ï¼Œä½†å½¼æ­¤ä¹‹é–“çš„ç›¸å°é—œä¿‚é‚„æ˜¯ä¿æŒä¸è®Šã€‚                       | æ•¸å€¼è¶Šé«˜ï¼ˆå¦‚ 1.0 ä»¥ä¸Šï¼‰ï¼Œè¼¸å‡ºæ›´æœ‰å‰µæ„ï¼›è¶Šä½ï¼ˆå¦‚ 0.1ï¼‰è¼¸å‡ºæ›´åš´è¬¹ã€‚             |
    | `repetition_penalty` | æ‡²ç½°é‡è¤‡çš„è¼¸å‡ºï¼Œé™ä½é‡è¤‡å…§å®¹çš„ç”Ÿæˆå¯èƒ½æ€§ã€‚                                    | `> 1.0` æŠ‘åˆ¶é‡è¤‡ï¼Œ`< 1.0` å®¹æ˜“é‡è¤‡ã€‚è·³é‡æƒ…æ³ä¸‹å»ºè­°èª¿é«˜æ­¤åƒæ•¸ã€‚           |
* å–æ¨£åƒæ•¸ç¯„ä¾‹ï¼š
    ```python
    from transformers import GenerationConfig

    gen_config = GenerationConfig(
        max_new_tokens=16,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.75,
        repetition_penalty=1.1,
    )

    output = model.generate(**inputs, generation_config=gen_config, streamer=ts,)
    ```
* é€™äº›åƒæ•¸ä¹Ÿæœƒå°å½¼æ­¤ä¹‹é–“ç”¢ç”Ÿä¸åŒç¨‹åº¦çš„å½±éŸ¿ï¼Œå› æ­¤é¸æ“‡ä¸€çµ„é©åˆçš„åƒæ•¸é€²è¡Œç”Ÿæˆä»»å‹™ï¼Œä¹Ÿæ˜¯ä¸€å ‚ç›¸ç•¶é‡è¦çš„èª²é¡Œã€‚
* Transformers å¯ä»¥ç”¨ä¾†æ§åˆ¶ç”Ÿæˆçš„åƒæ•¸é‚„æœ‰å¾ˆå¤šï¼Œè©³ç´°è³‡è¨Šå¯ä»¥åƒè€ƒ[å®˜æ–¹æ–‡ä»¶](https://huggingface.co/docs/transformers/main_classes/text_generation)ã€‚

---

## è¨­å®šåœæ­¢é» (Stopped Words)
åœæ­¢é»æ˜¯ç”¨ä¾†å‘Šè¨´ç³»çµ±åœ¨ç”Ÿæˆçš„éç¨‹ä¸­ï¼Œé™¤äº†é‡åˆ° EOS (End-of-Sentence) Token ä»¥å¤–ï¼Œé‚„æœ‰é‡åˆ°å“ªäº› Token æ‡‰è©²åœæ­¢è¼¸å‡ºã€‚
* ä¾‹å¦‚ï¼Œå°‡å¥è™Ÿæˆ–æ›è¡Œç¬¦è™Ÿç•¶æˆåœæ­¢é»ã€‚
* æœ€åŸºæœ¬çš„æ–¹æ³•æ˜¯è¨­å®š `GenerationConfig` çš„ `eos_token_id`ï¼š
    ```python
    generation_config = GenerationConfig(
        eos_token_id=[
            tk.eos_token_id,       # ç•™è‘—åŸæœ¬çš„ EOS
            tk.encode(".")[-1],    # é‡åˆ°å¥é»åœä¸‹
            tk.encode("\n")[-1],   # é‡åˆ°æ›è¡Œåœä¸‹
            tk.encode("\n\n")[-1], # é‡åˆ°é›™æ›è¡Œåœä¸‹
        ],
    )
    ```
    * æ­¤æ–¹æ³•åªé©ç”¨æ–¼åœæ­¢é»æ˜¯ç¨ç«‹ä¸€å€‹ Token çš„æƒ…æ³ã€‚å¦‚æœåœæ­¢é»ä¸åªä¸€å€‹ Token ç”šè‡³æ˜¯ä¸€å€‹å­—ä¸²çš„è©±ï¼Œéœ€è¦è‡ªå·±å¯¦ä½œ `StoppingCriteria` é¡åˆ¥ã€‚
* è‡ªå·±å¯¦ä½œ `StoppingCriteria` é¡åˆ¥ï¼š
    ```python
    from transformers import StoppingCriteria, StoppingCriteriaList


    class StopWords(StoppingCriteria):
        def __init__(self, tk: TkCls, stop_words: list[str]):
            self.tk = tk
            self.stop_tokens = stop_words

        def __call__(self, input_ids, *_) -> bool:
            s = self.tk.batch_decode(input_ids)[0]
            for t in self.stop_tokens:
                if s.endswith(t):
                    return True
            return False


    sw = StopWords(tk, ["ã€‚", "ï¼", "ï¼Ÿ"])
    scl = StoppingCriteriaList([sw])
    ```
    * æ¯æ¬¡ç³»çµ±æª¢æŸ¥çš„æ™‚å€™ï¼Œå°‡æ•´ä»½è¼¸å‡º Decode å›ç´”æ–‡å­—ï¼Œä¸¦ä¸”æª¢æŸ¥æ–‡å­—çš„çµå°¾æ˜¯å¦ç¬¦åˆä½¿ç”¨è€…è¨­å®šçš„åœæ­¢é»ã€‚
    * æ­¤ç¯„ä¾‹ç‚ºå–®ä¸€è¼¸å…¥é€²è¡Œæ–‡æœ¬ç”Ÿæˆçš„æƒ…æ³ï¼Œæ‰¹æ¬¡æ¨è«–æ™‚æƒ…æ³æœƒè¤‡é›œè¨±å¤šã€‚
* æ¥è‘—å°‡ `StopWords` ç‰©ä»¶æ”¾é€²ä¸€å€‹ `StoppingCriteriaList` è£¡é¢ï¼Œç„¶å¾Œå‚³å…¥ `model.generate` è£¡é¢å³å¯ï¼š
    ```python
    output = model.generate(
        input_ids,
        max_new_tokens=2048,
        streamer=TextStreamer(tk),
        stopping_criteria=scl,
    )
    # æ¨¡å‹åœ¨è¼¸å‡ºé‡åˆ° "ã€‚ï¼ï¼Ÿ" æ™‚å°±æœƒåœä¸‹ä¾†äº†ã€‚
    ```
* Transformers åœ¨ `GenerationConfig` ä¸­æ–°å¢äº† `stop_strings` çš„åƒæ•¸ï¼Œå¯ä»¥æ›´è¼•é¬†çš„è¨­å®šåœæ­¢é»ï¼š
    ```python
    from transformers import GenerationConfig

    config = GenerationConfig(
        stop_strings=[".", "\n"],
    )

    inputs = tk(["hello,", "goodbye, "], padding=True, return_tensors="pt")
    inputs = inputs.to(model.device)
    output = model.generate(**inputs, generation_config=config, tokenizer=tk)    
    ```
    * é€™æ¨£æ‰¹æ¬¡æ¨è«–ä¹Ÿèƒ½ä½¿ç”¨åœæ­¢é»åŠŸèƒ½äº†ã€‚

---

## è‡ªè¿´æ­¸è§£ç¢¼ (Autoregressive Decoding)
æè¿°æ¨¡å‹ä¸æ–·ç”Ÿæˆ Token çš„éç¨‹ã€‚æ¨¡å‹ç”Ÿæˆçš„ Token æœƒé‡æ–°è®Šæˆæ¨¡å‹çš„è¼¸å…¥ï¼Œæ¨¡å‹åœ¨æ ¹æ“šé€™äº›æ–°ç”Ÿæˆçš„ Token ç¹¼çºŒå¾€ä¸‹ç”Ÿæˆï¼Œç›´åˆ°è§¸ç™¼çµæŸæ¢ä»¶ç‚ºæ­¢ã€‚
* ç›¸å°æ–¼éè‡ªè¿´æ­¸ (Non-Autogressive, NAR) æ¨¡å‹è€Œè¨€ï¼Œå…¶å·®åˆ¥åœ¨æ–¼è‡ªè¿´æ­¸æ¨¡å‹æœƒæŠŠè‡ªå·±çš„è¼¸å‡ºç•¶æˆè¼¸å…¥ï¼Œè€Œ NAR æ¨¡å‹æœƒä¸€æ¬¡æŠŠæ•´å€‹åºåˆ—ç”Ÿå‡ºä¾†ã€‚
* åœ¨ LLM è£¡é¢ï¼Œæ¯æ¬¡**æ¨è«– (Inference)** åªæœƒç”Ÿæˆä¸€å€‹ Tokenï¼Œå¿…é ˆè¦ç¶“éå¤šæ¬¡æ¨è«–ï¼Œæ‰èƒ½å®Œæˆå®Œæ•´çš„æ–‡æœ¬**ç”Ÿæˆ (Generation)**ã€‚

### æ‹†è§£ model.generate çš„èƒŒå¾ŒåŸç†ï¼š
* PyTorch ä¸‹ï¼Œæ¯å€‹æ¨¡å‹éƒ½å¯ä»¥ç›´æ¥é€é `.forward()` ä¾†é€²è¡Œæ¨è«–ï¼š
    * ä½¿ç”¨ `model.forward(input_ids)` èˆ‡ `model(input_ids)` åŒæ¨£å¯ä»¥å¾—åˆ°æ¨¡å‹æ¨è«–çµæœã€‚
    * ä½¿ç”¨ `.forward()` ç•¶ç¯„ä¾‹ï¼Œä½è¦æ˜¯å¼·èª¿è©²æ­¥é©Ÿç‚ºå‰å‘é‹ç®—ï¼Œå¦å¤–é€™æ¨£çš„å›å‚³çµæœä¹Ÿæœƒæœ‰æ¯”è¼ƒæ˜ç¢ºçš„å‹åˆ¥æç¤ºã€‚
    * å¯¦éš›é€šå¸¸ä¸ä½¿ç”¨ `.forward()` ï¼Œå› ç‚º PyTorch ä¸­æœ‰ä¸€äº›éŒ¢è™•ç†èˆ‡å¾Œè™•ç†çš„ Hooks ï¼Œå¦‚æœä½¿ç”¨å®ƒå¯èƒ½æœƒå¿½ç•¥æ‰é€™äº› Hoooks å°è‡´æŸäº›ç’°ç¯€è™•ç†ä¸æ­£å¸¸ã€‚
    ```python
    import torch

    input_ids = tokens["input_ids"].to("cuda")

    with torch.no_grad():
        output = model.forward(input_ids)
    ```
    * ç”¢ç”Ÿä¸€å€‹é¡åˆ¥ç‚º `CausalLMOutputWithPast` çš„è¼¸å‡ºï¼Œè£¡é¢æœ‰å…©å€‹é‡è¦çš„è³‡è¨Šï¼Œåˆ†åˆ¥ç‚º `logits` èˆ‡ `past_key_values`ã€‚
        * `logits` : 
            * æ¨¡å‹é æ¸¬ä¸‹å€‹ Token çš„æ©Ÿç‡è¡¨ã€‚
            * å¯ä»¥å° `logits` åšéš¨æ©Ÿå–æ¨£ï¼Œæˆ–è€…ç›´æ¥è¨ˆç®— `argmax` åšå–æ¨£æ©Ÿæœ€é«˜çš„ Token åš Greedy Decodeã€‚
        * `past_key_values` :
            * ä¿—ç¨±**éµå€¼å¿«å– (Key-Values Cache)**
            * æ˜¯æ¨¡å‹å°**å·²çŸ¥è¼¸å…¥**çš„é‹ç®—çµæœã€‚
            * å› ç‚ºè‡ªè¿´æ­¸çš„ç‰¹æ€§ï¼Œæ‰€ä»¥ Decoder LM æ¯æ¬¡ç”Ÿæˆçš„è¼¸å‡ºéƒ½æœƒè®Šæˆä¸‹å€‹å›åˆçš„è¼¸å…¥ï¼Œå› æ­¤é€™å€‹ KV Cache æœƒè¶Šé•·è¶Šå¤§ï¼ŒåŒæ™‚ä¹Ÿæ˜¯**æ¶ˆè€— GPU è¨˜æ†¶é«”çš„å…ƒå…‡ä¹‹ä¸€**ã€‚
* æœ‰äº†é€™ä»½ KV Cacheï¼Œå°±ä¸ç”¨æ¯æ¬¡æ¨è«–æ™‚éƒ½æŠŠæ•´å€‹ `input_ids` ä¸Ÿé€²å»ï¼Œåªè¦ç•™æ–°é•·å‡ºä¾†çš„ Token å°±å¥½ï¼Œå› ç‚ºèˆŠçš„è¼¸å…¥éƒ½å·²ç¶“è¢«æ¨¡å‹ Decode æˆ KV Cache äº†ã€‚
    * ä¸‹å€‹å›åˆçš„æ¨è«–å°±æœƒé•·é€™æ¨£ï¼š
        ```python
        with torch.no_grad():
            outputs = model(
                outputs.logits.argmax(-1)[:, -1:],  # Greedy Decode
                past_key_values=output.past_key_values,
            )
        ```
* å°‡æ­¤é‚è¼¯æ•´ç†æˆä¸€å€‹è¿´åœˆä¾†é€²è¡Œï¼š
    ```python
    pkv = None
    output_tokens = list()
    for i in range(16):
        with torch.no_grad():
            outputs = model.forward(input_ids, past_key_values=pkv)
        
        next_token = outputs.logits.argmax(-1)[:, -1:]
        token_id = next_token.item()

        if token_id == tk.eos_token_id:
            break

        output_tokens.append(token_id)
        input_ids = next_token
        pkv = outputs.past_key_values

    print(tk.decode(output_tokens))
    # è¼¸å‡ºçµæœï¼šå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰
    ```
    * é€™é‚Šå›ºå®šé€²è¡Œ 16 æ¬¡æ¨è«–ï¼Œä¸­é–“å¦‚æœé‡åˆ° EOS Token å°±æœƒè·³å‡ºè¿´åœˆã€‚
    * æ‹†è§£ç”Ÿæˆæ­¥é©Ÿçš„å¥½è™•ï¼š
        * å¯ä»¥é‡å° `logits` çš„æ©Ÿç‡åˆ†ä½ˆåšè§€å¯Ÿã€‚
        * æœ‰äº›æ™‚å€™å¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨ã€Œ**èªªè¬Š**ã€æ™‚ï¼Œé‚£å€‹éƒ¨ä»½è¼¸å‡ºçš„ Token çš„æ©Ÿç‡åˆ†ä½ˆæœƒç‰¹åˆ¥çš„ä½ï¼Œå› æ­¤ä¹Ÿå¯ä»¥ç”¨ä¾†è¨ˆç®—æ¨¡å‹çš„**ä¿¡å¿ƒåº¦**ä¹‹é¡çš„ã€‚
* `torch.no_grad()` æ˜¯ç”¨ä¾†åœç”¨æ¢¯åº¦çš„ Context Managerã€‚
    * å› ç‚º PyTorch çš„æ¨¡å‹åœ¨é€²è¡Œè¨ˆç®—æ™‚ï¼Œé è¨­éƒ½æœƒé †ä¾¿ç®—å‡ºä¸€åˆ†æ¢¯åº¦ç”¨ä¾†è¨“ç·´ã€‚
    * ä½†é€™è£¡ä¸æ˜¯è¦åšè¨“ç·´ï¼Œæ‰€ä»¥ç”¨å®ƒä¾†éµå°‘ä¸å¿…è¦çš„è¨ˆç®—ï¼Œä¹Ÿå¯ä»¥é¿å…èªè¨€æ¨¡å‹ç”¢ç”Ÿéå¤§çš„æ¢¯åº¦è€Œå°è‡´è¨˜æ†¶é«”ä¸è¶³ã€‚
    * ä½¿ç”¨ `torch.inference_mode()` ä¹Ÿæœ‰é¡ä¼¼çš„æ•ˆæœï¼š
        ```python
        with torch.no_grad():
            outputs = model.forward(...) 

        @torch.inference_mode()
        def main():
            outputs = model.forward(...)
        ```

---

## å¤šé¡¯å¡æ¨è«– (Multi-GPUs)
* ç•¶æ©Ÿå™¨æœ‰å¤šé¡† GPUs æ™‚ï¼ŒæŒ‡å®šè¦ä½¿ç”¨å“ªé¡† GPU é€²è¡Œé‹ç®—ï¼š
    1. å¯ä»¥é€éç’°å¢ƒè®Šæ•¸ `CUDA_VISIBLE_DEVICES` ä¾†æŒ‡å®šï¼š
        ```bash
        # æŒ‡å®šä½¿ç”¨ 1 è™Ÿ GPU é€²è¡Œé‹ç®—
        CUDA_VISIBLE_DEVICES=1 python main.py
        ```
    2. é€é Python çš„ `os.environ` ä¾†æŒ‡å®šï¼š
        ```python
        import os

        os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

        import transformers
        ```
        * åœ¨æŒ‡å®šç’°å¢ƒè®Šæ•¸æ™‚ï¼Œä¸€å®šè¦==å…ˆæ–¼åŒ¯å…¥ Transformers å¥—ä»¶==ï¼Œå¦å‰‡å®¹æ˜“å¤±æ•ˆã€‚
* ä½¿ç”¨å¤šå¼µ GPUs åŒæ™‚é€²è¡Œæ¨è«–çš„ç¨‹å¼ç¢¼ï¼š
    * ==è¦è¨˜å¾—å°‡ `device_map` è¨­å®šç‚º `"auto"`==ï¼Œå‰©é¤˜ç¨‹å¼ç¢¼èˆ‡ä¸€èˆ¬æ¨è«–åŸºæœ¬ç›¸åŒï¼Œ
    ```python
    import os

    # ä½¿ç”¨ 0 è™Ÿèˆ‡ 1 è™Ÿ GPU åŒæ™‚é€²è¡Œæ¨è«–
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

    from transformers import LlamaForCausalLM as ModelCls
    from transformers import LlamaTokenizer as TkCls
    from transformers import TextStreamer

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelCls.from_pretrained(
        model_path,
        device_map="auto"
    )
    tk: TkCls = TkCls.from_pretrained(model_path)
    ts = TextStreamer(tk)

    prompt = "Hello, "
    input_ids = tk(prompt, return_tensors="pt")["input_ids"].to("cuda")
    model.generate(input_ids, max_new_tokens=16, streamer=ts)
    ```

---

## Candle
åŒæ¨£ç”± Hugging Face é–‹ç™¼çš„ [Candle](https://github.com/huggingface/candle) æ¡†æ¶ï¼Œæ˜¯ä½¿ç”¨ Rust èªè¨€é–‹ç™¼çš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶ã€‚
* Rust æ˜¯ä»¥é«˜æ•ˆç‡ã€é«˜è¨˜æ†¶é«”å®‰å…¨èåçš„ç¨‹å¼èªè¨€ã€‚
* ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼
    * [Candle Llama 2 Demo](https://huggingface.co/spaces/lmz/candle-llama2)
    * è‡ªå·±å˜—è©¦: .../LLM/project/hugging_face_transformers

---

## èªè¨€æ¨¡å‹åˆ°åº•æ˜¯ "å¤§" åœ¨å“ªè£¡ï¼Ÿ
é€šå¸¸åœ¨è«‡è«–ä¸€å€‹æ¨¡å‹çš„æ•ˆèƒ½ç“¶é ¸æ™‚ï¼Œæœƒåˆ†æˆå…©å€‹éƒ¨åˆ†ä¾†çœ‹ï¼š
1. GPU çš„**é‹ç®—èƒ½åŠ›**ï¼š èƒ½ç®—çš„å¤š**å¿«**ã€‚
2. GPU çš„**è¨˜æ†¶é«”å®¹é‡**ï¼š èƒ½æ”¾ä¸‹å¤š**å¤§**çš„æ¨¡å‹ã€‚

å¯å¯«ä¸€å€‹å´ç“¶é€Ÿåº¦èˆ‡è¨˜æ†¶é«”çš„ç¨‹å¼ï¼š
1. å»ºç«‹å‡çš„è¼¸å…¥è³‡æ–™ï¼Œè¨­å®šé•·åº¦èˆ‡æ‰¹æ¬¡å¤§å°ï¼Œä¸¦ä¸”å…¨éƒ¨å¡« 0ã€‚
    ```python
    import torch

    batch_size, seqlen = 1, 1024
    torch.zeros(batch_size, dtype=torch.int64)
    ```
2. (è¨˜æ†¶é«”) é€é PyTorch å¥—ä»¶å–å¾— GPU è¨˜æ†¶é«”çš„ç”¨é‡ï¼š
    ```python
    # å–å¾—è®€å–æ¨¡å‹å¾Œçš„ GPU è¨˜æ†¶é«”ä½¿ç”¨é‡
    unit = 1024**2 # MiB
    init_mem = torch.cuda.memory_reserved() / unit
    print(f"Model Weight Memory Usage: {init_mem:.0f} MiB")
    ```
3. (é€Ÿåº¦) å€ŸåŠ© `tqdm` é€²åº¦æ¢å¥—ä»¶ä¾†æ¸¬é‡ï¼š
    ```python
    from tqdm import trange

    n_decode = 128
    with trange(n_decode, ncols=100) as prog:
        for i in prog:
            # ... Autoregressive Decoding
            total_mem = torch.cuda.memory_reserved() / unit
            prog.desc = f"Memory Usage: {total_mem:.0f} MiB"
    ```
4. åœ¨è¿´åœˆå…§æ”¾å…¥è‡ªå›æ­¸è§£ç¢¼çš„ç¨‹å¼ï¼Œä¸¦åœ¨æ¯ä¸€æ¬¡è¿´åœˆçµæŸå¾Œè¨ˆç®—è¨˜æ†¶é«”çš„ç”¨é‡ï¼Œä¾†è§€å¯Ÿè¨˜æ†¶é«”éš¨è‘—ç”Ÿæˆçš„éç¨‹è€Œç”¢ç”Ÿçš„è®ŠåŒ–ã€‚
    * é€™ä»½ç¨‹å¼ç¢¼çš„åƒæ•¸åŒ…å«ï¼š
        * æ¨¡å‹è·¯å¾‘ã€æ‰¹æ¬¡å¤§å°ã€åˆå§‹è¼¸å…¥é•·åº¦ã€å¾ŒçºŒè¼¸å‡ºçš„é•·åº¦ã€æ³¨æ„åŠ›æ©Ÿåˆ¶çš„å¯¦ä½œç¨®é¡ã€‚
5. æ¸¬è©¦ Llama2 7B çš„é€Ÿåº¦ï¼š (ä»¥ RTX 3090 åšæ¸¬è©¦)
    ```bash
    $ python HF-Bench.py meta-llama/Llama-2-7b-hf
    
    Model Weight Memory Usage: 12854 MibB
    Sequence Length: 1024
    Memory: 3332/16186 MiB: 1024/1024 [00:26<00:00, 39.21it/s]
    Decode Length: 1024
    Total Length: 2048
    ```
    * çµæœé¡¯ç¤ºï¼Œé–‹å§‹æ¨è«–å‰ï¼Œæ¨¡å‹æ¬Šé‡ä½”ç”¨äº† 12854 MB çš„ GPU è¨˜æ†¶é«”ï¼ŒåŸ·è¡Œåˆ°æœ€å¾Œç”¨æ‰äº† 3332 MBï¼Œç¸½å…±ä½”ç”¨ 16186 MBï¼ŒèŠ±è²» 26 ç§’åŸ·è¡Œï¼Œå¹³å‡æ¯ç§’å¯ä»¥æ¨è«– 39.12 æ¬¡ï¼Œæœ€å¾Œè¼¸å…¥åŠ ä¸Šè¼¸å‡ºçš„é•·åº¦ç‚º 2048ã€‚
6. æ¸¬è©¦ä¸åŒè¼¸å…¥é•·åº¦ï¼š
    | é•·åº¦ | æ¬¡/ç§’          |
    | ---- | -------------- |
    | 2K   | 35             |
    | 4K   | 29             |
    | 5K   | 25             |
    | 6K   | ? (è¨˜æ†¶é«”çˆ†ç‚¸) |
    * éš¨è‘—é•·åº¦å¢åŠ ï¼Œæ•´é«”è¨ˆç®—é‡æå‡ï¼Œé€Ÿåº¦è®Šæ…¢ã€‚
    * ç›´æ¥æ¨ 6K æ¨ä¸å‹•ï¼Œä½†æ”¹å¾ 5 K é–‹å§‹ä¸¦è§£ç¢¼å»èƒ½è§£åˆ° 10K (ç¸½é•·)ã€‚
        * Decoder èªè¨€æ¨¡å‹é€šå¸¸ä¸­ï¼Œé€šå¸¸æœƒä½¿ç”¨**é å¡«å……** (Prefill) ä»£è¡¨ä¸€é–‹å§‹è™•ç†è¼¸å…¥çš„éšæ®µï¼Œä¸¦ä½¿ç”¨**ç”Ÿæˆ** (Generate) æˆ–**è§£ç¢¼** (Decode)ä¾†ä»£è¡¨å¾ŒçºŒç”¢ç”Ÿé€å€‹ Token è¼¸å‡ºçš„éç¨‹ã€‚
        * ä¸€èˆ¬è€Œè¨€ Transformer Decoder æ¨¡å‹æœƒåœ¨ Prefill éšæ®µæ¶ˆè€—æ‰å¤§é‡è¨˜æ†¶é«”ï¼Œé€™æ˜¯å› ç‚º**æ³¨æ„åŠ›æ©Ÿåˆ¶çš„é‹ç®—æœƒåœ¨ Prefill éšæ®µç”¢ç”Ÿå¾ˆå¤§çš„ Peak Memory**ã€‚
        * å¾ŒçºŒç”Ÿæˆéšæ®µå› ç‚ºæ¯ä¸€æ¬¡åªéœ€è¦è™•ç†ä¸€å€‹ Tokenï¼Œæ‰€ä»¥è¨˜æ†¶é«”çš„æ¶ˆè€—ç›¸å°è¼ƒå°‘ã€‚
    * ç¾å¯¦æ‡‰ç”¨å±¤é¢ä¾†èªªï¼Œåªèƒ½è¼¸å…¥ 5K å¯¦åœ¨å¤ªå°‘ã€‚
* è®“è¼¸å…¥æ›´é•·ä¸€é»çš„æ–¹æ³•ï¼š (å°‡æ³¨æ„åŠ›å¯¦ä½œæ”¹æˆ `sdpa`)
    ```bash
    $ python HF-Bench.py meta-llama/Llama-2-7b-hf \
          --seqlen-k=7 --decode-k=8 --attn=sdpa

    Model Weight Memory Usage: 12854 MibB
    Sequence Length: 7168
    Memory: 11034/23888 MiB (OOM): 2562/8192 [01:45<03:52, 24.26it/s]
    Decode Length: 2562
    Total Length: 9731
    ```
    * è¼¸å…¥å¯å¾ 7K ä¸€åº¦æ¨åˆ° 9K äº†ã€‚é›–ç„¶ç¸½é•·è®ŠçŸ­ï¼Œä½†å·®è·ä¸¦ä¸ç®—å¤§ã€‚
* è£œå……èªªæ˜ï¼š
    * **Scaled Dot Product Attention** (SDPA) 
        * æ˜¯ PyTorch 2.0 çš„æ–°åŠŸèƒ½ä¹‹ä¸€ï¼Œå¯ä»¥æ¸›å°‘ Attention è¨ˆç®—çš„ Peak Memoryï¼Œè®“æˆ‘å€‘èƒ½ç”¨æ›´å°‘çš„è¨˜æ†¶é«”å»è¨ˆç®—æ›´é•·çš„åºåˆ—è¼¸å…¥ã€‚
        * èƒŒå¾Œçš„åŸç†ä¹‹ä¸€å°±æ˜¯å‚³èªªçš„ï¼š**Flash Attention**ã€‚
    * **Flash Attention**
        * ç”±å²ä¸¹ä½›å¤§å­¸çš„åšå£«ç”Ÿ Tri Dao æ‰€æå‡º,ä½œè€…æŒ‡å‡º Attention é‹ç®—æœ€å¤§çš„ç“¶é ¸åœ¨æ–¼è¨˜æ†¶é«”,å°¤å…¶æ˜¯Softmax é‹ç®—å› ç‚ºéœ€è¦æ‹œè¨ªæ‰€æœ‰å…ƒç´ ä¾†è¨ˆç®—å„è‡ªçš„æ©Ÿç‡,æ‰€ä»¥æ¶ˆè€—çš„è¨˜æ†¶é«”ç‰¹åˆ¥å·¨å¤§ï¼Œå› æ­¤é€é **IO-Aware Tiling** èˆ‡ **Fused Kernel** é€™å…©å€‹æ ¸å¿ƒæŠ€è¡“ä¾†çªç ´è¨˜æ†¶é«”ç“¶é ¸ã€‚
        * ä½œè€…å¾Œä¾†åˆæå‡ºäº†æ”¹è‰¯ç‰ˆçš„ Flash Attention 2ï¼Œé€²ä¸€æ­¥æ”¹å–„äº†é‹ç®—æ•ˆç‡ï¼Œä¸¦ä¸”æä¾› Python å¥—ä»¶å¯ä»¥å®‰è£ã€‚
        * ä½† Flash Attention 2 å‰åªæ”¯æ´ Ampere (RTX30ç³»åˆ—) ä»¥ä¸Šçš„GPUä½¿ç”¨ï¼Œå› æ­¤è‹¥ç‚º Turing æ¶æ§‹ä»¥ä¸‹çš„GPUï¼Œæ”¹ç”¨SDPA æ˜¯å€‹ä¸éŒ¯çš„æ–¹æ¡ˆã€‚
        * ç­†è€…å¯¦æ¸¬ PyTorch çš„ SDPA åœ¨é€Ÿåº¦ã€è¨˜æ†¶é«”ç”¨é‡èˆ‡æº–ç¢ºç‡ä¸Šèˆ‡ Flash Attention 2 æ²’æœ‰å¤ªå¤§çš„å·®ç•°ï¼Œè€Œä¸”æœ‰è£ PyTorch å°±æœ‰ SDPA èƒ½ç”¨ï¼Œæ˜¯å€‹ç›¸å°æ–¹ä¾¿çš„é¸æ“‡ã€‚
    * **IO-Aware Tiling**
        * æŒ‡çš„æ˜¯æ‹†è§£ Softmax é‹ç®—ï¼Œé¿å…éœ€è¦å®Œæ•´æ‹œè¨ªæ‰€æœ‰å…ƒç´ ã€‚ 
        * é€™å°±åƒæ˜¯åœ¨çµ±è¨ˆæ ¡éš›æ’åæ™‚ï¼Œå¦‚æœä¸€æ¬¡æŠŠæ‰€æœ‰å­¸ç”Ÿçš„æˆç¸¾éƒ½æ‹¿ä¾†æ’åºï¼Œé‚£æœƒèŠ±æ¯”æ•¸å¤šçš„åŠ›æ°£ã€‚ä½†å¦‚æœå…ˆç”±å„å€‹ç­ç´šçµ±è¨ˆå‡ºç­æ’åï¼Œå†çµ±åˆèµ·ä¾†è¨ˆç®—æ ¡æ’åï¼Œå°±æœƒè¼•é¬†ä¸€äº›ã€‚
    * **Fused Kernel** 
        * æŒ‡çš„å‰‡æ˜¯åˆä½µæ³¨æ„åŠ›æ©Ÿåˆ¶çš„å„ç¨®é‹ç®—ï¼Œé›–ç„¶é€™å€‹é‹ç®—è£¡é¢çš„å¯¦åšé‚„æ˜¯åŒ…å«æ‰€æœ‰é‹ç®—ï¼Œä½†æ˜¯æ¸›å°‘äº†å½¼æ­¤æºé€šçš„é–‹éŠ·ã€‚
        * å°±åƒåŸæœ¬ä¸€å€‹ç”¢å“å¯èƒ½éœ€è¦ç¶“éå››äº”é–“æµæ°´ç·šå·¥å» æ‰èƒ½å®Œæˆï¼Œä½†ç¾åœ¨æˆç«‹ä¸€é–“æ•´åˆæ‰€æœ‰æµæ°´ç·šçš„å·¥å» ï¼Œæ¸›å°‘äº†å·¥å» ä¹‹é–“çš„æºé€šæˆæœ¬ï¼Œé€²è€Œææ˜‡ç”¢å“çš„å®Œå·¥æ•ˆç‡ã€‚
* ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼
    * [å®Œæ•´ç¨‹å¼ç¢¼](https://github.com/penut85420/LLM-Note-Labs/blob/main/HF/HF-Bench.py)
    * è‡ªå·±å˜—è©¦: .../LLM/project/hugging_face_transformers

---

## åƒè€ƒ
* [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
* [Hugging Face: LLM Tutorial](https://huggingface.co/docs/transformers/v4.33.2/en/llm_tutorial)
* [Hugging Face: NLP Course - Fast Tokenizer](https://huggingface.co/learn/nlp-course/chapter6/3)
* [Llama 2 Prompt Template Discussion](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3#64b71f7588b86014d7e2dd71)

---

## çµè«–
* ä»‹ç´¹äº†é–‹æºç¬‘è‡‰ Hugging Face Transformers å¥—ä»¶ï¼Œä¸¦è‘—é‡åœ¨æ¨è«–æ–¹é¢é€²è¡Œè¬›è§£ã€‚
* å¦‚æœæƒ³è¦æ›´ç­è§£ Hugging Faceï¼Œå¯ä»¥é—œæ³¨ä»–å€‘çš„[éƒ¨è½æ ¼](https://huggingface.co/blog)ï¼Œé–±è®€ä»–å€‘è±å¯Œçš„[æ–‡ä»¶](https://huggingface.co/docs)ï¼Œé‚„æœ‰è¨±å¤š[æ•™å­¸è³‡æº](https://huggingface.co/learn)å¯ä»¥åƒè€ƒã€‚
* æ˜¯ç‚ºäº† Traning è€Œç”Ÿçš„æ¡†æ¶ï¼Œå…¶ä¸»è¦çš„åŠŸèƒ½é‚„æ˜¯åœ¨**è¨“ç·´æ¨¡å‹**ä¸Šã€‚å› æ­¤åœ¨æ¨è«–é€™å¡Šï¼Œä¸¦**ä¸æ˜¯æœ€ä½³é¦–é¸**çš„æ¡†æ¶ã€‚

---