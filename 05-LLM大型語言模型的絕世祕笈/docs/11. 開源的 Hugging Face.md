# 開源的 Hugging Face Transformers
[Hugging Face](https://huggingface.co/) 🤗 [Transformers](https://huggingface.co/docs/transformers/index) 是訓練 Transformer 模型最知名的套件沒有之一，此套件收入了許多知名模型架構、訓練演算法以及各種模型權重的分享，使開發者可以輕鬆駕馭這些理論複雜的操作。
* Transformers 是個著重在訓練的框架，但也支援了相當豐富的推理功能。
    * 以下將會著重在如何使用此框架進行 LLM 的推論，並剖析 Transformer Decoder 進行 Autoregressive 的流程。

---

## 安裝
Transformers 支援的後端 ML 框架包含 PyTorch, Tensorflow, JAX 等等。
* 筆者是以 PyTorch 為主：
    ```bash
    # 安裝方式以官網為主： https://pytorch.org/get-started/previous-versions/
    conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia
    pip install transformers
    ```
* 使用 Transformers 套件時，也推薦安裝以下額外的套件進一步提升運作效率。
    ```bash
    pip install accelerate bitsandbytes optimum
    ```
    這些套件能夠讓我們使用量化 (Quantization) 機制來減少 GPU 記憶體消耗。

---

## 讀取模型
方式有很多種。
* 最常見的是使用 `.from_pretrained` 讀取：
    * 使用 TheBloke 大神提供的 [Llama-2 7B Chat](https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16) 為例：
        ```python
        from transformers import BitsAndBytesConfig
        from transformers import LlamaForCausalLM

        model_path = "TheBloke/Llama-2-7b-chat-fp16"
        
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,    # 約需 8 GiB GPU 記憶體
            # load_in_4bit=True,  # 約需 6 GiB GPU 記憶體
        )
        
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            low_cpu_mem_usage=True,
            quantization_config=bnb_config,
        )
        ```
        * 在 Colab 這種資源相對有限的環境中，需要使用**量化 (Quantization)** 技術來減少 GPU 記憶體的消耗
        * 透過 `BitsAndBytesConfig` 可以輕鬆地將模型量化 8 到 4 位元。
        * 為了啟用量化功能，需要將 `device_map` 設定為 `"auto"`，以確保模型權重都放在 GPU 裡面。
        * 如果 CPU 記憶體也不是很夠的話，那就需要將 `low_cpu_mem_usage` 設定為 `True` 才能順利讀取模型。
    * 以上步驟實際上會從 [Hugging Face Hub](https://huggingface.co/models) (HF Hub)下載模型的權重，預設會放在 `~/.cache/huggingface/` 裡面，也可以透過`cache_dir` 參數指定這些檔案要存放在哪裡：
        ```python
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            cache_dir="./cache_models"
        )
        ```
    * 多數模型都可以在 HF Hub 中找到，HF Hub 主要使用 Git 做管理的，所以也能直接用 `git clone` 將模型複製下來，在此前會需要先安裝 Git LFS 模組：
        ```bash
        sudo apt install -y git git-lfs
        
        # 啟用 Git LFS 功能
        git lfs install
        
        # 下載模型
        git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16
        ```
    * 下載完成後，直接指定資料夾路徑名稱即可讀取模型：
        ```bash
        LlamaForCausalLM.from_pretrained("Llama-2-7b-chat-fp16")
        ```
    * Git 傳輸完成大型檔案時並不會顯示下載圖紙，但如果你真的很想看網速圖紙紙條，其實也有辦法的：
        ```bash
        # 跳過所有 LFS 檔案的下載
        GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16
        cd Llama-2-7b-chat-fp16

        # 透過 Git LFS 進行 Fetch
        git lfs fetch
        git lfs checkout
        ```
        除非懷疑網路速度產生的問題，否則通常不太需要對方。
* 大部分模型權重上傳之後就不會在變動了，但 Git 的機制會讓下載下來的模型包含一份額外複製的權重做備份，因此 10 GB 的模型可能會吃掉 20 GB 的硬碟空間。
    * 一般情況下可以直接把權重資料夾中的 `.git` 資料夾刪除以節省硬碟空間，有需要再下載回來就好。 
* 有時可能會遇到有存取限制的模型 (Gated Model)。
    * 例如， Meta 官方的 Llama 系列就有存取限制。
    * 此時要先去該模型介紹頁面填寫存取表單。
    * 有些模型填完表單就能馬上存取了，但像 Meta Llama 就會需要等待一段時間才會收到核准通知。
* 有存取限制的模型，核准後就可以下載模型，需要一它方式才能下載模型。
    1. 設定 SSH 金鑰，然後透過 SSH 下載：
        ```bash
        git clone git@hf.co:meta-llama/Meta-Llama-3-8B
        ```
    2. 透過 `huggingface-cli` 登入：
        * 前往 HF 帳號設定的 Access Tokens 頁面，並建立一個新的 Token。
            * 如果只要下載模型的話，權限只要設定 Read 就好。
        * 使用建立好的 Access Token 登入：
        ```bash
        huggingface-cli login
        # 貼上 建立好的 Access Token
        ```
        * 下載模型：
        ```bash
        huggingface-cli download meta-llama/Meta-Llama-3-8B \
            --local-dir Models/Meta-Llama-3-8B
        ```
* [TheBloke](https://huggingface.co/TheBloke) 是 Hugging Face 社群相當活躍的一個用戶，每次有很猛的模型釋出時，他就會手刀衝第一個幫大家把模型的格式轉換成 HF 支援的格式，進行各種量化壓縮後上傳。據說他的[網路環境達數 GB/s](https://github.com/ggerganov/llama.cpp/issues/2262#issuecomment-1640712066) 之快，是個非常熱心的大神。
* 📝 範例程式碼
    * [完整程式碼](https://colab.research.google.com/drive/1wezFUpFlBXK-AwUjjd0vBl50qnjOwq6e?hl=en)
    * 自己嘗試: .../LLM/project/hugging_face_transformers

---

## 讀取分詞器
Tokenizer 的讀取方式，與模型讀取方式差不多，通常 Tokenizer 會跟模型權重的專案放在一起，所以直接用相同的命名空間下載即可。
```python
from transformers import LlamaTokenizer as TkCls

tk: TkCls = TkCls.from_pretrained("TheBloke/Llama-2-7b-chat-fp16")
```
* 也可以使用類似的 `LlamaTokenizerFast` 快速類別。
    * 一般版本的 Tokenizer 使用 Python 實做
    * 快速版本的 Tokenizer 使用 Rust 實做。
    * 如果要對大量長文本進行分詞的話，可以考慮使用快速版本，但如果只是要處理幾個短文本的話就不需要。
* 在舊版的 Transformers 裡面，有些快速版的 Tokenizer 需要初始化非常久。
    * 解決方法除了改用一般版本以外，也可以將 Tokenizer 讀出來之後，轉存快速版本：
    ```python
    from transformers import LlamaTokenizerFast as TkCls

    tk: TkCls = TkCls.from_pretrained("/path/to/tokenizer")
    tk.save_pretrained("/path/to/tokenizer")
    ```
    這個問題在最新版的 Transformers 裡已經被修改了

---

## 自動類別 (Auto Class)
已經知道 Llama 2 使用的是 Llama 架構，所以直接呼叫 `LlamaForCausalLM` 與 `LlamaTokenizer` 這兩個類別。但有些模型與 Tokenizer 第一時間可能無法判別的使用什麼架構，或是想寫出相容性更廣泛的方案碼，那就可以使用 Transformers 的 Auto Class 來進行讀取。
* 判別的使用架構：
    ```python
    from transformers import AutoModelForCausalLM as ModelCls
    from transformers import AutoTokenizer as TkCls

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelCls.from_pretrained(model_path)
    tk: TkCls = TkCls.from_pretrained(model_path)
    ```
    印出實際類別，發現還是具體的 Llama 類別：
    ```python
    print(type(model))  # <class 'transformers...LlamaForCausalLM'>
    print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>
    ```
    如果不想用快速版本的 Tokenizer 可以把 `use_fast` 參數設定成 False：
    ```python
    tk: TkCls = TkCls.from_pretrained(model_path, use_fast=False)
    print(type(tk))     # <class 'transformers...LlamaTokenizer'>
    ```
* 但 Auto Class 的缺點是**型別提示 (Tyoe Hint)** 比較少，就是在寫程式時 VSCode 會自己跳出的自動完成提示清單。
![RkddTFp](https://hackmd.io/_uploads/r1O0W1Mbxe.png =500x)
  如果非常仰賴 Type Hint 的開發者，可能要自己做 Type Annotation 來解。
* 其實大部分的方法都來自 `PreTrainedModel` 與 `PreTrainedTokenizer` 這兩個型別，因此可以給出型別註記：(別名)
    ```python
    from transformers import AutoModelForCausalLM as ModelImp
    from transformers import AutoTokenizer as TkImp
    from transformers import PreTrainedModel as ModelCls
    from transformers import PreTrainedTokenizer as TkCls

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelImp.from_pretrained(model_path)
    tk: TkCls = TkImp.from_pretrained(model_path)
    ```
    自動完成又回來啦！
    * 最新版的 Transformers 套件中，`AutoTokenizer.from_pretrained()` 已經會自動把回傳物件標記為 `PreTrainedTokenizer` 型別，所以分詞器的部分就不一定照以上放式做了。

---

## 文本生成 (Text Generation)
* 需要先將輸入字元 Token 並轉換成 PyTorch Tensor 物件：
    ```python
    model_path = "Llama-2-7b-chat-fp16"
    tk: TkCls = TkCls.from_pretrained(model_path)
    tokens = tk("Hello, ", return_tensors="pt")
    print(tokens) # Tokenize 結果
    """
    Output:
    {
        "input_ids": tensor([[1, 15043, 29892, 29871]]),
        "attention_mask": tensor([[1, 1, 1, 1]]),
    }
    """
    ```
    * Tensor 的字典：
        * `input_ids`: 分詞的結果。
        * `attention_mask`: 用來遮罩非輸入部分。
* 一開始只需要關注 `input_ids` ，先把 `attention_mask` 當成模型的必要輸入：
    ```python
    input_ids = tokens["input_ids"].to("cuda")
    print(input_ids)
    # tensor([[    1, 15043, 29892, 29871]], device='cuda:0')
    ```
    * 模型與 Tensor 必須放在同一個裝置中才能做運算。
        * 例如，放在 CPU 上的模型只能跟 CPU 上的 Tensor 運算。
        * 這裡模型放在 GPU 上，所以要用 `.to("cuda")` 將整個分詞放進 GPU 裡面。
* 開始使用 `model.generate` 進行文字產生：
    ```python
    output = model.generate(input_ids, max_new_tokens=32)
    print(tk.batch_decode(output))
    # <s> Hello, I am a beginner in Python ...
    ```
    * `model.generate` 會回傳產生的 Token ID，必須用 Tokenizer 進行 Decode 才能得到文字版的輸出。
    * `max_new_tokens` 是用來設定要輸出的 Token 數量。
        * 在 Transformer Decoder 裡面，輸出的Token數量越多，佔用的GPU記憶體就會越多，生成需要的時間理所當然的也比較久。
* 一次做很多個文字生成，俗稱**批次推論 (Batch Inference)**：
    ```python
    tk.pad_token = tk.eos_token  # LlamaTokenizer 沒有 Padding Token
    prompt = ["Hello, ", "Hi, my name is"]
    tokens = tk(prompt, return_tensors="pt", padding=True)
    input_ids = tokens["input_ids"].to("cuda")

    outputs = model.generate(input_ids, max_new_tokens=16)
    print(tk.batch_decode(outputs))
    ```
    * **注意**，因為 Llama Tokenizer 沒有預設填充用的 Padding Token，所以需要幫它指定一個，才能做填充。
    
* 有時會出現輸出結果怪怪的結果：
    ```
    [
        "<s> Hello, </s></s>0000000000000000",
        "<s> Hi, my name is [Your Name] and I am a [Your Profession] ...",
    ]
    ```
    * 因為 Transformers Decoder 是自回歸解碼 (Autoregressive) 的關係，放在越右邊的 Token 通常會對下一個生成的 Token 有越顯著的影響。
        * 若是將 Padding Token 都放右邊，模型看到一堆 Padding Token 後也不知道要輸出什麼。
        * 所以在進行批次生成時，記得要把 Padding Token 放在左邊，可以在初始化 Tokenizer 時設定 `padding_side` 這個參數。
    ```python
    tk: TkCls = TkCls.from_pretrained(model_path, padding_side="left")
    # 重新執行批次推論就能看到正常結果
    ...
    ```
    ```
    [
        "</s></s><s> Hello, I am a 35 year old woman ...",
        "<s> Hi, my name is [Your Name] and I am a [Your Profession] ...",
    ]
    ```
* 短句左側加上一堆 Padding Token 時是否會影響輸出？
    * Attention Mask 就是用來告訴模型 "我不是輸入！" 的一項資訊，模型看到一堆 Padding Token 會假裝沒看到。
    * 因此進行批次推論時，輸入適當的 Attention Mask 相當重要。

---

## 串流輸出 (Streaming)
進行長文本生成可以透過 `TextStreamer` 來進行串流輸出，來獲得即時回饋。不需要等上一段時間，才會看到完整的輸出。
* 利用 Tokenizer 初始化一個 `TextStreamer` 物件後當成參數丟進 `model.generate` 中：
    * **注意**，`TextStreamer` 只支援單一輸入，無法進行批次推論。
    ```python
    from transformers import TextStreamer
    
    inputs = tk("Hello, ", return_tensors="pt").to("cuda")
    output = model.generate(
        **inputs, 
        max_new_tokens=2048,
        streamer=TextStreamer(tk),
    )
    ```

---

## 提示樣板 (Prompt Template)
若要發動 LLM 的對話能力，我們需要借助提示樣本的機制。
* 提示樣本其實就是把使用者與模型之間一問一答的過程，寫成一個像是**聊天紀錄的格式**，用來「欺騙」語言模型生成下個聊天片段。
* 這個樣板會根據每個語言模型訓練的方式而有所不同，一般而言遵守模型開發者的建議會比較好。
* [Llama2 的官方樣板](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3#64b71f7588b86014d7e2dd71)：
    ```
    [INST] <使用者訊息> [/INST] <模型的回覆從這開始>
    ```
    * 使用相同樣板來詢問模型：
        ```python
        prompt = "[INST] 使用繁體中文回答，請問什麼是大型語言模型？[/INST] "

        inputs = tk(prompt, return_tensors="pt").to("cuda")
        output = model.generate(
            **inputs,
            max_new_tokens=128,
            streamer=TextStreamer(tk),
        )

        """
        輸出結果：
        <s> [INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] 大型語言模型（Large Language Model，LLM）是一種 ...</s>
        """
        ```
* 直接調用行開發者設定的模板： (樣板隨模型而變化)
    * 以 Google 的 Gemma 為例：
    * 使用 Transformer 套件內建的**聊天樣板 (Chat Template)** 功能，只要呼叫分詞器的 `apply_chat_template` 方法即可。
        ```python
        tk = AutoTokenizer.from_pretrained("google/gemma-2b-it")
        query = " 使用繁體中文回答，請問什麼是大型語言模型？ "
        conversation = [{"role": "user", "content": query}]
        prompt = tk.apply_chat_template(conversation, tokenize=False)
        print(prompt)

        """
        Output: <bos><start_of_turn>user
        使用繁體中文回答，請問什麼是大型語言模型？ <end_of_turn>
        """
        ```
    * 透過 `add_generation_prompt` 參數，可以把模型開始回答前的格式也附加上，讓模型比較不容易生成失敗。
        ```python
        tk = AutoTokenizer.from_pretrained("google/gemma-2b-it")
        query = " 使用繁體中文回答，請問什麼是大型語言模型？ "
        conversation = [{"role": "user", "content": query}]
        prompt = tk.apply_chat_template(
            conversation, 
            tokenize=False,
            add_generation_prompt=True,
        )
        print(prompt)

        """
        Output: <bos><start_of_turn>user
        使用繁體中文回答，請問什麼是大型語言模型？ <end_of_turn>
        <start_of_turn>model               <--- 模型開始回答前的格式
        """
        ```
    * 格式確認完畢後，來取得模型的輸入：
        ```python
        input_ids = tk.apply_chat_template(
            conversation, 
            # tokenize=False,
            return_tensors="pt",
            add_generation_prompt=True,
        ).to("cuda")
        print(input_ids)
        
        # Output: tensor([[2, 106, ..., 2516, 108]], device='cuda:0')
        ```
    * 輸入模型：
        因為沒有 `attention_mask`，所以輸入模型的方式有些不同。
        ```python
        # output = model.generate(**inputs) # 原本
        output = model.generate(input_ids)  # 現在
        ```
* Trasormers 套件的聊天樣板功能是使用 [Jinja](https://github.com/pallets/jinja/) 套件實作的。一些相對早期釋出的模型可能沒有在設定檔裡附上樣板，此時可以考慮自行撰寫聊天樣板。
    * 詳細用法請參考[官方教學](https://tinyurl.com/llm-chat-template)

---

## 取樣參數 (Sample Parameters)
模型在進行推論的時候，其實會生成一張機率表，用來代表下一個 Token 可能會是誰。
* 取樣參數就是我們**如何從這分機率表裡挑選下個 Token 的參數**。
* 可以在 `GenerationConfig` 類別裡面指定一些取樣參數，讓模型不會每次都挑機率最高的那個 Token 當輸出，使每次輸出都會長的不太一樣，提升輸出的多元型外。
    * 如果取樣的隨機性太高，可能會影響模型回答的正確性，請慎選。
* 常見的參數：
    | 參數名稱                 | 功能說明                                                     | 數值影響或建議使用方式                                       |
    | -------------------- | -------------------------------------------------------- | ------------------------------------------------- |
    | `do_sample`          | 控制是否使用隨機取樣。<br>-- `True` : 為隨機取樣。<br>-- `False` : 為 Greedy Decode，只取機率最高的 Token。     | 使用 Greedy Decode 時（`False`），建議不要設定其他取樣參數，以免結果不穩定。 |
    | `top_k`              | 從機率最高的前 K 個 token 中取樣。                                   | K 值越小，輸出越穩定；越大則輸出越多樣。                             |
    | `top_p`              | 累加機率直到總和超過 `top_p`，然後從這些 token 中取樣（又稱 nucleus sampling）。 | 彈性高，會根據實際機率分布自動決定取樣範圍；常用值如 0.9。                   |
    | `temperature`        | 調整機率分布的平滑度，改變 token 的出現機率。<br>改造機率表，temperature 越大時，第一名跟最後一名的距離就會越近，但彼此之間的相對關係還是保持不變。                       | 數值越高（如 1.0 以上），輸出更有創意；越低（如 0.1）輸出更嚴謹。             |
    | `repetition_penalty` | 懲罰重複的輸出，降低重複內容的生成可能性。                                    | `> 1.0` 抑制重複，`< 1.0` 容易重複。跳針情況下建議調高此參數。           |
* 取樣參數範例：
    ```python
    from transformers import GenerationConfig

    gen_config = GenerationConfig(
        max_new_tokens=16,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.75,
        repetition_penalty=1.1,
    )

    output = model.generate(**inputs, generation_config=gen_config, streamer=ts,)
    ```
* 這些參數也會對彼此之間產生不同程度的影響，因此選擇一組適合的參數進行生成任務，也是一堂相當重要的課題。
* Transformers 可以用來控制生成的參數還有很多，詳細資訊可以參考[官方文件](https://huggingface.co/docs/transformers/main_classes/text_generation)。

---

## 設定停止點 (Stopped Words)
停止點是用來告訴系統在生成的過程中，除了遇到 EOS (End-of-Sentence) Token 以外，還有遇到哪些 Token 應該停止輸出。
* 例如，將句號或換行符號當成停止點。
* 最基本的方法是設定 `GenerationConfig` 的 `eos_token_id`：
    ```python
    generation_config = GenerationConfig(
        eos_token_id=[
            tk.eos_token_id,       # 留著原本的 EOS
            tk.encode(".")[-1],    # 遇到句點停下
            tk.encode("\n")[-1],   # 遇到換行停下
            tk.encode("\n\n")[-1], # 遇到雙換行停下
        ],
    )
    ```
    * 此方法只適用於停止點是獨立一個 Token 的情況。如果停止點不只一個 Token 甚至是一個字串的話，需要自己實作 `StoppingCriteria` 類別。
* 自己實作 `StoppingCriteria` 類別：
    ```python
    from transformers import StoppingCriteria, StoppingCriteriaList


    class StopWords(StoppingCriteria):
        def __init__(self, tk: TkCls, stop_words: list[str]):
            self.tk = tk
            self.stop_tokens = stop_words

        def __call__(self, input_ids, *_) -> bool:
            s = self.tk.batch_decode(input_ids)[0]
            for t in self.stop_tokens:
                if s.endswith(t):
                    return True
            return False


    sw = StopWords(tk, ["。", "！", "？"])
    scl = StoppingCriteriaList([sw])
    ```
    * 每次系統檢查的時候，將整份輸出 Decode 回純文字，並且檢查文字的結尾是否符合使用者設定的停止點。
    * 此範例為單一輸入進行文本生成的情況，批次推論時情況會複雜許多。
* 接著將 `StopWords` 物件放進一個 `StoppingCriteriaList` 裡面，然後傳入 `model.generate` 裡面即可：
    ```python
    output = model.generate(
        input_ids,
        max_new_tokens=2048,
        streamer=TextStreamer(tk),
        stopping_criteria=scl,
    )
    # 模型在輸出遇到 "。！？" 時就會停下來了。
    ```
* Transformers 在 `GenerationConfig` 中新增了 `stop_strings` 的參數，可以更輕鬆的設定停止點：
    ```python
    from transformers import GenerationConfig

    config = GenerationConfig(
        stop_strings=[".", "\n"],
    )

    inputs = tk(["hello,", "goodbye, "], padding=True, return_tensors="pt")
    inputs = inputs.to(model.device)
    output = model.generate(**inputs, generation_config=config, tokenizer=tk)    
    ```
    * 這樣批次推論也能使用停止點功能了。

---

## 自迴歸解碼 (Autoregressive Decoding)
描述模型不斷生成 Token 的過程。模型生成的 Token 會重新變成模型的輸入，模型在根據這些新生成的 Token 繼續往下生成，直到觸發結束條件為止。
* 相對於非自迴歸 (Non-Autogressive, NAR) 模型而言，其差別在於自迴歸模型會把自己的輸出當成輸入，而 NAR 模型會一次把整個序列生出來。
* 在 LLM 裡面，每次**推論 (Inference)** 只會生成一個 Token，必須要經過多次推論，才能完成完整的文本**生成 (Generation)**。

### 拆解 model.generate 的背後原理：
* PyTorch 下，每個模型都可以直接透過 `.forward()` 來進行推論：
    * 使用 `model.forward(input_ids)` 與 `model(input_ids)` 同樣可以得到模型推論結果。
    * 使用 `.forward()` 當範例，住要是強調該步驟為前向運算，另外這樣的回傳結果也會有比較明確的型別提示。
    * 實際通常不使用 `.forward()` ，因為 PyTorch 中有一些錢處理與後處理的 Hooks ，如果使用它可能會忽略掉這些 Hoooks 導致某些環節處理不正常。
    ```python
    import torch

    input_ids = tokens["input_ids"].to("cuda")

    with torch.no_grad():
        output = model.forward(input_ids)
    ```
    * 產生一個類別為 `CausalLMOutputWithPast` 的輸出，裡面有兩個重要的資訊，分別為 `logits` 與 `past_key_values`。
        * `logits` : 
            * 模型預測下個 Token 的機率表。
            * 可以對 `logits` 做隨機取樣，或者直接計算 `argmax` 做取樣機最高的 Token 做 Greedy Decode。
        * `past_key_values` :
            * 俗稱**鍵值快取 (Key-Values Cache)**
            * 是模型對**已知輸入**的運算結果。
            * 因為自迴歸的特性，所以 Decoder LM 每次生成的輸出都會變成下個回合的輸入，因此這個 KV Cache 會越長越大，同時也是**消耗 GPU 記憶體的元兇之一**。
* 有了這份 KV Cache，就不用每次推論時都把整個 `input_ids` 丟進去，只要留新長出來的 Token 就好，因為舊的輸入都已經被模型 Decode 成 KV Cache 了。
    * 下個回合的推論就會長這樣：
        ```python
        with torch.no_grad():
            outputs = model(
                outputs.logits.argmax(-1)[:, -1:],  # Greedy Decode
                past_key_values=output.past_key_values,
            )
        ```
* 將此邏輯整理成一個迴圈來進行：
    ```python
    pkv = None
    output_tokens = list()
    for i in range(16):
        with torch.no_grad():
            outputs = model.forward(input_ids, past_key_values=pkv)
        
        next_token = outputs.logits.argmax(-1)[:, -1:]
        token_id = next_token.item()

        if token_id == tk.eos_token_id:
            break

        output_tokens.append(token_id)
        input_ids = next_token
        pkv = outputs.past_key_values

    print(tk.decode(output_tokens))
    # 輸出結果：大型語言模型（Large Language Model，LLM）
    ```
    * 這邊固定進行 16 次推論，中間如果遇到 EOS Token 就會跳出迴圈。
    * 拆解生成步驟的好處：
        * 可以針對 `logits` 的機率分佈做觀察。
        * 有些時候可以看到模型在「**說謊**」時，那個部份輸出的 Token 的機率分佈會特別的低，因此也可以用來計算模型的**信心度**之類的。
* `torch.no_grad()` 是用來停用梯度的 Context Manager。
    * 因為 PyTorch 的模型在進行計算時，預設都會順便算出一分梯度用來訓練。
    * 但這裡不是要做訓練，所以用它來鍵少不必要的計算，也可以避免語言模型產生過大的梯度而導致記憶體不足。
    * 使用 `torch.inference_mode()` 也有類似的效果：
        ```python
        with torch.no_grad():
            outputs = model.forward(...) 

        @torch.inference_mode()
        def main():
            outputs = model.forward(...)
        ```

---

## 多顯卡推論 (Multi-GPUs)
* 當機器有多顆 GPUs 時，指定要使用哪顆 GPU 進行運算：
    1. 可以透過環境變數 `CUDA_VISIBLE_DEVICES` 來指定：
        ```bash
        # 指定使用 1 號 GPU 進行運算
        CUDA_VISIBLE_DEVICES=1 python main.py
        ```
    2. 透過 Python 的 `os.environ` 來指定：
        ```python
        import os

        os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

        import transformers
        ```
        * 在指定環境變數時，一定要==先於匯入 Transformers 套件==，否則容易失效。
* 使用多張 GPUs 同時進行推論的程式碼：
    * ==要記得將 `device_map` 設定為 `"auto"`==，剩餘程式碼與一般推論基本相同，
    ```python
    import os

    # 使用 0 號與 1 號 GPU 同時進行推論
    os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

    from transformers import LlamaForCausalLM as ModelCls
    from transformers import LlamaTokenizer as TkCls
    from transformers import TextStreamer

    model_path = "TheBloke/Llama-2-7b-chat-fp16"
    model: ModelCls = ModelCls.from_pretrained(
        model_path,
        device_map="auto"
    )
    tk: TkCls = TkCls.from_pretrained(model_path)
    ts = TextStreamer(tk)

    prompt = "Hello, "
    input_ids = tk(prompt, return_tensors="pt")["input_ids"].to("cuda")
    model.generate(input_ids, max_new_tokens=16, streamer=ts)
    ```

---

## Candle
同樣由 Hugging Face 開發的 [Candle](https://github.com/huggingface/candle) 框架，是使用 Rust 語言開發的機器學習框架。
* Rust 是以高效率、高記憶體安全聞名的程式語言。
* 📝 範例程式碼
    * [Candle Llama 2 Demo](https://huggingface.co/spaces/lmz/candle-llama2)
    * 自己嘗試: .../LLM/project/hugging_face_transformers

---

## 語言模型到底是 "大" 在哪裡？
通常在談論一個模型的效能瓶頸時，會分成兩個部分來看：
1. GPU 的**運算能力**： 能算的多**快**。
2. GPU 的**記憶體容量**： 能放下多**大**的模型。

可寫一個側瓶速度與記憶體的程式：
1. 建立假的輸入資料，設定長度與批次大小，並且全部填 0。
    ```python
    import torch

    batch_size, seqlen = 1, 1024
    torch.zeros(batch_size, dtype=torch.int64)
    ```
2. (記憶體) 透過 PyTorch 套件取得 GPU 記憶體的用量：
    ```python
    # 取得讀取模型後的 GPU 記憶體使用量
    unit = 1024**2 # MiB
    init_mem = torch.cuda.memory_reserved() / unit
    print(f"Model Weight Memory Usage: {init_mem:.0f} MiB")
    ```
3. (速度) 借助 `tqdm` 進度條套件來測量：
    ```python
    from tqdm import trange

    n_decode = 128
    with trange(n_decode, ncols=100) as prog:
        for i in prog:
            # ... Autoregressive Decoding
            total_mem = torch.cuda.memory_reserved() / unit
            prog.desc = f"Memory Usage: {total_mem:.0f} MiB"
    ```
4. 在迴圈內放入自回歸解碼的程式，並在每一次迴圈結束後計算記憶體的用量，來觀察記憶體隨著生成的過程而產生的變化。
    * 這份程式碼的參數包含：
        * 模型路徑、批次大小、初始輸入長度、後續輸出的長度、注意力機制的實作種類。
5. 測試 Llama2 7B 的速度： (以 RTX 3090 做測試)
    ```bash
    $ python HF-Bench.py meta-llama/Llama-2-7b-hf
    
    Model Weight Memory Usage: 12854 MibB
    Sequence Length: 1024
    Memory: 3332/16186 MiB: 1024/1024 [00:26<00:00, 39.21it/s]
    Decode Length: 1024
    Total Length: 2048
    ```
    * 結果顯示，開始推論前，模型權重佔用了 12854 MB 的 GPU 記憶體，執行到最後用掉了 3332 MB，總共佔用 16186 MB，花費 26 秒執行，平均每秒可以推論 39.12 次，最後輸入加上輸出的長度為 2048。
6. 測試不同輸入長度：
    | 長度 | 次/秒          |
    | ---- | -------------- |
    | 2K   | 35             |
    | 4K   | 29             |
    | 5K   | 25             |
    | 6K   | ? (記憶體爆炸) |
    * 隨著長度增加，整體計算量提升，速度變慢。
    * 直接推 6K 推不動，但改從 5 K 開始並解碼卻能解到 10K (總長)。
        * Decoder 語言模型通常中，通常會使用**預填充** (Prefill) 代表一開始處理輸入的階段，並使用**生成** (Generate) 或**解碼** (Decode)來代表後續產生逐個 Token 輸出的過程。
        * 一般而言 Transformer Decoder 模型會在 Prefill 階段消耗掉大量記憶體，這是因為**注意力機制的運算會在 Prefill 階段產生很大的 Peak Memory**。
        * 後續生成階段因為每一次只需要處理一個 Token，所以記憶體的消耗相對較少。
    * 現實應用層面來說，只能輸入 5K 實在太少。
* 讓輸入更長一點的方法： (將注意力實作改成 `sdpa`)
    ```bash
    $ python HF-Bench.py meta-llama/Llama-2-7b-hf \
          --seqlen-k=7 --decode-k=8 --attn=sdpa

    Model Weight Memory Usage: 12854 MibB
    Sequence Length: 7168
    Memory: 11034/23888 MiB (OOM): 2562/8192 [01:45<03:52, 24.26it/s]
    Decode Length: 2562
    Total Length: 9731
    ```
    * 輸入可從 7K 一度推到 9K 了。雖然總長變短，但差距並不算大。
* 補充說明：
    * **Scaled Dot Product Attention** (SDPA) 
        * 是 PyTorch 2.0 的新功能之一，可以減少 Attention 計算的 Peak Memory，讓我們能用更少的記憶體去計算更長的序列輸入。
        * 背後的原理之一就是傳說的：**Flash Attention**。
    * **Flash Attention**
        * 由史丹佛大學的博士生 Tri Dao 所提出,作者指出 Attention 運算最大的瓶頸在於記憶體,尤其是Softmax 運算因為需要拜訪所有元素來計算各自的機率,所以消耗的記憶體特別巨大，因此透過 **IO-Aware Tiling** 與 **Fused Kernel** 這兩個核心技術來突破記憶體瓶頸。
        * 作者後來又提出了改良版的 Flash Attention 2，進一步改善了運算效率，並且提供 Python 套件可以安裝。
        * 但 Flash Attention 2 前只支援 Ampere (RTX30系列) 以上的GPU使用，因此若為 Turing 架構以下的GPU，改用SDPA 是個不錯的方案。
        * 筆者實測 PyTorch 的 SDPA 在速度、記憶體用量與準確率上與 Flash Attention 2 沒有太大的差異，而且有裝 PyTorch 就有 SDPA 能用，是個相對方便的選擇。
    * **IO-Aware Tiling**
        * 指的是拆解 Softmax 運算，避免需要完整拜訪所有元素。 
        * 這就像是在統計校際排名時，如果一次把所有學生的成績都拿來排序，那會花比數多的力氣。但如果先由各個班級統計出班排名，再統合起來計算校排名，就會輕鬆一些。
    * **Fused Kernel** 
        * 指的則是合併注意力機制的各種運算，雖然這個運算裡面的實做還是包含所有運算，但是減少了彼此溝通的開銷。
        * 就像原本一個產品可能需要經過四五間流水線工廠才能完成，但現在成立一間整合所有流水線的工廠，減少了工廠之間的溝通成本，進而提昇產品的完工效率。
* 📝 範例程式碼
    * [完整程式碼](https://github.com/penut85420/LLM-Note-Labs/blob/main/HF/HF-Bench.py)
    * 自己嘗試: .../LLM/project/hugging_face_transformers

---

## 參考
* [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
* [Hugging Face: LLM Tutorial](https://huggingface.co/docs/transformers/v4.33.2/en/llm_tutorial)
* [Hugging Face: NLP Course - Fast Tokenizer](https://huggingface.co/learn/nlp-course/chapter6/3)
* [Llama 2 Prompt Template Discussion](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3#64b71f7588b86014d7e2dd71)

---

## 結論
* 介紹了開源笑臉 Hugging Face Transformers 套件，並著重在推論方面進行講解。
* 如果想要更瞭解 Hugging Face，可以關注他們的[部落格](https://huggingface.co/blog)，閱讀他們豐富的[文件](https://huggingface.co/docs)，還有許多[教學資源](https://huggingface.co/learn)可以參考。
* 是為了 Traning 而生的框架，其主要的功能還是在**訓練模型**上。因此在推論這塊，並**不是最佳首選**的框架。

---