# å†è¨ªæª¢ç´¢æ¨¡å‹
**è³‡è¨Šæª¢ç´¢ (Information Retrieval, IR)** æ˜¯åœ¨ç ”ç©¶å¦‚ä½•å¿«é€Ÿæœå°‹åˆ°ä½¿ç”¨è€…æƒ³è¦æ‰¾çš„çµæœã€‚
* ä¾‹å¦‚å¤§å®¶å¤©å¤©åœ¨ç”¨çš„ Google Search å°±æ˜¯æœ€ç¶“å…¸çš„ä¸€ç¨®è³‡è¨Šæª¢ç´¢æŸ¥ç”¨ï¼Œç›¸è¼ƒæ–¼ç”Ÿæˆå¼èªè¨€æ¨¡å‹è€Œè¨€ï¼Œæ˜¯å€‹å·²ç¶“ç™¼å±•è¨±ä¹…ä¸”ç›¸å°æˆç†Ÿçš„é ˜åŸŸã€‚
* æƒ³è¦å°å¤§é‡æ–‡æª”å»ºç«‹å¿«é€Ÿçš„æª¢ç´¢å¼•æ“ï¼Œé¦–å…ˆéœ€è¦**å»ºç«‹ç´¢å¼• (Indexing)**ï¼Œæ–¹æ³•åŒ…å«é—œéµå­—æå–ã€å‘é‡ç©ºé–“æˆ– BM25 ç­‰ç­‰ã€‚æ¥è‘—æ ¹æ“šä½¿ç”¨è€…çš„æŸ¥è©¢ï¼Œå°‹æ‰¾æœ€ç›¸ä¼¼çš„è³‡è¨Šä¸¦é€²è¡Œ**æ’å (Ranking)**ï¼Œé€™æ•´å€‹æœå°‹çš„æ¨¡å¼è¢«ç¨±ç‚º**æª¢ç´¢æ¨¡å‹ (Retrieval Model)**ã€‚
* å°‡èªè¨€æ¨¡å‹èˆ‡è³‡è¨Šæª¢ç´¢çµåˆï¼Œä¸åƒ…èƒ½è®“èªè¨€æ¨¡å‹æ¥è§¸åˆ°å¤–éƒ¨çŸ¥è­˜ï¼Œæ”¹å–„èªè¨€æ¨¡å‹ç„¡æ³•æ›´æ–°è³‡è¨Šçš„å•é¡Œï¼Œä¹Ÿèƒ½ææ˜‡å›ç­”çš„å“è³ªä¸¦æ¸›å°‘éŒ¯èª¤èˆ‡å¹»è¦ºï¼Œè‹¥æ˜¯é€²ä¸€æ­¥èˆ‡æƒ…å¢ƒå­¸ç¿’çµåˆï¼Œä¾¿èƒ½ä½¿æ¨¡å‹é€²è¡Œæ›´ç²¾æº–çš„ä»»å‹™ã€‚

---

## é—œéµå­—
**é—œéµå­— (Keyword)** æ˜¯å€‹ç°¡å–®æœ‰æ•ˆå»ç¶“å¸¸è¢«å¿½ç•¥çš„åšæ³•ï¼Œé›–ç„¶ä¸­æ–‡çš„é—œéµå­—ç¶“å¸¸æœƒé‡åˆ°æ–·è©èˆ‡é‚Šç•Œçš„å•é¡Œï¼Œä½†æ˜¯åœ¨ç°¡å–®çš„å¯¦åšä¸Šï¼Œèƒ½ä»¥å­—å…ƒç‚ºå–®ä½çš„æ–¹å¼ä¾†è™•ç†ã€‚
* å…¶ä¸­æœ€å¸¸è¢«ç”¨ä¾†å¯¦åšé—œéµå­—ç´¢å¼•çš„æ–¹æ³•å°±æ˜¯ N-Gram ç´¢å¼•ã€‚
* N-Gram çš„æ¦‚å¿µå¾ˆå–®ç´”ï¼Œä¸€æ¬¡ä»¥ä¸€å€‹å­—æˆ–å…©å€‹å­—ç‚ºå–®ä½çš„æ–¹å¼ä¾†å»ºç«‹ç´¢å¼•ã€‚
    1. å‡è¨­æœ‰å…©ä»½æ–‡æœ¬å¦‚ä¸‹ï¼š
        ```
        æ–‡æœ¬ Aï¼šä»Šå¤©å¤©æ°£çœŸå¥½
        æ–‡æœ¬ Bï¼šä»Šå¤©å‡ºé–€å¯å¥½
        ```
    2. è‹¥ä»¥ N-Gram å»ºç«‹ç´¢å¼•çš„è©±ï¼Œæœƒå°‡æ–‡æœ¬åˆ‡æˆä»¥ä¸‹ç‰‡æ®µï¼š
        ```
        ä»Šã€å¤©ã€å¤©ã€æ°£ã€çœŸã€å¥½ã€ä»Šå¤©ã€å¤©å¤©ã€å¤©æ°£ã€æ°£çœŸã€çœŸå¥½ã€ä»Šå¤©å¤©ã€...
        ä»Šã€å¤©ã€å‡ºã€é–€ã€å¯ã€å¥½ã€ä»Šå¤©ã€å¤©å‡ºã€å‡ºé–€ã€é–€å¯ã€å¯å¥½ã€ä»Šå¤©å‡ºã€...
        ```
    3. æœ€å¾Œå°‡é€™äº›ç´¢å¼•èˆ‡æ–‡æœ¬å°æ‡‰èµ·ä¾†ï¼š
        ```
        ä»Š: A, B
        å¤©: A, B
        æ°£: A
        é–€: B
        ä»Šå¤©: A, B
        å¤©å¤©: A
        å¤©å‡º: B
        ```
    * ä¾æ­¤é¡æ¨ï¼Œé›–ç„¶æœƒåˆ‡å‡ºå¾ˆå¤šä¸æˆè©å½™çš„ç‰‡æ®µï¼Œä½†æ˜¯åœ¨æ–‡æœ¬é‡è¼ƒå°‘çš„æƒ…æ³ä¸‹ï¼Œå½±éŸ¿é€šå¸¸ä¸å¤§ã€‚
* è‹¥æ˜¯å°‡åˆ—èˆ‰ N-Gram çš„é‚è¼¯å¯«æˆ Python æœƒæ˜¯é€™æ¨£ï¼š
    ```python
    def ngram(text: str, n: int):
        for i in range(0, len(text) - n + 1):
            yield text[i : i + n]


    def all_ngram(text: str, a, b, step=1):
        a = max(1, a)
        b = min(len(text), b)
        for i in range(a, b + 1, step):
            for seg in ngram(text, i):
                yield seg
    ```
    * ä½¿ç”¨èµ·ä¾†æœƒåƒé€™æ¨£ï¼š
    ```python
    # å¾ 2-Gram é–‹å§‹ï¼Œæ¯æ¬¡éå¢ 2ï¼Œæœ€å¤§åˆ° 6-Gram
    # å› æ­¤æœƒæœ‰ 2-Gramï¼Œ 4-Gramï¼Œ 6-Gram çš„çµæœ
    
    segs = [seg for seg in all_ngram("ä»Šå¤©å¤©æ°£çœŸå¥½", 2, 6, 2)]
    print(segs)
    # ['ä»Šå¤©', 'å¤©å¤©', 'å¤©æ°£', 'æ°£çœŸ', 'çœŸå¥½', 'ä»Šå¤©å¤©æ°£', 'å¤©å¤©æ°£çœŸ', 'å¤©æ°£çœŸå¥½', 'ä»Šå¤©å¤©æ°£çœŸå¥½']
    ```
    * ä½¿ç”¨è€…è¼¸å…¥çš„æŸ¥è©¢å¯èƒ½æœƒå¾—å‡ºå¾ˆå¤šç‰‡æ®µï¼Œé€™æ™‚å¯ä»¥ä½¿ç”¨ [**Jaccard Similarity**](https://en.wikipedia.org/wiki/Jaccard_index) åšæ’åºï¼Œä¹Ÿå°±æ˜¯å°‡å½¼æ­¤æ‰€æœ‰ N-Gram çš„äº¤é›†é™¤ä»¥è¯é›†ï¼š
    ```python
    def calc_jaccard(query: str, chunk: str):
        query_length = len(query)
        query_ngrams = {s for s in all_ngram(query, 1, query_length)}
        chunk_ngrams = {s for s in all_ngram(chunk, 1, query_length)}

        inter = query_ngrams & chunk_ngrams
        union = query_ngrams | chunk_ngrams

        return len(inter) / len(union)
    ```
    é€éé€™äº›æŠ€å·§å°±èƒ½å»ºç«‹å‡ºå®Œæ•´çš„N-Gram æœå°‹å¼•æ“å›‰ï¼
* åœ¨æ‡‰ä»˜å°‘é‡æ–‡æœ¬æ™‚ï¼Œä½¿ç”¨N-Gram å»ºç«‹ç´¢å¼•é€šå¸¸ä¸æœƒæœ‰å¤ªå¤§çš„å•é¡Œã€‚ä½†æ˜¯ç•¶æ–‡æœ¬é‡é€æ¼¸ä¸Šå‡æ™‚ï¼Œç´¢å¼•æœ¬èº«æ‰€ä½”ç”¨çš„è¨˜æ†¶é«”æœƒçˆ†ç‚¸æ€§æˆé•·ã€‚åŠ ä¸Šé€™ç¨®ç´¢å¼•åªèƒ½åšæ–‡å­—æ¯”å°ï¼Œä¸¦æ²’æœ‰è¾¦æ³•é€²è¡Œç›¸è¿‘èªæ„çš„æ¯”è¼ƒï¼Œæ–·è©é‚Šç•Œä¹Ÿæ˜¯å€‹å¤§å•é¡Œï¼Œå› æ­¤ä¸æ˜¯æœ€ç†æƒ³çš„åšæ³•ã€‚
* ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼
    * [ç­†è€…ç¨‹å¼ç¢¼](https://tinyurl.com/llm-note-12)
    * è‡ªå·±å˜—è©¦: .../LLM/project/search

---

## BM25 Best Matching Ranking
**[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) (Best Matching 25)** æ˜¯ä¸€ç¨®åŸºæ–¼ [TF-IDF](https://zh.wikipedia.org/wiki/Tf-idf) çš„çµ±è¨ˆæ’åæ–¹æ³•ï¼Œé›–ç„¶æ˜¯å€‹å¤è€çš„æ¼”ç®—æ³•ï¼Œä½†é‚„æ˜¯ç›¸ç•¶ç°¡å–®æœ‰æ•ˆã€‚
* å¯ä»¥é€é [Rank-BM25](https://github.com/dorianbrown/rank_bm25) é€™å€‹ Python å¥—ä»¶è¼•é¬†ä½¿ç”¨é€™å€‹ç¶“å…¸æ¼”ç®—æ³•ï¼š
    ```python
    from rank_bm25 import BM25Okapi

    corpus = ["é€™æ˜¯ç¬¬ä¸€ç¯‡æ–‡æª”", "é€™æ˜¯ç¬¬äºŒç¯‡æ–‡æª”", "æ–‡æª”çš„å…§å®¹å¾ˆé‡è¦"]

    #  ä»¥å­—å…ƒç‚ºå–®ä½æ–·è©
    corpus_chars = [[ch for ch in doc] for doc in corpus]
    bm25 = BM25Okapi(corpus_chars)

    query = "é‡è¦"
    query_chars = [ch for ch in query]

    # è¨ˆç®— BM25 åˆ†æ•¸
    scores = bm25.get_scores(query_chars)
    print(f"Scores: {scores}")

    # è¼¸å‡ºæŸ¥è©¢çµæœ
    best = scores.argmax()
    print(f"Query: {query}")
    print(f"Result: {corpus[best]}")

    """
    è¼¸å‡º:
    Scores: [0. 0. 0.98149902]
    User Query: é‡è¦
    Best Result: æ–‡æª”çš„å…§å®¹å¾ˆé‡è¦
    """
    ```
* BM25 æ˜¯ä¸€å€‹æ’åæ¼”ç®—æ³•ï¼Œå¯ä»¥èˆ‡å…¶ä»–æª¢ç´¢æ–¹æ³•åšçµåˆï¼Œä¾‹å¦‚å…ˆç”¨ N-Gram æˆ– Embedding é€²è¡Œæ¯”è¼ƒå¤§ç¯„åœçš„æ¨¡ç³Šæœå°‹ï¼Œå†ç”¨ BM25 åšæ’åºä¹‹é¡çš„ã€‚

---

## æ–‡æœ¬å‘é‡ Text Embedding
å…ˆå‰çš„ç« ç¯€å·²ç¶“ä»‹ç´¹é Embedding æ˜¯ä¸€ç¨®**å°‡æ–‡å­—å‘é‡åŒ–**çš„çµæœï¼Œè‹¥ Embeddling ä»¥å­—è©ç‚ºå–®ä½ï¼Œå‰‡ç¨±ç‚º**è©å‘é‡ (Word Embedding)**ï¼Œè‹¥ä»¥å¥å­ç‚ºå–®ä½å‰‡ç¨±ç‚º**æ–‡å¥å‘é‡ (Sentence Embedding)**ï¼Œä¹Ÿæœ‰ä»¥æ–‡ä»¶ç‚ºå–®ä½çš„**æ–‡ä»¶å‘é‡ (Document Embedding)**ï¼Œå› æ­¤ Embedding ä¸€è©çš„ç”¨æ³•æ˜¯å¾ˆå½ˆæ€§çš„ã€‚
* è£œå……ï¼š
    * å–®ä½è¼ƒå¤§çš„å‘é‡ï¼Œä¾‹å¦‚ä¸€å¥è©±æˆ–ä¸€ä»½æ–‡ä»¶ï¼Œé€šå¸¸æ˜¯ç”±å¤šå€‹å–®ä½è¼ƒå°çš„å‘é‡åˆä½µè¨ˆç®—å‡ºä¾†çš„ã€‚
        * ä¾‹å¦‚ Transformer Encoder æœƒå°‡ä¸€å¥è©±åˆ‡æˆæ•¸å€‹ Tokens å¾Œå¾—åˆ° Token Embeddingï¼Œç¶“éæ•¸å±¤ Transformer Blocks çš„ç²¾å¯†è¨ˆç®—ï¼ŒåŠ ä¸Š Pooling é‹ç®—å¾—åˆ°æœ€çµ‚çš„ Sentence Embeddingã€‚
* ä¸€èˆ¬ä¾†èªªï¼Œåœ¨æª¢ç´¢æ¨¡å‹è£¡é¢ä½¿ç”¨çš„éƒ½æ˜¯ä»¥å¥å­æˆ–æ–‡ä»¶ç‚ºå–®ä½ï¼Œé€™æ¨£å°±å¯ä»¥å°‡ä¸åŒé•·åº¦çš„å¥å­éƒ½ç·¨ç¢¼æˆå›ºå®šç¶­åº¦çš„å‘é‡ï¼Œé€éä¸€å€‹ç°¡å–®çš„çŸ©é™£é‹ç®—ä¾¿èƒ½å¿«é€Ÿè¨ˆç®—ç›¸ä¼¼åº¦ã€‚
* å°ˆé–€ç”¨ä¾†ç”¢ç”Ÿé€™ç¨®å‘é‡çš„æ¨¡å‹æœ‰å¾ˆå¤šï¼Œä½†æ˜¯æœ‰æ”¯æ´ä¸­æ–‡æˆ–å¤šèªçš„é¸æ“‡ç›¸å°è¼ƒå°‘ã€‚
1. ä¸€å€‹å¤è€ä½†æ˜¯ç¶“å…¸çš„é¸æ“‡æ˜¯ Google çš„ [Universal Sentence Encoder](https://www.kaggle.com/models/google/universal-sentence-encoder/) ç³»åˆ—ï¼Œå…¶ä¸­çš„ [Multilingual Large](https://tinyurl.com/llm-useml) æ˜¯ç­†è€…è¼ƒæ¨è–¦çš„ç‰ˆæœ¬ã€‚
    * é¦–å…ˆè¦å®‰è£å¥—ä»¶ï¼š
        ```bash
        pip install tensorflow tensorflow_hub tensorflow_text
        ```
    * åŸºæœ¬ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š
        ```python
        import numpy as np
        import tensorflow as tf
        import tensorflow_hub as hub
        import tensorflow_text

        # # ä¸è®“ Tensorflow ä½¿ç”¨ GPU
        # tf.config.set_visible_devices([], "GPU")

        # ä¸‹è¼‰ & è®€å–æ¨¡å‹
        hub_url = (
            "https://www.kaggle.com/models/google/",
            "universal-sentence-encoder/tensorFlow2/",
            "multilingual-large/2"
        )

        # ä¹Ÿå¯ä»¥æŠŠæ¨¡å‹ä¸‹è¼‰ä¸‹ä¾†å¾Œï¼Œç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾‘çºŒå–
        model = hub.load(hub_url)

        # è¨ˆç®—å„è‡ªçš„ Embedding
        # ã€Œê³ ì–‘ì´ã€è·Ÿã€Œê°œã€åˆ†åˆ¥æ˜¯éŸ“æ–‡çš„è²“å’Œç‹—
        text = ["cat", "è²“", "ê³ ì–‘ì´", "dog", "ç‹—", "ê°œ"]
        embeddings = model(text)

        # è¨ˆç®—å…§ç©ä½œç‚ºç›¸ä¼¼åº¦
        similarity = np.inner(embeddings, embeddings)
        np.set_printoptions(precision=2)
        print(similarity)
        ```
        * è¼¸å‡ºçµæœå¦‚ä¸‹ï¼š
        ```
        [
            [1.      0.94    0.92    0.75    0.76    0.73]
            [0.94    1.      0.92    0.73    0.75    0.73]
            [0.92    0.92    1.      0.61    0.63    0.62]
            [0.75    0.73    0.61    1.      0.98    0.98]
            [0.96    0.75    0.63    0.98    1.      0.97]
            [0.73    0.73    0.62    0.98    0.97    1.  ]
        ]
        ```
        å¯ä»¥çœ‹åˆ°è²“ï¼Œê³ ì–‘ì´ è·Ÿ cat ä¹‹é–“çš„å…§ç©è¼ƒå¤§ï¼Œä»£è¡¨ä»–å€‘åœ¨èªæ„ä¸Šè¼ƒç‚ºç›¸è¿‘ï¼Œä½†ä»–å€‘è·Ÿã€Œç‹—ã€çš„å…§ç©å°±æ¯”è¼ƒå°ï¼Œä»£è¡¨ä»–å€‘çš„èªæ„æ¯”è¼ƒä¸ç›¸è¿‘ã€‚
    * æ³¨æ„ï¼š
        * å¦‚æœåœ¨ Tensorflow èˆ‡ PyTorch æ··ç”¨çš„å°ˆæ¡ˆè£¡é¢ï¼Œå…ˆè®€å– Tensorflow æ¨¡å‹å¯èƒ½æœƒé€ æˆå…©å€‹å¥—ä»¶æ¶ç”¨ GPU çš„å•é¡Œï¼Œè§£æ±ºæ–¹æ³•ä¹‹ä¸€æ˜¯å…ˆåŒ¯å…¥ PyTorch ç›¸é—œçš„å¥—ä»¶ï¼Œæˆ–è€…é€é`tf.config.set_visible_devices([], "GPU")` ä¾†åœç”¨ Tensorflow ä½¿ç”¨ GPU é€²è¡Œé‹ç®—ï¼Œå› ç‚ºé€™å€‹æ¨¡å‹å¾ˆå°ï¼Œæ‰€ä»¥åœ¨æ²’æœ‰ GPU çš„æƒ…æ³ä¸‹ä¹Ÿå¯ä»¥ç®—çš„å¾ˆå¿«ã€‚
        * å¦å¤–è¦æ³¨æ„ `tensorflow_text` é›–ç„¶çœ‹èµ·ä¾†æ²’æœ‰ç”¨åˆ°ï¼Œä½†æ˜¯å¦‚æœæ²’æœ‰å…ˆåŒ¯å…¥çš„è©±æœƒç™¼ç”Ÿ `RuntimeError` å–”ï¼
2. ç¬¬äºŒå€‹é¸æ“‡æ˜¯ä½¿ç”¨è·Ÿ Hugging Face ç”Ÿæ…‹ç³»è¼ƒè²¼è¿‘çš„ [**Sentence Transformers**](https://www.sbert.net/) æ¨¡å‹ï¼Œåœ¨ HF Hub ä¸Šå¯ä»¥æ‰¾åˆ°å¾ˆå¤š[**ç›¸é—œçš„æ¨¡å‹**](https://huggingface.co/sentence-transformers)ã€‚
    * é¦–å…ˆè¦å®‰è£å¥—ä»¶ï¼š
        ```bash
        pip install sentence_transformers
        ```
    * åŸºæœ¬ç”¨æ³•å¦‚ä¸‹ï¼š
        ```python
        import numpy as np
        from sentence_transformers import SentenceTransformer

        # ä¸‹è¼‰ & è®€å–æ¨¡å‹
        model = SentenceTransformer("intfloat/multilingual-e5-base")

        # å–å¾— Embedding
        text = ["cat", "è²“", "ê³ ì–‘ì´", "dog", "ç‹—", "ê°œ"]
        embeddings = model.encode(text, normalize_embeddings=True)

        # è¨ˆç®—å…§ç©ä½œç‚ºç›¸ä¼¼åº¦
        similarity = np.inner(embeddings, embeddings)
        np.set_printoptions(precision=2)
        print(similarity)
        ```
    * é™¤äº†ä»¥ä¸Šç¯„ä¾‹ä½¿ç”¨çš„ E5 ä»¥å¤–ï¼Œé‚„æœ‰ [BGE-M3](https://huggingface.co/BAAI/bge-m3) ä¹Ÿæ˜¯æ•ˆæœå¾ˆå¥½çš„æ¨¡å‹ã€‚

---

## Text Embedding Inference (TEI)
çœ‹åˆ°é€™é‚Šä½ å¯èƒ½æœƒå•ï¼šå¥½åƒéƒ½æ˜¯ Offline çš„ç”¨æ³•æ¬¸ï¼Œé€™äº› Embedding Model å¯ä»¥åš Online Serving å—ï¼Ÿæ²’å•é¡Œï¼Œé‚£å€‹é–‹æºç¬‘è‡‰å†æ¬¡å‡ºæ‰‹ï¼Œæ¨å‡ºäº†èˆ‡ TGI é¡ä¼¼çš„ [Text Embedding Inference](https://github.com/huggingface/text-embeddings-inference) æœå‹™ã€‚
* TEI ä¸€æ¨£èƒ½é€é Docker æ“ä½œï¼Œä½†æ˜¯ Image å€åˆ†çš„ç‰ˆæœ¬æ¯”è¼ƒå¤šä¸€é»ï¼Œæœ‰åˆ†æˆ CPUã€RTX 30 ç³»åˆ—æˆ– RTX 40 ç³»åˆ—ä¹‹é¡çš„ï¼Œä»¥ç­†è€…çš„ RTX 3090 ä¾†èªªè¦é¸æ“‡ Ampere 86 çš„ç‰ˆæœ¬ï¼š
    ```bash
    docker pull ghcr.io/huggingface/text-embeddinga-inference:86-1.2
    ```
* TEI çš„ç”¨æ³•èˆ‡TGIååˆ†ç›¸ä¼¼ï¼š
    ```bash
    docker run --gpus all -p 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-embeddings-inference:86-1.2 \
        --model-id BAAI/bge-m3
    ```
    * è£œå……ï¼š
        * é›–ç„¶åœ¨ `docker run` æ™‚ä¸è¦çµ¦ `--gpus` åƒæ•¸å°±èƒ½é”åˆ° CPU é‹ç®—çš„æ•ˆæœï¼Œä½†æ˜¯ç´” CPU ç‰ˆçš„TEI Docker Image æœƒæ¯”è¼ƒå°ï¼Œå› æ­¤è‹¥è¦é™ä½éƒ¨ç½²è¶³è·¡å¯ä»¥è€ƒæ…®æ”¹ç”¨ç´” CPU ç‰ˆã€‚
* é€éä»¥ä¸‹æŒ‡ä»¤é€²è¡Œæ¸¬è©¦ï¼š
    ```bash
    curl -X POST http://127.0.0.1:8080/embed \
        -H 'Content-Type: application/json' \
        -d '{"inputs":["è²“","ç‹—","cat", "dog"]}'

    # Output: [[0.0021725853,0.03031409,...]]
    ```
* é‚£éº¼ä½•æ™‚è©²ä½¿ç”¨ GPU ç‰ˆï¼Œä½•æ™‚è©²ä½¿ç”¨ CPU ç‰ˆå‘¢ï¼Ÿ
    * GPU çš„å„ªé»åœ¨æ–¼å¯ä»¥åŒæ™‚å¤§é‡é‹ç®—ï¼Œå› æ­¤åœ¨å°å¤§é‡æ–‡ä»¶å»ºç«‹ç´¢å¼•æ™‚ä½¿ç”¨ GPU æœƒæ¯”è¼ƒå¿«ã€‚
    * è€Œå¯¦éš›éƒ¨ç½²æ™‚ï¼Œå› ç‚ºä½¿ç”¨è€…çš„æŸ¥è©¢éƒ½æ˜¯ä¸€ç­†ä¸€ç­†é€²ä¾†çš„ï¼Œç›¸å°æ¯”è¼ƒåˆ†æ•£ï¼Œè€Œä¸”å‘é‡æ¨¡å‹é€šå¸¸ä¸åƒèªè¨€æ¨¡å‹æœ‰é‚£éº¼å¤šçš„åƒæ•¸é‡ï¼Œå› æ­¤è¨ˆç®—é‡ä¹Ÿä¸å¤§ï¼Œæ‰€ä»¥åªè¦ä½¿ç”¨ CPU ç‰ˆå³å¯ã€‚
* è£œå……ï¼š
    * ä»¥ä¸Šçš„æª¢ç´¢æ–¹æ³•é€šå¸¸æœƒè¢«å€åˆ†ç‚ºå…©å¤§é¡ï¼š
        1. **ç¨€ç–æª¢ç´¢(Sparse Retrieval)**
        2. **å¯†é›†æª¢ç´¢(Dense Retrieval)**
    * åƒæ˜¯ BM25 å°±å±¬æ–¼ç¨€ç–ç´¢ï¼Œè€Œæ–‡æœ¬å‘é‡å‰‡å±¬æ–¼å¯†é›†æª¢ç´¢ã€‚

### ç›¸ä¼¼åº¦
è¡¡é‡å‘é‡**ç›¸ä¼¼åº¦ (Similarity)** çš„æ–¹æ³•é™¤äº†å…§ç©ä»¥å¤–ï¼Œé‚„æœ‰**é¤˜å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)** ä»¥åŠ**æ­åŸºé‡Œå¾—è·é›¢ (Euclidean Distance)** ç­‰æ–¹æ³•ã€‚
* æ­åŸºé‡Œå¾—è·é›¢å…¶å¯¦å°±æ˜¯ä¿—ç¨±çš„åº§æ¨™ç›¸æ¸›å¹³æ–¹é–‹æ ¹è™Ÿï¼Œä¹Ÿå°±æ˜¯æ±‚å…©é»åº§æ¨™ä¹‹é–“çš„ç›´ç·šè·é›¢å…¬å¼ï¼Œå°æ‡‰åˆ°é«˜ç¶­åº¦çš„å‘é‡ä¾†èªªï¼Œå°±æ˜¯å°æ¯ä¸€å€‹ç¶­åº¦éƒ½è¨ˆç®—ç›¸æ¸›å¹³æ–¹é–‹æ ¹è™Ÿï¼Œç„¶å¾Œå†åŠ ç¸½èµ·ä¾†ã€‚
    * å¯ä»¥åœ¨ `scikit-learn` å¥—ä»¶è£¡é¢æ“ä½œæ­åŸºé‡Œå¾—è·é›¢ï¼š
        ```python
        from sklearn.metrics.pairwise import euclidean_distances

        dist = euclidean_distances(embeddings, embeddings)
        ```
* é¤˜å¼¦ç›¸ä¼¼åº¦æ˜¯åœ¨è¨ˆç®—å…©å€‹å‘é‡ä¹‹é–“çš„å¤¾è§’ï¼Œä¸¦å°é€™å€‹è§’åº¦å–é¤˜å¼¦ã€‚ç•¶å…©å€‹å‘æœ€è¶Šè²¼è¿‘æ™‚ï¼Œä»–å€‘çš„å¤¾è§’å°±æœƒè¶Šå°ï¼Œé¤˜å¼¦å€¼å°±æœƒè¶Šå¤§ï¼š
    ```
    cos(0Â°) = 1, cos(90Â°) = 0, cos(180Â°) = -1
    ```
    * åœ¨ `scikit-learn` è£¡é¢ä¹Ÿæœ‰é¤˜å¼¦ç›¸ä¼¼åº¦çš„å‡½æ•¸ï¼š
        ```python
        # åŸºæœ¬ä¸Šå°±æ˜¯ 1 - Cosine Similarity
        from sklearn.metrics.pairwise import cosine_distances
        from sklearn.metrics.pairwise import cosine_similarity

        dist = cosine_distances(embeddings, embeddings)
        sim = cosine_similarity(embeddings, embeddings)
        ```
* æ³¨æ„ï¼š
    * åˆ¥å¿˜äº†ç›¸ä¼¼åº¦(Similarity)æ˜¯è¶Šå¤§ä»£è¡¨è¶Šç›¸è¿‘ï¼Œè€Œè·é›¢(Distance)å‰‡æ˜¯è¶ŠçŸ­ä»£è¡¨è¶Šç›¸è¿‘ï¼Œåƒè¬åˆ¥æåå›‰ï¼

### Faiss
[**Facebook Al Similarity Search (Faiss)**](https://github.com/facebookresearch/faiss) æ˜¯ Facebook é–‹ç™¼çš„å‘é‡æœå°‹å¥—ä»¶ï¼Œå¯ä»¥åœ¨å¤§é‡é«˜ç¶­åº¦çš„å‘é‡è£¡é¢é€²è¡Œå¿«é€Ÿæœç´¢ï¼Œæ˜¯å€‹ç›¸ç•¶æœ‰æ•ˆç‡çš„å·¥å…·ã€‚
* å…ˆä¾†å®‰è£å¥—ä»¶ï¼š
```bash
pip install faiss-cpu
```
* Faiss ä¹Ÿæœ‰æ”¯æ´ GPU çš„ç‰ˆæœ¬ï¼Œéœ€è¦é€é Conda é€²è¡Œå®‰è£ï¼š
```bash
conda install -c conda-forge faiss-gpu
```
* Faiss çš„ç”¨æ³•ç›¸ç•¶ç°¡æ½”ï¼š
```python
import faiss
import numpy as np

# å®šç¾©å‘é‡é›†åˆ
dim = 3
value = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1]]
value = np.array(value)

# å»ºç«‹å‘é‡ç´¢å¼•
index = faiss.IndexFlatL2(dim)
index.add(value)

# é–‹å§‹æŸ¥è©¢
top_k = 3
query = np.array([[1, 0, 0]])
dist, indices = index.search(query, top_k)

# è¼¸å‡ºè·é›¢èˆ‡æœ€è¿‘çš„å‘é‡
print(dist)
for i in indices[0]:
    print(value[i])
```
* å¯ä»¥é€é `faiss.write_index` æŠŠå»ºç«‹å¥½çš„ç´¢å¼•å¯«æˆæª”æ¡ˆï¼Œå†ç”¨ `faiss.read_index` æŠŠç´¢å¼•è®€å–å‡ºä¾†ï¼š
```python
faiss.write_index(index, "index.faiss")
index = faiss.read_index("index.faiss")
print(type(index)) # <class 'faiss.swigfaiss_avx2.IndexFlat'>
```
* Faiss é™¤äº†èƒ½å¤ è™•ç†ä¸€èˆ¬çš„å‘é‡æœå°‹ä»¥å¤–ï¼Œé‚„æœ‰å¾ˆå¤šå…¶ä»–å°ˆé–€è™•ç†è¶…ç´šå¤§é‡ä¸”é«˜ç¶­åº¦å‘é‡çš„æ–¹æ³•ï¼Œå¯ä»¥åƒè€ƒ[å®˜æ–¹æ–‡ä»¶](https://github.com/facebookresearch/faiss/wiki)çš„èªªæ˜ã€‚
* åœ¨æ™®é€šçš„åŸºæœ¬æ‡‰ç”¨è£¡é¢ï¼Œä»¥ä¸Šçš„ç”¨æ³•é€šå¸¸å°±å·²ç¶“ç›¸ç•¶æœ‰æ•ˆç‡äº†ã€‚

---

## é€£çµ
* [Wikipedia: Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)
* [GitHub: Rank-BM25](https://github.com/dorianbrown/rank_bm25)
* [Wikipedia: TF-IDF](https://zh.wikipedia.org/wiki/Tf-idf)
* [Wikipedia: BM25](https://en.wikipedia.org/wiki/Okapi_BM25)
* [Sentence Transformers](https://www.sbert.net/)
* [Wikipedia: Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)
* [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
* [GitHub: Faiss](https://github.com/facebookresearch/faiss)
* [GitHub: Faiss Wiki](https://github.com/facebookresearch/faiss/wiki)

---

## çµè«–
æœ¬ç« ç¯€ä»‹ç´¹äº†ä¸€äº›æª¢ç´¢ç›¸é—œçš„çŸ¥è­˜èˆ‡å·¥å…·ï¼Œé€™äº›æŠ€è¡“çš„ç™¼å±•å·²ç¶“ç›¸ç•¶æˆç†Ÿï¼Œèˆ‡ç”Ÿæˆèƒ½åŠ›å¼·å¤§çš„èªè¨€æ¨¡å‹æ­é…å¾Œæ›´æ˜¯å¦‚è™æ·»ç¿¼ï¼Œå› æ­¤æª¢ç´¢æ¨¡å‹æ˜¯è¨±å¤šèªè¨€æ¨¡å‹æ‡‰ç”¨çš„é—œéµæŠ€è¡“ï¼Œä¸åƒ…éœ€è¦å¼·åŠ›çš„æª¢ç´¢æ¨¡å‹ï¼Œæ›´è¦æœ‰é©ç•¶çš„æª¢ç´¢æµç¨‹ï¼Œæ‰èƒ½å®Œæ•´ç™¼æ®ç³»çµ±çš„èƒ½åŠ›ï¼

---