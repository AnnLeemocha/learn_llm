# 再訪檢索模型
**資訊檢索 (Information Retrieval, IR)** 是在研究如何快速搜尋到使用者想要找的結果。
* 例如大家天天在用的 Google Search 就是最經典的一種資訊檢索查用，相較於生成式語言模型而言，是個已經發展許久且相對成熟的領域。
* 想要對大量文檔建立快速的檢索引擎，首先需要**建立索引 (Indexing)**，方法包含關鍵字提取、向量空間或 BM25 等等。接著根據使用者的查詢，尋找最相似的資訊並進行**排名 (Ranking)**，這整個搜尋的模式被稱為**檢索模型 (Retrieval Model)**。
* 將語言模型與資訊檢索結合，不僅能讓語言模型接觸到外部知識，改善語言模型無法更新資訊的問題，也能提昇回答的品質並減少錯誤與幻覺，若是進一步與情境學習結合，便能使模型進行更精準的任務。

---

## 關鍵字
**關鍵字 (Keyword)** 是個簡單有效卻經常被忽略的做法，雖然中文的關鍵字經常會遇到斷詞與邊界的問題，但是在簡單的實做上，能以字元為單位的方式來處理。
* 其中最常被用來實做關鍵字索引的方法就是 N-Gram 索引。
* N-Gram 的概念很單純，一次以一個字或兩個字為單位的方式來建立索引。
    1. 假設有兩份文本如下：
        ```
        文本 A：今天天氣真好
        文本 B：今天出門可好
        ```
    2. 若以 N-Gram 建立索引的話，會將文本切成以下片段：
        ```
        今、天、天、氣、真、好、今天、天天、天氣、氣真、真好、今天天、...
        今、天、出、門、可、好、今天、天出、出門、門可、可好、今天出、...
        ```
    3. 最後將這些索引與文本對應起來：
        ```
        今: A, B
        天: A, B
        氣: A
        門: B
        今天: A, B
        天天: A
        天出: B
        ```
    * 依此類推，雖然會切出很多不成詞彙的片段，但是在文本量較少的情況下，影響通常不大。
* 若是將列舉 N-Gram 的邏輯寫成 Python 會是這樣：
    ```python
    def ngram(text: str, n: int):
        for i in range(0, len(text) - n + 1):
            yield text[i : i + n]


    def all_ngram(text: str, a, b, step=1):
        a = max(1, a)
        b = min(len(text), b)
        for i in range(a, b + 1, step):
            for seg in ngram(text, i):
                yield seg
    ```
    * 使用起來會像這樣：
    ```python
    # 從 2-Gram 開始，每次遞增 2，最大到 6-Gram
    # 因此會有 2-Gram， 4-Gram， 6-Gram 的結果
    
    segs = [seg for seg in all_ngram("今天天氣真好", 2, 6, 2)]
    print(segs)
    # ['今天', '天天', '天氣', '氣真', '真好', '今天天氣', '天天氣真', '天氣真好', '今天天氣真好']
    ```
    * 使用者輸入的查詢可能會得出很多片段，這時可以使用 [**Jaccard Similarity**](https://en.wikipedia.org/wiki/Jaccard_index) 做排序，也就是將彼此所有 N-Gram 的交集除以聯集：
    ```python
    def calc_jaccard(query: str, chunk: str):
        query_length = len(query)
        query_ngrams = {s for s in all_ngram(query, 1, query_length)}
        chunk_ngrams = {s for s in all_ngram(chunk, 1, query_length)}

        inter = query_ngrams & chunk_ngrams
        union = query_ngrams | chunk_ngrams

        return len(inter) / len(union)
    ```
    透過這些技巧就能建立出完整的N-Gram 搜尋引擎囉！
* 在應付少量文本時，使用N-Gram 建立索引通常不會有太大的問題。但是當文本量逐漸上升時，索引本身所佔用的記憶體會爆炸性成長。加上這種索引只能做文字比對，並沒有辦法進行相近語意的比較，斷詞邊界也是個大問題，因此不是最理想的做法。
* 📝 範例程式碼
    * [筆者程式碼](https://tinyurl.com/llm-note-12)
    * 自己嘗試: .../LLM/project/search

---

## BM25 Best Matching Ranking
**[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) (Best Matching 25)** 是一種基於 [TF-IDF](https://zh.wikipedia.org/wiki/Tf-idf) 的統計排名方法，雖然是個古老的演算法，但還是相當簡單有效。
* 可以透過 [Rank-BM25](https://github.com/dorianbrown/rank_bm25) 這個 Python 套件輕鬆使用這個經典演算法：
    ```python
    from rank_bm25 import BM25Okapi

    corpus = ["這是第一篇文檔", "這是第二篇文檔", "文檔的內容很重要"]

    #  以字元為單位斷詞
    corpus_chars = [[ch for ch in doc] for doc in corpus]
    bm25 = BM25Okapi(corpus_chars)

    query = "重要"
    query_chars = [ch for ch in query]

    # 計算 BM25 分數
    scores = bm25.get_scores(query_chars)
    print(f"Scores: {scores}")

    # 輸出查詢結果
    best = scores.argmax()
    print(f"Query: {query}")
    print(f"Result: {corpus[best]}")

    """
    輸出:
    Scores: [0. 0. 0.98149902]
    User Query: 重要
    Best Result: 文檔的內容很重要
    """
    ```
* BM25 是一個排名演算法，可以與其他檢索方法做結合，例如先用 N-Gram 或 Embedding 進行比較大範圍的模糊搜尋，再用 BM25 做排序之類的。

---

## 文本向量 Text Embedding
先前的章節已經介紹過 Embedding 是一種**將文字向量化**的結果，若 Embeddling 以字詞為單位，則稱為**詞向量 (Word Embedding)**，若以句子為單位則稱為**文句向量 (Sentence Embedding)**，也有以文件為單位的**文件向量 (Document Embedding)**，因此 Embedding 一詞的用法是很彈性的。
* 補充：
    * 單位較大的向量，例如一句話或一份文件，通常是由多個單位較小的向量合併計算出來的。
        * 例如 Transformer Encoder 會將一句話切成數個 Tokens 後得到 Token Embedding，經過數層 Transformer Blocks 的精密計算，加上 Pooling 運算得到最終的 Sentence Embedding。
* 一般來說，在檢索模型裡面使用的都是以句子或文件為單位，這樣就可以將不同長度的句子都編碼成固定維度的向量，透過一個簡單的矩陣運算便能快速計算相似度。
* 專門用來產生這種向量的模型有很多，但是有支援中文或多語的選擇相對較少。
1. 一個古老但是經典的選擇是 Google 的 [Universal Sentence Encoder](https://www.kaggle.com/models/google/universal-sentence-encoder/) 系列，其中的 [Multilingual Large](https://tinyurl.com/llm-useml) 是筆者較推薦的版本。
    * 首先要安裝套件：
        ```bash
        pip install tensorflow tensorflow_hub tensorflow_text
        ```
    * 基本使用方法如下：
        ```python
        import numpy as np
        import tensorflow as tf
        import tensorflow_hub as hub
        import tensorflow_text

        # # 不讓 Tensorflow 使用 GPU
        # tf.config.set_visible_devices([], "GPU")

        # 下載 & 讀取模型
        hub_url = (
            "https://www.kaggle.com/models/google/",
            "universal-sentence-encoder/tensorFlow2/",
            "multilingual-large/2"
        )

        # 也可以把模型下載下來後，直接指定本地路徑續取
        model = hub.load(hub_url)

        # 計算各自的 Embedding
        # 「고양이」跟「개」分別是韓文的貓和狗
        text = ["cat", "貓", "고양이", "dog", "狗", "개"]
        embeddings = model(text)

        # 計算內積作為相似度
        similarity = np.inner(embeddings, embeddings)
        np.set_printoptions(precision=2)
        print(similarity)
        ```
        * 輸出結果如下：
        ```
        [
            [1.      0.94    0.92    0.75    0.76    0.73]
            [0.94    1.      0.92    0.73    0.75    0.73]
            [0.92    0.92    1.      0.61    0.63    0.62]
            [0.75    0.73    0.61    1.      0.98    0.98]
            [0.96    0.75    0.63    0.98    1.      0.97]
            [0.73    0.73    0.62    0.98    0.97    1.  ]
        ]
        ```
        可以看到貓，고양이 跟 cat 之間的內積較大，代表他們在語意上較為相近，但他們跟「狗」的內積就比較小，代表他們的語意比較不相近。
    * 注意：
        * 如果在 Tensorflow 與 PyTorch 混用的專案裡面，先讀取 Tensorflow 模型可能會造成兩個套件搶用 GPU 的問題，解決方法之一是先匯入 PyTorch 相關的套件，或者透過`tf.config.set_visible_devices([], "GPU")` 來停用 Tensorflow 使用 GPU 進行運算，因為這個模型很小，所以在沒有 GPU 的情況下也可以算的很快。
        * 另外要注意 `tensorflow_text` 雖然看起來沒有用到，但是如果沒有先匯入的話會發生 `RuntimeError` 喔！
2. 第二個選擇是使用跟 Hugging Face 生態系較貼近的 [**Sentence Transformers**](https://www.sbert.net/) 模型，在 HF Hub 上可以找到很多[**相關的模型**](https://huggingface.co/sentence-transformers)。
    * 首先要安裝套件：
        ```bash
        pip install sentence_transformers
        ```
    * 基本用法如下：
        ```python
        import numpy as np
        from sentence_transformers import SentenceTransformer

        # 下載 & 讀取模型
        model = SentenceTransformer("intfloat/multilingual-e5-base")

        # 取得 Embedding
        text = ["cat", "貓", "고양이", "dog", "狗", "개"]
        embeddings = model.encode(text, normalize_embeddings=True)

        # 計算內積作為相似度
        similarity = np.inner(embeddings, embeddings)
        np.set_printoptions(precision=2)
        print(similarity)
        ```
    * 除了以上範例使用的 E5 以外，還有 [BGE-M3](https://huggingface.co/BAAI/bge-m3) 也是效果很好的模型。

---

## Text Embedding Inference (TEI)
看到這邊你可能會問：好像都是 Offline 的用法欸，這些 Embedding Model 可以做 Online Serving 嗎？沒問題，那個開源笑臉再次出手，推出了與 TGI 類似的 [Text Embedding Inference](https://github.com/huggingface/text-embeddings-inference) 服務。
* TEI 一樣能透過 Docker 操作，但是 Image 區分的版本比較多一點，有分成 CPU、RTX 30 系列或 RTX 40 系列之類的，以筆者的 RTX 3090 來說要選擇 Ampere 86 的版本：
    ```bash
    docker pull ghcr.io/huggingface/text-embeddinga-inference:86-1.2
    ```
* TEI 的用法與TGI十分相似：
    ```bash
    docker run --gpus all -p 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-embeddings-inference:86-1.2 \
        --model-id BAAI/bge-m3
    ```
    * 補充：
        * 雖然在 `docker run` 時不要給 `--gpus` 參數就能達到 CPU 運算的效果，但是純 CPU 版的TEI Docker Image 會比較小，因此若要降低部署足跡可以考慮改用純 CPU 版。
* 透過以下指令進行測試：
    ```bash
    curl -X POST http://127.0.0.1:8080/embed \
        -H 'Content-Type: application/json' \
        -d '{"inputs":["貓","狗","cat", "dog"]}'

    # Output: [[0.0021725853,0.03031409,...]]
    ```
* 那麼何時該使用 GPU 版，何時該使用 CPU 版呢？
    * GPU 的優點在於可以同時大量運算，因此在對大量文件建立索引時使用 GPU 會比較快。
    * 而實際部署時，因為使用者的查詢都是一筆一筆進來的，相對比較分散，而且向量模型通常不像語言模型有那麼多的參數量，因此計算量也不大，所以只要使用 CPU 版即可。
* 補充：
    * 以上的檢索方法通常會被區分為兩大類：
        1. **稀疏檢索(Sparse Retrieval)**
        2. **密集檢索(Dense Retrieval)**
    * 像是 BM25 就屬於稀疏索，而文本向量則屬於密集檢索。

### 相似度
衡量向量**相似度 (Similarity)** 的方法除了內積以外，還有**餘弦相似度 (Cosine Similarity)** 以及**歐基里得距離 (Euclidean Distance)** 等方法。
* 歐基里得距離其實就是俗稱的座標相減平方開根號，也就是求兩點座標之間的直線距離公式，對應到高維度的向量來說，就是對每一個維度都計算相減平方開根號，然後再加總起來。
    * 可以在 `scikit-learn` 套件裡面操作歐基里得距離：
        ```python
        from sklearn.metrics.pairwise import euclidean_distances

        dist = euclidean_distances(embeddings, embeddings)
        ```
* 餘弦相似度是在計算兩個向量之間的夾角，並對這個角度取餘弦。當兩個向最越貼近時，他們的夾角就會越小，餘弦值就會越大：
    ```
    cos(0°) = 1, cos(90°) = 0, cos(180°) = -1
    ```
    * 在 `scikit-learn` 裡面也有餘弦相似度的函數：
        ```python
        # 基本上就是 1 - Cosine Similarity
        from sklearn.metrics.pairwise import cosine_distances
        from sklearn.metrics.pairwise import cosine_similarity

        dist = cosine_distances(embeddings, embeddings)
        sim = cosine_similarity(embeddings, embeddings)
        ```
* 注意：
    * 別忘了相似度(Similarity)是越大代表越相近，而距離(Distance)則是越短代表越相近，千萬別搞反囉！

### Faiss
[**Facebook Al Similarity Search (Faiss)**](https://github.com/facebookresearch/faiss) 是 Facebook 開發的向量搜尋套件，可以在大量高維度的向量裡面進行快速搜索，是個相當有效率的工具。
* 先來安裝套件：
```bash
pip install faiss-cpu
```
* Faiss 也有支援 GPU 的版本，需要透過 Conda 進行安裝：
```bash
conda install -c conda-forge faiss-gpu
```
* Faiss 的用法相當簡潔：
```python
import faiss
import numpy as np

# 定義向量集合
dim = 3
value = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1]]
value = np.array(value)

# 建立向量索引
index = faiss.IndexFlatL2(dim)
index.add(value)

# 開始查詢
top_k = 3
query = np.array([[1, 0, 0]])
dist, indices = index.search(query, top_k)

# 輸出距離與最近的向量
print(dist)
for i in indices[0]:
    print(value[i])
```
* 可以透過 `faiss.write_index` 把建立好的索引寫成檔案，再用 `faiss.read_index` 把索引讀取出來：
```python
faiss.write_index(index, "index.faiss")
index = faiss.read_index("index.faiss")
print(type(index)) # <class 'faiss.swigfaiss_avx2.IndexFlat'>
```
* Faiss 除了能夠處理一般的向量搜尋以外，還有很多其他專門處理超級大量且高維度向量的方法，可以參考[官方文件](https://github.com/facebookresearch/faiss/wiki)的說明。
* 在普通的基本應用裡面，以上的用法通常就已經相當有效率了。

---

## 連結
* [Wikipedia: Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)
* [GitHub: Rank-BM25](https://github.com/dorianbrown/rank_bm25)
* [Wikipedia: TF-IDF](https://zh.wikipedia.org/wiki/Tf-idf)
* [Wikipedia: BM25](https://en.wikipedia.org/wiki/Okapi_BM25)
* [Sentence Transformers](https://www.sbert.net/)
* [Wikipedia: Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)
* [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
* [GitHub: Faiss](https://github.com/facebookresearch/faiss)
* [GitHub: Faiss Wiki](https://github.com/facebookresearch/faiss/wiki)

---

## 結論
本章節介紹了一些檢索相關的知識與工具，這些技術的發展已經相當成熟，與生成能力強大的語言模型搭配後更是如虎添翼，因此檢索模型是許多語言模型應用的關鍵技術，不僅需要強力的檢索模型，更要有適當的檢索流程，才能完整發揮系統的能力！

---