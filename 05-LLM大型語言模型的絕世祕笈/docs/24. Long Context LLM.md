# Long Context LLM
**上下文長度 (Context Length / Context Window)** 指的是一個模型的輸入加上輸出最多可以處理到多少個 Tokens。
* 例如一開始的 LLaMA 只支援到 2048 個 Tokens，後來的 Llama 2 則支援到 4096 個 Tokens，模型的可用性就大幅提昇了，到現在 Llama 3 已經支援到 8192 Tokens，雖然已經滿大了，但面對某些長文本的應用還是略顯不足。
* 模型的上下文長度對中文應用尤其重要，因為中文所佔用的 Token 數量通常都比英文來得多，若是需要檢索的應用，提示裡面加上檢索出來的文件，消耗的 Token 數量是相當可觀的。
* 那語言模型到底如何拓寬上下文長度呢？

---

## ChatGPT & Claude
* 目前 OpenAI 的 GPT-3.5 可以支援到 16K，而 GPT-4 則支援到 128K，其實對一般應用來說 16K 已經相當足夠，況且 GPT-4 還比 GPT-3.5 貴個 10 到 20 倍左右。
* 如果會特地用到長上下文模型的應用，消耗的 Token 數量鐵定也是特別高！長期用下來其實成本是滿高的，現階段要創造出與成本匹配的收益，是不太容易的。
* 來自 Anthropic 的 Claude 最高可以支援到 200K，是個相當驚人的長度，筆者實際輸入幾篇非常長的文章，其效果是真的挺不錯的，而且處理速度也很快。
    * Claude 一開始最令筆者不滿的地方在於模型的回覆，會把全形與半形標點符號混用，令人強迫症大爆發。
    * 但是 Claude 最近的更新開始逐漸改善這個問題了，非常期待 Claude 不再全形半形傻傻分不清的那天到來！
* 雖然這些模型支援的長度都很長，但可惜的是這些模型也都沒有發表他們實際的訓練手法，也有可能他們純粹就是拉長訓練資料的序列長度，反正他們的機器也是夠多夠硬。
* 但這種做法對一般個人或中小型研究單位而言不太實際，於是大家開始思考有沒有更低成本又能拉長長度的方法。

---

## Gradient Checkpointing
* 在訓練長序列模型時，最大的問題就是梯度，當訓練資料的長度越長，反向傳播時花在梯度上的 GPU 記憶體用量就會越多。
* 幸好 2016 年由 Tianqi Chen 等人提出了**梯度檢查點 (Gradient Checkpointing)** 的概念，可以減少梯度所消耗的記憶體用量，雖然會提昇計算量，但能夠克服長序列訓練的瓶頸，推薦可以看看[這篇文章](https://github.com/cybertronai/gradient-checkpointing)的介紹。
* 一般訓練流程的梯度方向：
    ![一般梯度檢查點](images/一般梯度檢查點.png)
    * 輸入 A 會依序拜訪模型的每一層，並各自計算出一個輸出狀態。
    * 每一層的結果都算完之後，可以得到一份梯度，並根據這份梯度往回做反向傳播運算，從尾到頭傳播到模型的每一層後，完成一次模型權重的更新。
    * 以上圖為例，算到最後一個節點時，需要把八個節點的狀態全部放在記憶體裡面，因此是最消耗記憶體的做法，但也是速度最快的方法。
* 另外一種做法是把輸出狀態全部捨棄，在進行反向傳播時，每回頭計算一個節點，就重新計算一次前面所有節點的輸出。
* 低記憶體反向傳播：
    ![梯度檢查點 - 低記憶體反向傳播](images/梯度檢查點%20-%20低記憶體反向傳播.png)
    * 每次反向傳播，只會記住一兩個節點而已，相比於原始的反向傳播，減少了相當多的記憶體用量。
    * 但缺點非常顯而易見，不僅需要反覆的配置與釋放記憶體，整體計算量也大幅提昇。
* 在梯度檢查點機制裡面，選擇了一個折衷的方案：
    * 沒有全部記下來，也沒有全部忘掉，只保留幾個節點的輸出。
* 梯度檢查點：
    ![梯度檢查點](images/梯度檢查點.png)
    * 挑選幾個中間的節點當作檢查點，把這幾個檢查點的狀態記下來放在記憶體裡面，遇到沒有存狀態的節點，就從最近的檢查點開始計算。
    * 這樣雖然會提高一點記憶體用量，但是計算量比第二種做法低的多。
* 那實際上該如何使用梯度檢查點來進行模型訓練呢？
* 首先在讀取模型時，需要幫模型設定 `use_cache=False`：
    ```python
    from transformers import LlamaForCausalLM as ModelCls

    model = ModelCls.from_pretrained(..., use_cache=False)
    model.config.une_cache = False # 或者這樣設定
    ```
* 接著開啟 `enable_input_require_grads` 與 `gradient_checkpointing_enable` 這兩個設定：
    ```python
    model.enable_input_require_grads()
    model.gradient_checkpointing_enable()
    ```
* 最後在訓練參數裡面設定 `gradient_checkpointing=True`：
    ```python
    from transformers import TrainingArguments

    TraspirngArguments(..., gradient_checkpointing=True)
    ```
* 筆者實測使用長度 512 的資料對 Llama 2 13B 8-Bit 做訓練，權重本身約佔 13 GB，在沒有開啟梯度檢查點的情況下，總共需要消耗 19GB 的 GPU 記憶體，但如果有開梯度檢查點的話，總記憶體消耗降低到 14GB 而已。
* 記憶體用量大幅減少後，便能用長度更長的資料進行訓練，或者加大批次大小來提昇訓練效果。
* 然而原本幾秒鐘就能完成的訓練，開啟梯度檢查點後需要兩分多鐘，可見計算量提昇了相當多。
* 因此梯度檢查點就是個以時間換取記憶體的訓練方法，但是這個方法可以克服記憶體瓶頸，大幅拉長可訓練的長度，增加模型的可用性。

---

## 內插法 & 外插法
在 Llama 架構裡，使用**旋轉相對位置編碼 (Rotary Positional Embedding, RoPE)** 來表達位置資訊，RoPE 主要使用餘弦 (Cosine) 函數來表達相對位置的資訊，因此可以用餘弦線圖來當作 RoPE 的示意圖。
* 如果要將 RoPE 的長度拉大，最簡單的做法就是直接**外插 (Extrapolation)** 上去，把餘弦函數接續下去。
* 外插示意圖：
    ![外插示意圖](images/外插示意圖.png)
    * 但是實際使用時發現外插效果並不如想像中的好，於是有人改用**內插 (Interpolation)** 的方式，把餘弦函數給**拉寬**到指定長度。
* 內插示意圖：
    ![內插示意圖](images/內插示意圖.png)
    * 因此新的餘弦波週期還是一樣的，但是可以覆蓋的範圍變廣了。
    * 就像是幫相機裝上廣角鏡頭一樣，可以看到的視野變寬廣了，但是拍出來的照片解析度還是一樣的，因此在細節上會看的相對模糊，不過可以看到的風景樣貌整體上變多了。
* 調整基礎頻率示意圖：
    ![調整基礎頻率示意圖](images/調整基礎頻率示意圖.png)
    * 還有一種做法是透過**調整基礎頻率 (Adjusted Base Frequency, ABF)** 的做法來改造 RoPE，原本 2K 的長度裡面餘弦波會有四個週期，但是透過 ABF 把頻率提高，在 2K 的長度內塞進八個週期之類的。
    * 因此 RoPE 的粒度更細，可以學到相對細節的資訊，使模型更有辦法分辨不同位置之間的相對資訊。
* 那實際上，我們要如何對RoPE做這些調整呢？
* 在 HF Transformers 套件裡面，可以透過 `rope_theta` 與 `rope_scaling` 兩個參數來調整：
    ```python
    from transformers import LlamaForCausalLM

    LlamaForCausalLM.from_pretrained(
        "meta-llama/Mata-Llama-3-8B-Instruct",
        rope_theta=5e3,
        rope_scaling("type": "linear", "factor": 2),
    )
    ```
    * 其中 `rope_theta` 就是用來調整基礎頻率的參數，因為會取倒數，所以 `rope_theta` 設定的越小，產生的頻率就會越高。
    * 而 `rope_scaling` 則是對位置做縮放，以 `factor=2` 為例，原本在 1024 的旋轉向量，會被放到第 2048 個位置，也就是內插法。
* 可以使用之前量化章節寫過的困惑度評測程式，來觀察這些位置編碼的縮放對困惑度的影響。
    * 例如 TinyLlama 只有訓練 2K 的上下文長度，因此使用 2K 長度的序列測困惑度得到的結果為 8.0234，但是測 4K 序列長度，困惑度居然上升到 139，若是測到 8K 的話更是暴漲到 1291！
    * 這時試著將 Scaling Factor 設定為 2 並且重新測試，4K 的困惑度就被壓到 20 左右了！
    * 若是把 Scaling Factor 設定為 4 並測試 8K，則困惑度可以壓到 118 左右。
        * 雖然說困惑度有所下降，但其實這個困惑度也根本沒辦法用，生成出來的東西依然亂七八糟。
        * 但是因為有做擴展設定的模型困惑度比較低，所以 Loss 相對也會比較低，因此若是要用長序列的訓練資料做微調，可以讓訓練更快收斂。
* 但是很顯然的，只調整 RoPE 設定的話，並沒有辦法真正擴展模型的上下文長度。

---

## LongLoRA
[**LongLoRA**](https://github.com/dvlab-research/LongLoRA) 是一種結合 LoRA 來擴展模型長度的方法，他們提出 **Shift Short Attention (S²-Attn)** 的方法來減少注意力機制的計算量，並結合 LoRA 減少記憶體消耗量，使我們能夠用更少的硬體成本來訓練更長的模型。
* 原作者實驗發現，如果單純只把模型長度提高而不去做額外的微調，那模型的效果就會很差，但是直接用長序列的資料進行訓練，需要消耗的記憶體又太高了！
* 因此作者提出 S²-Attn 與 LoRA+ 結合的 LongLoRA 訓練方式，讓我們可以用更少的記憶體來訓練更長的模型。
* 這個 S²-Attn 的做法大致如下：
    1. 將 Attention Heads 切成兩半。
    2. 將其中一半的 Attention 位移 (Shift) 一點點。
    3. 將所有 Attention 切成好幾個群組 (Group)。
    4. 進行 Group-Wise Self-Attention 運算。
* S²-Attn 示意圖：
    ![S²-Attn 示意圖](images/S²-Attn%20示意圖.png)
    * 位移的目的是要讓群組之間的資訊可以交流，能夠提昇模型訓練的效果。
    * 而切成群組做 Group-Wise Self-Attention 則可以降低注意力層的計算量。
* 作者強調 S²-Attn 不僅能減少訓練階段的計算量，訓練出來的模型在推論階段也能夠直接使用原本的 Full Attention 進行運算，因此這個方法並不需要修改模型架構。
* 最後 LoRA+ 其實就是把詞向量與正規化層的權重打開來訓練：
    ```python
    model = ModelCls.from_pretrained(...)
    model = get_peft_model(model, ...)

    for name, param in model.named_parameters():
        if "embed" in name or "norm" in name:
            param.requires_grad_()
            break
    ```
    * 其中 `embed` 與 `norm` 在不同模型會有不同名稱，需要根據實際情況做調整，這裡是以 Llama 模型的名稱為例。
    * 筆者使用 Llama 3 8B 模型測量了一下，如果是使用原版 LoRA 的話，Adapter 的參數量約為 3.4M，但是改用 LORA+ 的話，Adapter 的參數量會提升到 528M。
    * 雖然說參數量大了不少，但這也僅是整個模型 6.5% 的參數量而已，所以需要訓練的參數量依然維持的很少。

---

## StreamingLLM & Attention Sinks
* 過去有些開發者可能會採用 Window Attention 的做法來減少長序列輸入的運算量，也就是只計算過去一段固定長度的序列，超過的就通通丟掉不算。
    * 但是這樣的做法被證實是不可行的，模型會開始亂輸出。
* 所以又有另外一種做法是結合 Sliding Window 的機制，將整個 Window 滑動一段距離後，重新計算一次所有注意力分數。
    * 這樣的做法雖然不會亂吐輸出，但是遇到需要重新計算的地方時，使用者會明顯感受到很長的停頓。
* [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) 的作者在探討 Window Attention 為何會失敗的過程中，觀察到一個相當有趣的現象：
    * 在語言模型計算注意力分數的時候，輸入模型的**前幾個 Token** 平均都能獲得相當高的注意力分數。
    * 即便把前幾個 Token 換成換行符號這類不具備任何實質語意的 Tokens，依然會獲得相對高的分數。
    * 作者認為 Window Attention 會失敗，就是因為這些分數高的 Tokens 被拋棄了，之所以會有這個現象，作者提出了兩個猜測：
        1. 即便下個被預測的 Token **並不需要關注任何上下文**，但是因為注意力層裡面的 Softmax 運算會使注意力分數**無論何時都不會有 Token 是零分**，這個時候前幾個 Tokens 就會來幫忙吸收掉這些多餘的分數。
        2. 在自回歸生成的過程中，**後續所有的 Token 都能看到前幾個 Tokens**，因此所有人都認識他們，導致前幾個 Tokens 能夠捕捉一些不重要的注意力。
    * 作者將這前幾個 Tokens 稱為 **Attention Sinks**。
        * Sink 指的是沉陷、沒入的意思。
        * 這些 Tokens 相當重要，就像是船錨一樣可以把整艘船穩住而不會亂飄。
* 基於這個觀察，作者提出了 StreamingLLM 的做法，一樣是以 Window Attention 為基礎，但是在截斷注意力時把前幾個 Tokens 的注意力給保留下來。
    * 這樣的做法非常驚人的有效果，不僅能穩住輸出不會亂生成，在推論時也相當有效率，計算量不會隨著長度而增加。
    * 因此作者建議以後大家訓練語言模型的時候，一定要多放幾個 Attention Sinks，這樣 StreamingLLM 就可以利用 Attention Sinks 來讓語言模型的處理長度變得**無限長**。
    * 這樣的發現，再次重申了 BOS Token 的重要性。
    * StreamingLLM 並不是真的把上下文長度變長，而是一種能讓模型穩定處理長上下文的方法。
* 可以透過 HF Transformers 套件的 `SinkCache` 類別來使用 Attention Sink 機制進行生成：
    ```python
    from transformers import AutoModelForCausalLM as ModelCls
    from transformers import AutoTokenizer as TkCls 
    from transformers import TextStreamer, SinkCache

    m = ModelCls.from_pretrained(...)
    tk = TkCls.from pretrained(...)
    ts = TextStreamer(tk)

    input_ids = tk(..., return_tensors="pt").to("cuda")
    cache = SinkCache(window_length=32, num_sink_tokens=4)
    m.generate(input_ids, past_key_values=cache, streamer=ts)
    ```
* 可以自己調整看看 `window_length` 與 `num_sink_tokens` 感受一下生成的內容有什麼不同，這兩個數值調的越高，輸出通常越穩定，但能夠節省的記憶體就會少一點。

---

## 連結
* [HF TGI Docs: RoPE Scaling](https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model#rope-scaling)
* [arXiv Paper - Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882)
* [arXiv Paper - Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595)
* [arXiv Paper - Effective Long-Context Scaling of Foundation Models](https://arxiv.org/abs/2309.16039)
* [arXiv Paper - LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
* [arXiv Paper - Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
* [arXiv Paper - Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)
* [iThome News: Llama Long](https://www.ithome.com.tw/news/159082)

---

## 結論
* 本章節介紹了一些與擴展語言模型能處理的上下文長度有關的做法，每個方法都有各自的優點與效果。
* 但是根據筆者的經驗，不管使用哪種方法，只要輸入的文本變長之後，模型產生的回應細緻度就會開始下降，無論是像 Llama 這種小的開源模型，還是像 ChatGPT 或 Claude 這種大的商用模型，都能觀察到類似的現象。
* 因此面對長文本的任務，筆者還是建議盡可能去做切割，把問題的範圍縮小，或者透過檢索的方法去解決，會是比較兼具效率與準確率的做法。

---