# 語言模型們
雖然多數的 Local LLM 不會像 ChatGPT 一樣高達 175B 的參數量，但即便模型只有 7B, 13B，在只有一兩張 3090, 4090 顯卡的規格下，想要自己從頭開始做預訓練也是非常困難。這時候選擇擁抱開源的語言模型，就是個相當經濟實惠的方案。

---

## 英文模型

### Llama
[LLaMA](https://arxiv.org/abs/2302.13971) 是 2023 年 2 月底同樣由 Meta AI 發表的模型，模型大小包含 7B, 13B, 30B, 65B 等四種規格。
* Llama 是大羊駝的西班牙語，也就是俗稱的草泥馬。
* 第一代模型所使用的訓練資料集全部都是公開資料集，例如網路爬蟲C4、維基百科、古騰堡計劃 Gutenberg 電子書、arXiv 論文、 GitHub 程式碼以及 StackOverflow 家族 StackExchange 系列網站的問答等等，總 Tokens 數約1.4T的文本量。有人將這些資料集整理成 RedPajama 資料集，用掉好幾個TB的硬碟空間。
    * 根據 LLaMA 的論文所說，他們拿 2048 張 A100 80GB 訓練 LLaMA 65B 花了 21 天才訓練完。
    * 原本是沒有開源的，需要填表申請才能取得，但是被大家偷偷上傳到Hugging Face Hub上，加上模型的效果是滿不錯的，因此引起了一波不小的LLAMA 熱潮，著名的 [llama.cpp](https://github.com/ggerganov/llama.cpp) 專案就是在此時誕生的。
    * 一代的模型，都會用 "LLaMA" 當作名稱代號。
* 2023 年 7 月的時候 [Llama2](https://arxiv.org/abs/2307.09288) 誕生了。Llama2 將訓練資料量擴大到 2T Tokens 的文本量，並將原本 2K 的模型長度擴展到了 4K，實用程度大幅上升。
    * 不同於一代只有釋出預訓練模型，還做了指令微調與RLHF訓練。
    * Llama 2正式在 Hugging Face 上[開源模型權重](https://huggingface.co/blog/llama2)，無論是預訓練版的 Llama 2 還是 Chat 版的 Llama2 都可以在 [HF Hub](https://huggingface.co/meta-llama) 上找到。
    * 中文能力其實並不好，因此若要應用在中文情境的話， 需要先有一定程度的微調進行中文化才行。
    * 二代的模型，都會用 "Llama 2" 當作名稱代號。
*  Llama 3 在 2024 年 4 月問世，雖然只有釋出 8B 與 70B 兩種規格，但是訓練資料量大幅上升到 15T Tokens，模型能力有了相當程度的提昇，使用中文的能力也改善許多。
* 參考連結：
    * [Official Website](https://ai.meta.com/llama/)
    * [GitHub](https://github.com/facebookresearch/llama)
    * [HF Models](https://huggingface.co/meta-llama)
    * [Llama 2 70B Demo](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI)
    * [LLaMA Paper](https://arxiv.org/abs/2302.13971)
    * [Llama 2 Paper](https://arxiv.org/abs/2307.09288)

### Alpaca
[Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 是 2023 年 3 月由 Stanford 大學公開的一套訓練方法與模型，這個模型是從 LLaMA 模型微調訓練而來。
* Alpaca 是羊駝的西班牙語，與大羊駝外觀相似，也算是一種草泥馬。
* 特別之處在於他的訓練資料是使用GPT-3 生成的，作者首先蒐集了一些**種子任務** (Seed Task) 並請 GPT-3 產生更多類似的範例：
    ```
    [USER]:
    請把“Starburst Stream"翻譯成中文星爆氣流斬
    請按照以上格式，產生更多類似的範例。

    [MODEL]:
    請把“Fire Dragon Roar" 翻譯成中文火龍咆哮
    請把"Thunderbolt Strike" 翻譯成中文雷電一擊
    ...
    ```
* 用這些產生出來的資料對LLaMA做指令微調，藉此萃取閉源大模型的知識，這種做法也被稱為[**知識蒸餾** (Knowledge Distillation)](https://en.wikipedia.org/wiki/Knowledge_distillation)。
* 這個模型發布之時，還沒有ChatGPT API可以使用，因此原作者是使用 GPT-3 來生成五萬多筆資料，花費將近600美元。
    * 有了GPT-3.5之後，這個成本可以減少 10到20倍左右，而且生成的資料品質還會比GPT-3的資料更好。
* 參考連結：
    * [Blog Post](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    * [GitHub](https://github.com/tatsu-lab/stanford_alpaca)
    * [HF Models](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff)

### Vicuna
[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) 是來自 UC Berkeley 柏克萊加州大學與數個學術研究單位聯手合作發表的模型，同樣是從 LLaMA 系列微調訓練而來。
* Vicuna 是小羊駝的西班牙語，與羊駝外觀比較不像。
* 特色在於它是使用 Vicuna 是來自 UC Berkeley 柏克萊加州大學以及數個學術研究單位聯手合作 ShareGPT 資料集進行訓練的。
* [ShareGPT](https://sharegpt.com/) 其實原本是一個瀏覽器外掛，在ChatGPT剛開放時，因為還沒有分享對話的功能，因此有些人會使用這個外掛來分享對話內容。
    * 一開始 ShareGPT 可以透過爬蟲程式爬取許多對話資料下來，而 Vicuna 的 ShareGPT 訓練資料集就是在那個時候蒐集而來的。
    * 但現在 ShareGPT 已經關閉爬蟲入口了，因此如果現在想要使用 ShareGPT 做訓練，可以參考 HF Hub上別人整理的 [ShareGPT 資料集](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)。
* 因為 Vicuna 使用的 ShareGPT 訓練集是**真實的對話資料**，所以相比於使用模型生成的 Alpaca 資料集，ShareGPT 的**資料多樣性**(Diversity)高很多。
    * 筆者的實測經驗來看，雖然 ShareGPT 裡面中文的資料比例不算很高，但是 Vicuna 的中文問答、摘要與翻譯能力都相當優異。
* 在Llama 2推出後，Vicuna 也跟著推出了 Llama 2 版本的 Vicuna 模型，效果同樣也相當好，可以到他們的 [Demo 網頁](https://chat.lmsys.org/)上試試看。
    * 這個 Demo 網頁不只有 Vicuna 模型可以試用，還有許多大大小小的模型供大家免費試玩。
* 參考連結：
    * [Blog Post](https://lmsys.org/blog/2023-03-30-vicuna/)
    * [GitHub](https://github.com/lm-sys/FastChat)
    * [Demo](https://chat.lmsys.org/)
    * [HF Models](https://huggingface.co/models?search=lmsys%20vicuna)

### TinyLlama
TinyLlama 是個很特別的專案，專案作者試圖訓練一個1.1B 參數量的 Llama 模型，並使用 3T Tokens 的訓練資料集，預計使用16張 A100 40 GB 進行訓練，約三個月可以完成。
* 迷你羊駝(算是一種小羊駝嗎?)
* 如果你也試圖打算從頭做預訓練，可以參考看看這個專案估算需要花多少時間。
* 目前已經完成訓練，作者也分享了許多訓練過程中的成果，包含模型權重、訓練過程的Loss 曲線圖，以及各種預訓練的踩雷經驗等。
* 作者也有對 TinyLlama進行指令微調，但是因為模型參數量很小的關係， 所以能力並沒有很強大。
* 也因為是個小模型的關係，所以很適合做一些小規模的研究，若是有些新的技術或框架想要嚐鮮，筆者通常也會優先拿 TinyLlama 來試刀，速度快體積小相容性高。
* 有趣的是作者有公開模型訓練的 [Wandb](https://tinyurl.com/bdz8stb5) 追蹤（這個連結好像經常會換，請以 GitHub 頁面上為準），我們鮮少有機會看到一個活跳跳的 LLM Loss Curve，有興趣的可以關注看看。
* 參考連結：
    * [GitHub](https://github.com/jzhang38/TinyLlama)
    * [HF Models](https://huggingface.co/PY007)

### Phi
[Phi 系列](https://huggingface.co/models?sort=trending&search=microsoft-phi)是由微軟推出的一系列語言模型，這個系列從參數量僅 1B 的  [Phi-1](https://arxiv.org/abs/2306.11644)、[Phi-1.5](https://arxiv.org/abs/2309.05463) 等，到現在 3B 左右的 Phi-2、Phi-3，依然維持少少的參數量。
* 除了小參數量的特色以外，還主打**教科書就是你所需要的** (Textbooks Are All You Need) 這個口號，這裡所說的教科書是指**請 LLM 生成一份教科書式的說明**。
    * 因為作者認為這類的資料包含更多**推理步驟**在裡面，這樣的邏輯資訊比起一般的爬蟲文本，對於語言模型的訓練而言是更有幫助的。
* 參考連結：
    * [Phi-1 Model](https://huggingface.co/microsoft/phi-1)
    * [Phi-1.5 Model](https://huggingface.co/microsoft/phi-1_5)
    * [Phi-1 Paper](https://arxiv.org/abs/2306.11644)
    * [Phi-1.5 Paper](https://arxiv.org/abs/2309.05463)

### Mistral & Mixtral
[Mistral](https://mistral.ai/news/announcing-mistral-7b/) 是由法國新創 Mistral Al所開發的語言模型，模型架構上與 Llama 略有不同，使用了**分組查詢注意力** (Grouped-Query Attention, **GQA**) 與**滑動窗口注意力** (Sliding Window Attention, **SWA**) 兩種技術，目的是減少語言型帶來的 GPU 記憶體消耗量，雖然只有 7B 參數量的規格，但是外界評測效果相當好，因此相當受到開源社群的歡迎。
* 獲得巨大的成功之後，官方又進一步推出 Mixtral [8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)、[8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) 高種**混合專家模型**(Mixture-of-Experts, MoE)。
    * MoE 架構就像是同時訓練八個專家的模型，但是每次只挑兩個專家來使用。
    * 因為參數量更大，所以可以容納更多的知識，而且每次推論只使用少量專家，所以能夠維持推論的速度優勢。
* 從官方的數據來看，可以說 Mixtral 8x7B 僅用約 50B 的參數量，就能與 70B 模型的知識量抗衡，而且維持 13B 的推論速度。
    * 但筆者認為知識量與訓練資料的關係也很大，不單純只是看模型架構而已。
    * 而且運行 50B 參數量的模型，所需消耗的記憶體也是相當龐大，即便推論速度很快。
* 比較可惜的是官方為了維持競爭優勢，所以沒有揭露太多與訓練資料相關的資訊。

---

## 中文模型

### Qwen
[Qwen (通義千問)](https://github.com/QwenLM)是由中國阿里巴巴集團所開發的中文語言模型，規模種類相當繁多，從1B、4B 到 72B、110B 都有，甚至多模態的 Qwen-VL、Qwen-Audio 等，全部都開源在HF Hub上，各方面來說都是規模相當大的系列模型。 
* 尤其是多模態模型，雖然與ChatGPT 或 Gemini 相比可能不算頂尖，但已經算是滿有效果了。

### Yi
[Yi](https://huggingface.co/01-ai) 是由李開復博士領銜的中國零一萬物(01-ai)團隊所開發的語言模型， 包含 6B、9B 與 34B 等三種規格，採用 Llama 架構，而且無論簡體中文或是繁體中文能力都相當好，是筆者用過的開源模型裡面數一數二頂尖的中文模型。

---

## 台灣模型
### Breeze & BreeXe
[Breeze](https://huggingface.co/MediaTek-Research) 是由聯發科技集團的 AI 研究單位聯發創新基地(MediaTek Research) 所開發的繁體中文語言模型，架構與權重承襲自 Mistral，同樣為 7B 參數量的模型，但是分詞器有針對繁體中文額外擴充詞表。
* 可能是因為參數量並不大的關係，所以能力上也只是普通而已。
* 開發團隊後來又推出了 [BreeXe-8x7B](https://tinyurl.com/llm-breexe) 的模型，與 Mixtral 一樣採用 MoE 架構，效果大大提昇。
* 是目前筆者用過最頂尖的繁體中文模型，可惜的是權重並不是完全開源的，只能在 [Demo](https://tinyurl.com/llm-breexe-demo) 網頁上簡單測試而已。

### Taiwan LLM
[Taiwan LLM](https://github.com/MiuLab/Taiwan-LLM) 是由國立臺灣大學資訊工程學系機器智慧與理解實驗室 (Machine Intelligence & Understanding Laboratory， MiuLab) 訓練的模型，如果你經常在 YouTube 上收看一些深度學習相關的學校課程影片，可能就會知道這是[陳縕儂教授](https://www.youtube.com/@VivianMiuLab)的實驗室。
* 使用超過 300億 Tokens 的繁體中文文本，對 Llama 系列的模型進行**持續預訓練**(Continual Pretraining， **CP**)，並使用超過 100萬 筆對話資料做指令微調。
* 這個模型不單單只是會輸出繁體中文而已，還主打臺灣文化的融入。
* 除了LLAMA 與 Llama2 以外，作者也基於 Mixtral-8x7B 與 Llama 3 等模型開發出不同版本的 Taiwan LLM，最近更是推出了效果十分強大的 Taiwan Llama 3 8B & 70B 的模型，外界評估各項表現都相當驚人，可以在 [Demo](https://twllm.com/) 網頁試用看看。

### TAIDE
[TAIDE](https://taide.tw/index) 是由臺灣國科會主持的計畫，目標是要發展繁體中文的可信任生成式 AI 對話引擎。
* 訓練資料包含了許多臺灣政府機關的文件，例如高雄旅遊網、慢遊雲林等觀光網頁，以及繁體中文維基百科、教育部的字典和各大媒體授權的新間文章等。總共大約 40B Tokens 的繁體中文文本，使用國網中心的 H100 訓練而成。
* 目前已經釋出了 Llama 27B 與 Llama 38B 等版本的模型，算是小有成果。

### Bailong
[白龍 Bailong](https://tinyurl.com/llm-bailong) 是由群創光電所開發的模型，基於 Llama 27B 使用 QLoRA 與 Zip-Tie Embedding 等技術訓練而成。
* QLoRA 會在之後的章節介紹。
* 所謂的 Zip-Tie Embedding 指的是在擴增詞表時，使用舊詞的Embedding 來合成新詞的 Embedding，這樣新詞的Embedding 就不需要重新訓練與學習了，能進一步提昇模型訓練的效率，這種做法同時也被稱為 Vocabulary Transfer。

### FFM
**福爾摩沙大型語言模型** (Formosa Foundation Model, **FFM**) 是由華碩子公司[台智雲](https://tws.twcc.ai/)開發的系列模型，第一版是以 BLOOMZ 7B 與 176B 為基礎，加入大量繁體中文語料進行訓練的模型，其中文資料佔總訓練資料的 30% 左右。
* 後續也推出了基於 Llama 2、Llama 3、Mistral 與 Mixtral 8x7B 等模型訓練而成的版本，但這些模型權重並沒有開源，而是放在他們的AFS平台上提供服務，有商用需求的讀者不妨參考看看。

---

## 其他資源

### Awesome LLM
[Awesome LLM](https://github.com/Hannibal046/Awesome-LLM) 整理了這些如天上繁星般多的語言模型們，其中有張[動畫](https://tinyurl.com/llm-tree-gif)非常有意思，這個動畫講述了整個語言模型的發展史，我想未來大概會跟這張 [Linux Distro Tree](https://tinyurl.com/linux-distro-tree) 一樣越長越大而且越來越複雜。

### Open LLMs
[Open LLMs](https://github.com/eugeneyan/open-llms) 整理了多個可商用的開源模型，如果有商用需求且需要借助開源力量的人可以參考看看此資訊。

### Open LLM Leaderboard
[Open LLM Leaderboard](https://tinyurl.com/llm-leaderboard) 是放在 HF Space 上的一個應用，裡面收錄了許多 HF Hub上的模型進行評估的結果，並整理而成的天梯榜。
* 雖然可以參考這個排行榜來選用模型，但這個排行榜進行的評估未必能充分反映實際情況，最好還是以自身的實際應用為主，並進行足夠完整的測試評估會比較恰當。

---

## 連結
* [iThome News: 台智雲](https://www.ithome.com.tw/news/158874)
* [國科會 TAIDE](https://www.nstc.gov.tw/folksonomy/detail/f094b57a-204e-4114-99d3-a9412ae42d7a?l=ch)
* 本篇以書籍為主，[iThome 中的文章](https://ithelp.ithome.com.tw/articles/10329007)裡還有一些其他的模型介紹。

---

## 結論
* 把 Awesome LLM 打開就知道，筆者所介紹的這些模型完全只是冰山一角。
* 本章節主要是介紹一般用途的語言模型，通常只用於普通的文字生成任務，例如問答、翻譯或摘要等等。但其實還有一系列的模型是寫程式的專家。

---