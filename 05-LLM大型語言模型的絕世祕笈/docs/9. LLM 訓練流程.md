# LLM 訓練流程
訓練一個 LLM 通常包含兩個步驟：**監督式微調**與**增強式學習**，不過也有許多模型只做監督式微調而不做增強式學習。

訓練流程說明:
* OpenAI 的 [InstructGPT 論文](https://arxiv.org/abs/2203.02155)。
* 台大電機系李宏毅教授的[影片講解](https://youtu.be/e0aKI2GGZNg)。

---

## 監督式微調 (Supervised Fine-Tuning)
本質是在描述傳統訓練深度學習模型的一個步驟，在 LLM 裡面這個步驟又分成**預訓練**與**指令微調**兩個子步驟。

### 預訓練 (Pretraining) 
此階段會使用大量文本作為訓練資料，並且經過精細的資料前處理而來。
* 來源: 
  網路爬蟲、維基百科、電子書文本等等。
* 這些資料透過**因果語言建模 (Causal Language Modeling)** 的訓練手法放進模型裡面。
* 語言模型只會學習如何進行==文字接龍==。
    * 通常不具備任何問答或聊天的能力，只會根據已知的輸入句，預測接下來的文字機率分佈，並在多次取樣後形成一個自然的語句。
        * 多次取向的步驟，也被稱為**自回歸生成 (Autoregressive Generation)**。
    * 學習並累積大量的詞彙、文法以及**世界知識 (World Knowledge)**，但不知道如何互動而已。
* 語言模型其實本來就具備執行此任務的能力，但他只是不知道什麼時候該發動這個能力。

### 指令微調 (Instruction Fine-Tuning) 
描述將一個自然語言的任務當作一組一組的 **問答組合 (QA Pairs)** 來表示。
* 說明範例
    * 情感分類的任務:
      ```
        我很開心 ➜ Positive
        我好難過 ➜ Negative
      ```
    * 表示為指令格式:
      ```
        指令：請問以下文句的情感分類為何？
        輸入：我很開心
        輸出：Positive
      ```
* 「語言模型社會化」的過程:
    * 讓資料集產生了**更貼近自然語言**的**文句因果關係**。
    * 把這個因果關係強烈的自然語言描述丟進解因果能力強大的預訓練 LLM 裡面，LLM 學會了如何回答問題！
* 在這個階段，語言模型主要在學習==識別任務==的能力。

### 補充說明: 對話微調 (Chat Fine-Tuning)
這個動作的內容其實與指令微調十分相似，但對話微調一詞更著重在表達==應用階段==時的對話能力。
* 例如模型打招呼的能力、模型自我介紹的能力等等。
* 需要建立一個能夠讓前端應用解析的格式。
    ```
    ### User: 你好啊，請問你是誰？

    ### Assistant: 你好，我是 Assistant 助手，有什麼我能幫助您的嗎？
    ```
    * 範例中，除了建立模型打招呼與自我介紹的能力以外，前端應用也可以透過偵測 `###` 是否出現，來確定 LLM 是否完成回答。
* 通常都會將 Instruction Tuning 與 Chat Fine-Tuning 兩個概念合在一起，不用仔細區分。但有些研究依然會將這兩個方法分開來談。

---

## 增強式學習 (Reinforcement Learning, RL) 
泛用於各種領域的一個訓練手法。
* 目的: 為了==最大化獲得的分數==。
* 例如，訓練一個 AI 玩遊戲。
    * 讓模型扮演**玩家 (Agent)** 並根據**遊玩策略 (Policy)** 來進行**動作 (Action)**。
        * 例如現在要移動還是射擊
    * 而這個動作會產生一個**分數 (Reward)** 用來代表玩家現在的表現如何。
        * 像是玩家獲得的金錢、獎勵等等。
* 對 LLM 進行 RL 的訓練，其目的在於讓模型的輸出變得**更符合人類的喜好**。
    * 這個階段的模型已經具備解答問題的能力，但是回覆的內容可能還不夠好。
        * 例如解釋的不夠詳細、摘要的長度太長、翻譯的細節太少等等。
    * **防止模型產生有害的輸出**。
* ChatGPT 使用**從人類反饋中進行強化學習 (Reinforcement Learning from Human Feedback, RLHF)** 這個做法。
    * 因為要從人類反饋中進行學習，所以需要先**將人類反饋訓練成一個獎勵模型**。

### 獎勵模型 (Reward Model, RM) 
主要用來對 LLM 的輸出進行**排名 (Ranking)**。
* 將兩個或數個句子輸入到 RM 裡面，而 RM 會對這些句子輸出一個排名，用來代表這些句子之間誰的品質更好。
* 根據 InstructGPT 論文所述，這個 RM 也是個 GPT 模型，但是只有 6B 的參數量。除了可以節省運算量以外，論文做法也發現使用太大的 RM 反而有不穩定的現象出現。
1. **請人幫忙打分數**
    * RLHF
    * InstructGPT 論文使用的訓練資料有三萬多筆。
    * 導致此做法在其他 LLM 裡面比較不常見。
2. **[GPT-Score](https://arxiv.org/abs/2302.04166) 透過 LLM 幫忙打分數**
    * 減輕對人工標注資料集的需求。
![Sa6GP5o](https://hackmd.io/_uploads/H11mDgsggl.png)

### 近端策略優化 (Proximal Policy Optimization, PPO) 
是 RLHF 裡面採取的策略，這個策略的出發點在於**避免太大的策略更新**。
* 如果突然往奇怪的方向更新了一大步，那可能就會再也回不來了。
* 在 RL 之前的 LLM 其實已經有很好的文字生成能力了，只是需要再調整一下回覆而已。
    * 開發者希望 RL 的過程不要對 LLM 帶來太大的破壞。

### 直接偏好最佳化 (Direct Preference Optimization, DPO)
為了省去 RM ，直接把 LLM 本身當成 獎勵分數 (Reward) 的來源並進行訓練。
[論文連結](https://arxiv.org/abs/2305.18290)
* 因為 RM 無論是訓練資料還是模型本身都太難取得了。
* 大幅簡化了整個 RLHF 的過程，節省更多的運算量與訓練成本。

### 增強是學習框架: TRL
由 Hugging Face 開發的 [TRL](https://huggingface.co/docs/trl/index) 套件替我們整合了整個 Transformer 模型會用到的強化學習演算法，讓我們可以簡單的操作整個強化學習的框架。
* 可以使用 `SFTTrainer`, `RewardTrainer`, `PPOTrainer` 等類別，來操作一個完整的 RLHF 流程。
* 另外也有 DPOTrainer 等其他訓練方法可以選擇。
* [官方文件](https://huggingface.co/docs/trl/index) & [GitHub](https://github.com/huggingface/trl)

---

## 參考
* [HackMD: shaoeChen - PPO](https://hackmd.io/@shaoeChen/Syez2AmFr)
* [Medium: Sandra - PPO & TRPO](https://tinyurl.com/mpf7dxzx)
* [HF Deep RL Course: PPO](https://huggingface.co/learn/deep-rl-course/unit8/introduction)
* [YouTube: 李宏毅教授 - Proximal Policy Optimization (PPO)](https://youtu.be/OAKAZhFmYoI)
* [Wikipedia: 近端策略優化](https://w.wiki/7Xy4)
* [Paper: Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)
* [Paper: Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
* [Paper: Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

---

## 結論
* 介紹了 LLM 常見的訓練流程，包含監督式微調 (SFT) 與增強式學習 (RL) 等，瞭解這個流程有助於我們更進一步理解 LLM 的本質。
* 多數的研究都還是著重在 SFT 這塊，但 RL 也是相當重要的一環，尤其是目標為訓練 Local LLM 進行複雜的文字任務，例如摘要、翻譯或問答等等。

---