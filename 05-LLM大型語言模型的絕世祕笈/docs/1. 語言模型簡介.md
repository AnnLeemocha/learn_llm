# 語言模型簡介
傳統: 統計式語言模型
目前主流: 以機器學習方法為主的神經網路模型

---

## 何謂語言模型
一種用來描述文字分佈的統計機率，廣義來說，任何透過統計文字機率的來處理自然語言問題的方法，都可以算是一種語言模型。
* 透過文本捕捉語言結構，進而建立一個統計機率模型，廣義而言就可以被稱作一種語言模型。

常見應用: 
* 語音辨識
    * 計算某句話出現的機率，用來判斷哪句話更有可能是使用者實際想說的話。

---

## 語言模型的種類
* N-Gram 模型
    * 一種相當單純且直觀的統計模型，主要透過統計連續字詞的頻率來建立機率模型。
    * 實際範例: Unigram、Bigram 和 Trigram 
    * 有足夠的記憶體消耗，可以更準確地捕捉語言模式和結構，從而進行更精確的分析和預測。
    * 缺點: 存在嚴重的稀疏問題與上下文捕捉能力不好等等，因此能夠實際應用的場景並不多。
* HMM 隱藏式馬可夫模型
    * 會考慮一些隱藏狀態的問題，比起前者可能還會考慮詞性之類的。
* RNN 循環神經網路
    * 語言模型踏入深度學習領域的先河
    * 實際範例: LSTM、GRU 
    * 運作方式: 
      給定一段輸入序列，神經網路會從頭到尾一個一個看，每看過一個元素就更新一次隱藏狀態。
    * 變體: Bidirectional RNN 
        * 從頭看到尾之後，會再從尾看到頭。
    * 缺點: 計算成本非常高昂，平行運算不易，有訓練效率不佳的問題。
* Transformers
    * 2017 年 Google 在 Attention Is All You Need 論文中提出 Transformers 架構進行機器翻譯的改善。[論文連結](https://arxiv.org/abs/1706.03762)
    * Attention 機制算法
        1. Bahdanau Attention [論文連結](https://arxiv.org/abs/1409.0473)
        2. Luong Attention [論文連結](https://arxiv.org/abs/1508.04025)
    * 2018 年來自 Google 的 Devlin 等人提出 BERT 模型 [論文連結](https://arxiv.org/abs/1810.04805)
    * 模型的參數量+++++

### Bigram LM 實作
* 📝 範例程式碼
    * [完整程式碼](https://colab.research.google.com/drive/1LWXJIVI2mOvj8AveEJkBrDiCZaNFrWmP?hl=en)
    * 自己嘗試: .../LLM/project/bigram_lm

---

## 參考
* [Wikipedia: N-Gram](https://w.wiki/7Tx$)
* [Wikipedia: 語言模型](https://w.wiki/7Ty3)
* [Large Language Model](https://w.wiki/7Ty5)
* [維基文庫](https://zh.wikisource.org/zh-hant/)
* [開放文學](http://open-lit.com/)
* [ChatGPT](https://chat.openai.com/)

---

## 結論
* Bigram 模型非常基礎的概念，實際上機率模型在運作時還有很多細節需要處理，例如遇到未知詞時，需要做 Fallback 之類的。
* 雖然 Bigram 模型的生成效果並不理想，但是一個「有趣但是沒用」的遊戲性實驗。

---