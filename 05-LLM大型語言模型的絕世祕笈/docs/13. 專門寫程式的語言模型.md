# å°ˆé–€å¯«ç¨‹å¼çš„èªè¨€æ¨¡å‹
Code LLMs æ˜¯å°ˆé–€æä¾›ç¨‹å¼ç¢¼å”åŠ©çš„å¤§å‹èªè¨€æ¨¡å‹ï¼Œå› ç‚ºå¯«ç¨‹å¼çš„æƒ…å¢ƒå‰›å¥½éå¸¸ç¬¦åˆ Decoder LM æ–‡å­—æ¥é¾çš„ç‰¹æ€§ï¼Œå› æ­¤ç™¼å±•æ¯” Chat LLM åˆæ—©äº†ä¸€äº›ï¼Œä¾‹å¦‚ Tabnine èˆ‡ Copilot ä¹‹é¡çš„ï¼Œå…¶å¯¦éƒ½å‡ºç¾çš„æ¯” ChatGPT é‚„è¦æ—©ã€‚

---

## StarCoder
[StarCoder](https://github.com/bigcode-project/starcoder) ç”± [BigCode](https://www.bigcode-project.org/) é–‹ç™¼ï¼Œèˆ‡ [BLOOM](https://arxiv.org/abs/2211.05100) çš„ [BigScience](https://bigscience.huggingface.co/) ä¸€æ¨£ï¼ŒBigCode ä¹Ÿæ˜¯ Hugging Face ä¸»æŒçš„ä¸€å€‹ Workshop çµ„ç¹”ã€‚
* StarCoder çš„æ¨¡å‹ç‚º BigCode ä»–å€‘è‡ªå·±ç ”ç™¼çš„ [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode) æ¶æ§‹ï¼Œå¾ GPT-2 çš„æ¶æ§‹å„ªåŒ–ä¿®æ”¹è€Œä¾†ã€‚StarCoder çš„å‰èº« [SantaCoder](https://arxiv.org/abs/2301.03988) ä¹Ÿæ˜¯ä½¿ç”¨æ­¤æ¶æ§‹ã€‚
* å…¶è¨“ç·´è³‡æ–™è¢«æ•´ç†ç‚º [StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata)ï¼ŒåŒ…å« 86 ç¨®ç¨‹å¼èªè¨€ä»¥åŠå„ç¨® GitHub Issues çš„è³‡æ–™ï¼Œå¤§å° 783GB å…± 250B Tokens çš„æ–‡æœ¬é‡ã€‚
* StarCoder åŸå…ˆåªæœ‰ 15B åƒæ•¸é‡çš„è¦æ ¼ï¼Œå¾Œä¾†é™¸çºŒå¢åŠ äº† 7B, 3B, 1B ç­‰ç‰ˆæœ¬ï¼Œè®“è³‡æºæœ‰é™çš„äººå¯ä»¥æ›´è¼•é¬†çš„éƒ¨ç½²é€™äº›æ¨¡å‹ã€‚
* ä¸»è¦æœ‰ä¸‰ç¨®ç‰ˆæœ¬ï¼š
    * StarCoderBaseï¼š åªç”¨ç¨‹å¼ç¢¼è³‡æ–™åšé è¨“ç·´ã€‚
    * StarCoderPlusï¼š ä½¿ç”¨ç¨‹å¼ç¢¼è³‡æ–™åŠ ä¸Šè‹±æ–‡è‡ªç„¶èªè¨€æ–‡æœ¬è³‡æ–™ä¸€èµ·è¨“ç·´ã€‚
    * StarCoderï¼š ä½¿ç”¨ç¨‹å¼ç¢¼è³‡æ–™è·Ÿå¾ˆå¤š Python åŸå§‹ç¢¼è³‡æ–™åšè¨“ç·´ã€‚
* BigCode å¦å¤–æœ‰ [StarChat](https://huggingface.co/HuggingFaceH4/starchat-beta) æ¨¡å‹ï¼Œç‚º StarCoderPlus é€²è¡Œ Instruction Tuning å¾Œçš„ç‰ˆæœ¬ã€‚
* å¾Œä¾†ä¹Ÿæ¨å‡ºäº† SantaCoder 2 ç³»åˆ—ï¼Œä½¿ç”¨è¶…é 600 ç¨®ç¨‹å¼èªè¨€ç´„ 4T Token çš„ [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2) çš„è³‡æ–™åŠé€²è¡Œè¨“ç·´ï¼Œèƒ½åŠ›æ¯”ä¸Šä¸€ä»£å¥½ã€‚
* å¯ä»¥ä½¿ç”¨ HF Transformers å¥—ä»¶ä¾†ç©çœ‹çœ‹é€™å€‹æ¨¡å‹ï¼Œé€™é‚Šä»¥StarCoder 2 7B ç¤ºç¯„ï¼š
    * é¦–å…ˆè®€å–æ¨¡å‹æ¬Šé‡èˆ‡åˆ†è©å™¨ï¼Œä¸¦å°‡ Generate åŒ…è£æˆå‡½å¼æ–¹ä¾¿å¾ŒçºŒä½¿ç”¨ï¼š
        ```python
        import torch
        from transformers import GPT2Tokenizer as TkCls
        from transformers import Starcoder2ForCausalLM as ModelCls
        from transformers import TextStreamer

        model_id= "bigcode/starcoder2-7b"
        model: ModelCls = ModelCls.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.float16,
        )
        tk: TkCls = TkCls.from_pretrained(model_id)
        ts = TextStreamer(tk)

        def generate(prompt, n):
            inputs = tk(prompt, return_tensors="pt").to("cuda")
            return model.generate(**inputs, max_new_tokens=n, streamer=ts)
        ```
        ä¸€èˆ¬çš„ç¨‹å¼ç¢¼ç”Ÿæˆï¼Œåªéœ€è¦é€éæ¨¡å‹æœ¬èº«çš„æ–‡å­—æ¥é¾èƒ½åŠ›å³å¯é”æˆï¼š
        ```python
        generate("def fibin: int):", 28)

        # æ˜¯å€‹éå¸¸æ¨™æº–çš„è²»æ³¢é‚£å¥‘æ•¸å‡½å¼
        """
        def fib(n: int):
            if n == 0:
                return 0
            elif n == 1:
                return 1
            else:
                return fib
        """
        ```
        * ä½†æ˜¯ä¸€èˆ¬ä¾†èªªï¼Œå¯«ç¨‹å¼æ™‚ä¸¦ä¸æœƒç¸½æ˜¯åœ¨æª”æ¡ˆçš„æœ€å°¾ç«¯ï¼Œå› æ­¤åœ¨æ–‡å­—ç·¨è¼¯å™¨çš„æ¸¸æ¨™å‰å¾Œæˆ–å¤šæˆ–å°‘éƒ½æœ‰å…¶ä»–ç¨‹å¼ç¢¼ï¼Œå¯«ç¨‹å¼æ™‚ä¸åƒ…è¦åƒè€ƒå‰é¢çš„ç¨‹å¼ç¢¼ï¼Œä¹Ÿè¦åƒè€ƒå¾Œé¢çš„ç¨‹å¼ç¢¼ã€‚
        * ç„¶è€ŒDecoder çš„ç‰¹æ€§ä½¿ä»–åªèƒ½å¾€å¾Œç”Ÿæˆæ–‡å­—ï¼Œè€Œä¸èƒ½åœ¨ä¸­é–“é€²è¡Œç”Ÿæˆã€‚
    * é€™æ™‚å€™ StarCoder é€éç‰¹æ®Šçš„ FIM Token ä¾†å®Œæˆ [**å¡«å……ç¨‹å¼ç¢¼ (Fill in the Middle, FIM**)](https://arxiv.org/abs/2207.14255) çš„ä»»å‹™ã€‚
        èˆ‰ä¾‹ä¾†èªªï¼š
        ```python
        def hello (name: str):
            print # <=éµç›¤æ¸¸æ¨™åœ¨é€™!

        def goodbye (name: str):
            print(f"### ç³»çµ±ï¼šå†æœƒäº†ï¼Œ{name}!")
        ```
        * é€™è£¡ä½¿ç”¨ `<æ¸¸æ¨™åœ¨é€™>` ä¾†è¡¨ç¤ºä½¿ç”¨è€…çš„æ¸¸æ¨™åœåœ¨æ­¤è™•
        * ä¸¦æ ¹æ“šæ¸¸æ¨™ä½ç½®å°‡ç¨‹å¼ç¢¼åˆ‡æˆå…©åŠï¼Œå‰åŠæ®µæ˜¯ **Prefix** å¾ŒåŠæ®µæ˜¯ **Suffix**
        * å°‡ Prefix çš„å…§å®¹æ”¾åœ¨ `<fim_prefix>` å¾Œé¢
        * å°‡ Suffix çš„å…§å®¹æ”¾åœ¨ `<fim_suffix>` å¾Œé¢
        * æœ€å¾Œæ”¾ä¸Š `<fim_middle>` ä¾†ç™¼å‹• StarCoder å¡«å……ç¨‹å¼ç¢¼çš„èƒ½åŠ›
        ```txt
        <fim_prefix> [å‰åŠæ®µçš„å…§å®¹] <fim_suffix> [å¾ŒåŠæ®µçš„å…§å®¹] <fim_middle>
        ```
        å°‡é€™ç¨®æ ¼å¼çš„æç¤ºè¼¸å…¥æ¨¡å‹ï¼Œå°±èƒ½å¤ ç”¢ç”Ÿä¸­é–“çš„ç¨‹å¼ç¢¼ã€‚
        * æ¨¡å‹èƒ½å¤ å…·æœ‰é€™æ¨£çš„èƒ½åŠ›ï¼Œå°±æ˜¯å› ç‚ºåœ¨è¨“ç·´çš„æ™‚å€™ï¼ŒFIM è¨“ç·´æœƒå°‡æŒ–ç©ºçš„ç¨‹å¼ç¢¼ç‰‡æ®µæ”¾åœ¨`<fim_middle>` å¾Œé¢ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæ¨¡å‹å°±å¯ä»¥åœ¨ç¶­æŒåŸæœ¬æ–‡å­—æ¥é¾èƒ½åŠ›çš„æƒ…æ³ä¸‹ï¼Œç²å¾—ç¨‹å¼ç¢¼å¡«å……çš„èƒ½åŠ›ã€‚
        * StarCoder 2 åœ¨ FIM æ¨¡å¼ä¸‹ç”¨ä¾†åœæ­¢ç”Ÿæˆçš„ Token æ˜¯ `<file_sep>`
        ```python
        def generate(prompt, n):
            inputs = tk(prompt, return_tensors="pt").to("cuda")
            return model.generate(
                **inputs,
                max_new_tokens=n,
                streamer=ts,
                eos_token_id=tk.encode("<file_sep>")[-1],
            )
        ```
    * å¯¦éš›æ“ä½œé€™ç¨®æ¨è«–æ–¹æ³•:
        ```python
        full_code = """

        def hello(name: str):
            print(<æ¸¸æ¨™åœ¨é€™>)

        def goodbye(name: str):
            print(f"### ç³»çµ±ï¼šå†æœƒäº†ï¼Œ{name}!")

        """

        prefix, suffix = full_code.split("<æ¸¸æ¨™åœ¨é€™>", 1)
        full_prompt = f"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>"
        outputs = generate(full_prompt, 16)

        # å¾—åˆ°ä»¥ä¸‹è¼¸å‡º:
        """
        <fim_prefix>

        def hello(name: str):
            print(<fim_suffix>)

        def goodbye(name: str):
            print(f"### ç³»çµ±ï¼šå†æœƒäº†ï¼Œ{name}!")

        <fim_middle>f"### ç³»çµ±ï¼šä½ å¥½ï¼Œ{name}!"<file_sep>
        """
        ```
    * æœ€å¾Œé€éä¸€äº›å¾Œè™•ç†ï¼Œå°‡å¡«å……ç¨‹å¼ç¢¼çš„ç”Ÿæˆçµæœé‡å»ºå›åŸæœ¬çš„ç¨‹å¼ç¢¼:
        ```python
        tokens = tk.encode(full_prompt)
        output = outputs[0][len(tokens):]
        middle = tk.decode(output, skip_special_tokens=True)
        print(full_code.replace("<æ¸¸æ¨™åœ¨é€™>", middle))
        
        # å¾—åˆ°æœ€å¾Œå®Œæ•´çš„ç¨‹å¼ç¢¼å¦‚ä¸‹:
        """
        def hello(name: str):
            print(f"### ç³»çµ±ï¼šä½ å¥½ï¼Œ{name}!")

        def goodbye(name: str):
            print(f"### ç³»çµ±ï¼šå†æœƒäº†ï¼Œ{name}!")
        """
        ```
* é€™ç¨®å…ˆæ”¾Prefix å†æ”¾ Suffix æœ€å¾Œæ¥ Middle çš„æ ¼å¼ï¼Œ**ä¹Ÿè¢«ç¨±ç‚ºPSM (Prefix-Suffix-Middle**) æ¨¡å¼ã€‚
* å¦å¤–ä¹Ÿæœ‰å…ˆæ”¾ Suffix å†æ”¾ Prefix æœ€å¾Œ Middle çš„ç‰ˆæœ¬ï¼Œ**è¢«ç¨±ç‚ºSPM (Suffix-Prefix-Middle**) æ¨¡å¼ã€‚
* åƒè€ƒé€£çµï¼š
    * [GitHub](https://github.com/bigcode-project/starcoder)
    * [HF Models](https://huggingface.co/collections/bigcode/%E2%AD%90-starcoder-64f9bd5740eb5daaeb81dbec)
    * [HF Datasets](https://huggingface.co/datasets/bigcode/starcoderdata)
    * [HF Docs](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)
    * [StarChat Demo](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)
    * [Paper](https://arxiv.org/abs/2305.06161)
* ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼
    * [ç­†è€…ç¨‹å¼ç¢¼](https://colab.research.google.com/drive/1i10_7S1XogsU1N9CngpJg-R3aDMDryv1)
    * è‡ªå·±å˜—è©¦: .../LLM/project/code_llms

---

## CodeGen
[CodeGen](https://github.com/salesforce/CodeGen) æ˜¯ [Salesforce](https://www.salesforce.com/) è¨“ç·´çš„ä¸€ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å« CodeGen, CodeGen2 èˆ‡ç¾åœ¨çš„ CodeGen2.5 é€™å¹¾ä»£ã€‚
* å‰å¹¾ä»£åƒæ•¸é‡éƒ½æœ‰å‡ºåˆ° 16B çš„è¦æ ¼ï¼Œä½† CodeGen 2.5 åªå‡ºäº† 7B é€™å€‹è¦æ ¼ã€‚
    * æ ¹æ“š Salesforce è«–æ–‡æ‰€è¿°ï¼ŒåŸæœ¬ OpenAI æå‡ºçš„**ç¸®æ”¾å®šå¾‹ (Scaling Law**) èªç‚ºæ¨¡å‹åƒæ•¸é‡èˆ‡è¨“ç·´è³‡æ–™é‡æœ‰å€‹ä¸€å®šçš„æ¯”ç‡ï¼Œç•¶è³‡æ–™é‡è¶…å‡ºæ¨¡å‹åƒæ•¸é‡çš„æŸå€‹æ¯”ä¾‹ä¹‹å¾Œï¼Œæ¨¡å‹çš„æ•ˆæœæå‡æœƒå‡ºç¾é‚Šéš›æ•ˆæ‡‰ã€‚
    * å¦å¤–ï¼Œåœ¨è¨“ç·´èªè¨€æ¨¡å‹æ™‚ï¼Œç‚ºäº†é¿å…éåº¦æ“¬åˆ (Over Fitting) çš„æƒ…æ³ï¼Œé€šå¸¸ä¸€ä»½æ–‡æœ¬åªæœƒè¨“ç·´ä¸€æ¬¡ã€‚
    * ä½† Salesforce å°é€™é»æŠ±æŒæ‡·ç–‘çš„æ…‹åº¦ï¼Œæ–¼æ˜¯çµåˆäº† MLM èˆ‡ CLM å…©ç¨®å»ºæ¨¡æ–¹å¼ï¼Œè—‰æ­¤å°èªè¨€æ¨¡å‹é€²è¡Œå¤šå€‹ Epochs çš„è¨“ç·´ï¼Œæœ€å¾Œå¾—åˆ°æ•ˆæœç›¸ç•¶å¥½çš„ CodeGen 2.5 æ¨¡å‹ï¼š
    ![24ONC1j](https://hackmd.io/_uploads/rkULHiTblx.png)
    * å¾è«–æ–‡çš„è©•ä¼°æ•¸æ“šå¯è¦‹ï¼Œé€™å€‹ CodeGen2.5 é›–ç„¶åªæœ‰ 7B çš„åƒæ•¸é‡ï¼Œä½†æ˜¯å¾ˆå¤§å¹…åº¦çš„æ‰“æ•—äº† CodeGen2 16B çš„æ¨¡å‹ï¼Œå› æ­¤ Salesforce ç›¸ç•¶æœ‰è‡ªä¿¡çš„æ¨å‡ºåƒ… 7B å¤§å°çš„ CodeGen2.5 æ¨¡å‹ï¼Œä¸¦å°‡éƒ¨è½æ ¼çš„ä»‹ç´¹æ–‡ç« æ¨™é¡Œå‘½åç‚ºã€Œ[å°è€Œå¼·å¤§ (Small, but mighty)](https://blog.salesforceairesearch.com/codegen25/)ã€ã€‚
* CodeGen2.5 åˆ†æˆä¸‰ç¨®ç‰ˆæœ¬ï¼š
    * [Multi](https://huggingface.co/Salesforce/codegen25-7b-multi) ä½¿ç”¨ StarCoderData è¨“ç·´çš„æ¨¡å‹ï¼ŒåŒ…å«å¤šç¨®ç¨‹å¼èªè¨€ã€‚
    * [Mono](https://huggingface.co/Salesforce/codegen25-7b-mono) åŸºæ–¼ StarCoderData é¡å¤–åŠ ä¸Šæ›´å¤š Python ç¨‹å¼ç¢¼çš„è¨“ç·´è³‡æ–™ã€‚
    * [Instruct](https://huggingface.co/Salesforce/codegen25-7b-instruct) åŸºæ–¼ Mono ç‰ˆåš Instruction Tuning å¾Œçš„æ¨¡å‹ã€‚
* é™¤äº†ä¸€èˆ¬çš„ç¨‹å¼ç¢¼æ¥é¾ä»¥å¤–ï¼ŒCodeGen2.5 åŒæ¨£å…·æœ‰å¡«å……ç¨‹å¼ç¢¼çš„èƒ½åŠ›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    ```txt
    prefix + "<mask_1>" + suffix + "<|endoftext|>" + "<sep>" + "<mask_1>"
    ```
    * ä½¿ç”¨æ­¤æ ¼å¼é€²è¡Œæ¨è«–æ™‚ï¼Œæœ€å¾Œæœƒç”Ÿæˆä¸€å€‹ `<eom>` çš„ End-Of-Mask Tokenï¼Œå¯ä»¥æ ¹æ“šæ­¤ Token å‡ºç¾èˆ‡å¦ä¾†åœæ­¢ç”Ÿæˆã€‚
* åƒè€ƒé€£çµï¼š
    * [GitHub](https://github.com/salesforce/CodeGen)
    * [Blog Post](https://blog.salesforceairesearch.com/codegen25/)
    * [HF Models](https://huggingface.co/models?search=salesforce%20codegen)
    * [Paper](https://arxiv.org/abs/2305.02309)

---

## CodeLlama
[CodeLlama](https://github.com/facebookresearch/codellama) æ˜¯å¾å·²ç¶“å®Œæˆè¨“ç·´çš„ Llama 2 ç¹¼çºŒè¨“ç·´è€Œä¾†çš„ï¼Œä½¿ç”¨ 500B Tokens è³‡æ–™é‡çš„ç¨‹å¼ç¢¼ï¼Œä¸¦åŠ ä¸Šç´„ 8% çš„è‡ªç„¶èªè¨€è³‡æ–™ä¾†ç¶­æŒåŸæœ¬è‡ªç„¶èªè¨€çš„èƒ½åŠ›ã€‚ä¸¦é€é**é•·ä¸Šä¸‹æ–‡å¾®èª¿ (Long Context Fine-Tuning, LCFT**) çš„æŠ€å·§ï¼Œå°‡ Llama 2 åŸæœ¬ 4K çš„æ¨¡å‹é•·åº¦æ“´å±•åˆ°äº† 16Kï¼Œé€™å€‹æ–¹æ³•é¡ä¼¼æ–¼**å…§æ’æ³• (Position Interpolation, PI**)ï¼Œé€éæ”¹é€ **æ—‹è½‰ä½ç½®ç·¨ç¢¼ (Rotary Position Embedding, RoPE**) ä¾†æé«˜æ¨¡å‹é•·åº¦ã€‚
* å®˜æ–¹å®£ç¨± CodeLlama å¯¦éš›ä½¿ç”¨å¯¦ï¼Œå¯ä»¥è™•ç†é•·åº¦é” 100K çš„ç¨‹å¼ç¢¼ã€‚
* CodeLlama åŒ…å« 7Bã€13Bã€34B èˆ‡ 70B å››ç¨®å¤§å°ï¼Œåœ¨ HF Hub ä¸Š[å…¨éƒ¨é–‹æº](https://tinyurl.com/llm-code-llama)ã€‚
* CodeLlama åŒæ¨£å…·æœ‰å¡«å……ç¨‹å¼ç¢¼çš„èƒ½åŠ›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    ```txt
    prompt = f"â–<PRE> {prefix} â–<SUF> {suffix} â–<MID>"
    ```
    * ç”Ÿæˆçš„çµæœæœ€å¾Œæœƒé™„ä¸Šä¸€å€‹ `â–<EOT>` çš„ End-Of-Mask Tokenï¼Œå¯ä»¥æ ¹æ“šæ­¤ Token å‡ºç¾èˆ‡å¦ä¾†åœæ­¢ç”Ÿæˆã€‚
* åƒè€ƒé€£çµï¼š
    * [GitHub](https://github.com/facebookresearch/codellama)
    * [Blog Post](https://ai.meta.com/blog/code-llama-large-language-model-coding/)
    * [Paper](https://arxiv.org/abs/2308.12950)

---

## DeepSeek Coder
[DeepSeek Coder](https://github.com/deepseek-ai/DeepSeek-Coder) æ˜¯ç”± **DeepSeek(æ·±åº¦æ±‚ç´¢)** é€™é–“ä¸­åœ‹å…¬å¸æ‰€é–‹ç™¼çš„ç³»åˆ—æ¨¡å‹ï¼Œå…¶æ¨¡å‹æ¶æ§‹åŸºæ–¼Llamaï¼ŒåŒ…å«1.3Bã€6.7Bèˆ‡33B ä¸‰ç¨®åƒæ•¸é‡ã€‚
* è¨“ç·´è³‡æ–™é‡é” 2T Tokensï¼ŒåŒ…å«87%çš„ç¨‹å¼ç¢¼ã€10%çš„è‹±æ–‡è‡ªç„¶èªè¨€èˆ‡3%çš„ä¸­æ–‡è‡ªç„¶èªè¨€ã€‚
* å› æ­¤é€™å€‹æ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›ç›¸å°å¥½ï¼Œå¾ˆé©åˆç”¨ä¾†è™•ç†åŒ…å«ä¸­æ–‡çš„ç¨‹å¼ç¢¼ã€‚
* åœ¨é–‹æºçš„ Code LLMs è£¡é¢ï¼ŒDeepSeek Coder çš„æ•ˆæœä¹Ÿæ˜¯ç›¸ç•¶é ‚å°–ã€‚

---

## é€£çµ
* [arXiv Paper: Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)
* [HF: Code Llama Usage](https://huggingface.co/blog/codellama)

## çµè«–
* é›–ç„¶å°ˆç²¾ç¨‹å¼ç¢¼çš„èªè¨€æ¨¡å‹ä¸å¦‚ä¸€èˆ¬ç”¨é€”çš„èªè¨€æ¨¡å‹é‚£éº¼å¤šï¼Œä½†ç­†è€…ä»‹ç´¹çš„é€™äº›æ¨¡å‹ä¾ç„¶ä¹Ÿåªæ˜¯å†°å±±ä¸€è§’ã€‚
* é€™äº›é–‹æºçš„ Code LLMs æä¾›çµ¦é–‹ç™¼è€…æ›´å¤šæœ¬åœ°ä¸”è¼•é‡éƒ¨ç½²çš„é¸æ“‡ã€‚
* æ¨¡å‹çš„ç”¨é€”ä¹Ÿå¾åŸæœ¬çš„ç¨‹å¼ç¢¼æ¥é¾ã€ç¨‹å¼ç¢¼å¡«ç©ºä¸€è·¯èµ°å‘å°è©±å½¢å¼ï¼Œæœªä¾†ç”šè‡³é‚„æœ‰æ©Ÿæœƒæœè‘—å¤šæ¨¡æ…‹(Multimodal)é‚é€²ï¼Œæˆç‚ºä¼´éš¨é–‹ç™¼è€…æ›´æœ‰åŠ›çš„åŠ©æ‰‹!

---