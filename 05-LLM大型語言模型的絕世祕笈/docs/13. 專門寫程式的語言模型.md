# 專門寫程式的語言模型
Code LLMs 是專門提供程式碼協助的大型語言模型，因為寫程式的情境剛好非常符合 Decoder LM 文字接龍的特性，因此發展比 Chat LLM 又早了一些，例如 Tabnine 與 Copilot 之類的，其實都出現的比 ChatGPT 還要早。

---

## StarCoder
[StarCoder](https://github.com/bigcode-project/starcoder) 由 [BigCode](https://www.bigcode-project.org/) 開發，與 [BLOOM](https://arxiv.org/abs/2211.05100) 的 [BigScience](https://bigscience.huggingface.co/) 一樣，BigCode 也是 Hugging Face 主持的一個 Workshop 組織。
* StarCoder 的模型為 BigCode 他們自己研發的 [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode) 架構，從 GPT-2 的架構優化修改而來。StarCoder 的前身 [SantaCoder](https://arxiv.org/abs/2301.03988) 也是使用此架構。
* 其訓練資料被整理為 [StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata)，包含 86 種程式語言以及各種 GitHub Issues 的資料，大小 783GB 共 250B Tokens 的文本量。
* StarCoder 原先只有 15B 參數量的規格，後來陸續增加了 7B, 3B, 1B 等版本，讓資源有限的人可以更輕鬆的部署這些模型。
* 主要有三種版本：
    * StarCoderBase： 只用程式碼資料做預訓練。
    * StarCoderPlus： 使用程式碼資料加上英文自然語言文本資料一起訓練。
    * StarCoder： 使用程式碼資料跟很多 Python 原始碼資料做訓練。
* BigCode 另外有 [StarChat](https://huggingface.co/HuggingFaceH4/starchat-beta) 模型，為 StarCoderPlus 進行 Instruction Tuning 後的版本。
* 後來也推出了 SantaCoder 2 系列，使用超過 600 種程式語言約 4T Token 的 [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2) 的資料及進行訓練，能力比上一代好。
* 可以使用 HF Transformers 套件來玩看看這個模型，這邊以StarCoder 2 7B 示範：
    * 首先讀取模型權重與分詞器，並將 Generate 包裝成函式方便後續使用：
        ```python
        import torch
        from transformers import GPT2Tokenizer as TkCls
        from transformers import Starcoder2ForCausalLM as ModelCls
        from transformers import TextStreamer

        model_id= "bigcode/starcoder2-7b"
        model: ModelCls = ModelCls.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.float16,
        )
        tk: TkCls = TkCls.from_pretrained(model_id)
        ts = TextStreamer(tk)

        def generate(prompt, n):
            inputs = tk(prompt, return_tensors="pt").to("cuda")
            return model.generate(**inputs, max_new_tokens=n, streamer=ts)
        ```
        一般的程式碼生成，只需要透過模型本身的文字接龍能力即可達成：
        ```python
        generate("def fibin: int):", 28)

        # 是個非常標準的費波那契數函式
        """
        def fib(n: int):
            if n == 0:
                return 0
            elif n == 1:
                return 1
            else:
                return fib
        """
        ```
        * 但是一般來說，寫程式時並不會總是在檔案的最尾端，因此在文字編輯器的游標前後或多或少都有其他程式碼，寫程式時不僅要參考前面的程式碼，也要參考後面的程式碼。
        * 然而Decoder 的特性使他只能往後生成文字，而不能在中間進行生成。
    * 這時候 StarCoder 透過特殊的 FIM Token 來完成 [**填充程式碼 (Fill in the Middle, FIM**)](https://arxiv.org/abs/2207.14255) 的任務。
        舉例來說：
        ```python
        def hello (name: str):
            print # <=鍵盤游標在這!

        def goodbye (name: str):
            print(f"### 系統：再會了，{name}!")
        ```
        * 這裡使用 `<游標在這>` 來表示使用者的游標停在此處
        * 並根據游標位置將程式碼切成兩半，前半段是 **Prefix** 後半段是 **Suffix**
        * 將 Prefix 的內容放在 `<fim_prefix>` 後面
        * 將 Suffix 的內容放在 `<fim_suffix>` 後面
        * 最後放上 `<fim_middle>` 來發動 StarCoder 填充程式碼的能力
        ```txt
        <fim_prefix> [前半段的內容] <fim_suffix> [後半段的內容] <fim_middle>
        ```
        將這種格式的提示輸入模型，就能夠產生中間的程式碼。
        * 模型能夠具有這樣的能力，就是因為在訓練的時候，FIM 訓練會將挖空的程式碼片段放在`<fim_middle>` 後面。如此一來，模型就可以在維持原本文字接龍能力的情況下，獲得程式碼填充的能力。
        * StarCoder 2 在 FIM 模式下用來停止生成的 Token 是 `<file_sep>`
        ```python
        def generate(prompt, n):
            inputs = tk(prompt, return_tensors="pt").to("cuda")
            return model.generate(
                **inputs,
                max_new_tokens=n,
                streamer=ts,
                eos_token_id=tk.encode("<file_sep>")[-1],
            )
        ```
    * 實際操作這種推論方法:
        ```python
        full_code = """

        def hello(name: str):
            print(<游標在這>)

        def goodbye(name: str):
            print(f"### 系統：再會了，{name}!")

        """

        prefix, suffix = full_code.split("<游標在這>", 1)
        full_prompt = f"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>"
        outputs = generate(full_prompt, 16)

        # 得到以下輸出:
        """
        <fim_prefix>

        def hello(name: str):
            print(<fim_suffix>)

        def goodbye(name: str):
            print(f"### 系統：再會了，{name}!")

        <fim_middle>f"### 系統：你好，{name}!"<file_sep>
        """
        ```
    * 最後透過一些後處理，將填充程式碼的生成結果重建回原本的程式碼:
        ```python
        tokens = tk.encode(full_prompt)
        output = outputs[0][len(tokens):]
        middle = tk.decode(output, skip_special_tokens=True)
        print(full_code.replace("<游標在這>", middle))
        
        # 得到最後完整的程式碼如下:
        """
        def hello(name: str):
            print(f"### 系統：你好，{name}!")

        def goodbye(name: str):
            print(f"### 系統：再會了，{name}!")
        """
        ```
* 這種先放Prefix 再放 Suffix 最後接 Middle 的格式，**也被稱為PSM (Prefix-Suffix-Middle**) 模式。
* 另外也有先放 Suffix 再放 Prefix 最後 Middle 的版本，**被稱為SPM (Suffix-Prefix-Middle**) 模式。
* 參考連結：
    * [GitHub](https://github.com/bigcode-project/starcoder)
    * [HF Models](https://huggingface.co/collections/bigcode/%E2%AD%90-starcoder-64f9bd5740eb5daaeb81dbec)
    * [HF Datasets](https://huggingface.co/datasets/bigcode/starcoderdata)
    * [HF Docs](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)
    * [StarChat Demo](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)
    * [Paper](https://arxiv.org/abs/2305.06161)
* 📝 範例程式碼
    * [筆者程式碼](https://colab.research.google.com/drive/1i10_7S1XogsU1N9CngpJg-R3aDMDryv1)
    * 自己嘗試: .../LLM/project/code_llms

---

## CodeGen
[CodeGen](https://github.com/salesforce/CodeGen) 是 [Salesforce](https://www.salesforce.com/) 訓練的一系列模型，包含 CodeGen, CodeGen2 與現在的 CodeGen2.5 這幾代。
* 前幾代參數量都有出到 16B 的規格，但 CodeGen 2.5 只出了 7B 這個規格。
    * 根據 Salesforce 論文所述，原本 OpenAI 提出的**縮放定律 (Scaling Law**) 認為模型參數量與訓練資料量有個一定的比率，當資料量超出模型參數量的某個比例之後，模型的效果提升會出現邊際效應。
    * 另外，在訓練語言模型時，為了避免過度擬合 (Over Fitting) 的情況，通常一份文本只會訓練一次。
    * 但 Salesforce 對這點抱持懷疑的態度，於是結合了 MLM 與 CLM 兩種建模方式，藉此對語言模型進行多個 Epochs 的訓練，最後得到效果相當好的 CodeGen 2.5 模型：
    ![24ONC1j](https://hackmd.io/_uploads/rkULHiTblx.png)
    * 從論文的評估數據可見，這個 CodeGen2.5 雖然只有 7B 的參數量，但是很大幅度的打敗了 CodeGen2 16B 的模型，因此 Salesforce 相當有自信的推出僅 7B 大小的 CodeGen2.5 模型，並將部落格的介紹文章標題命名為「[小而強大 (Small, but mighty)](https://blog.salesforceairesearch.com/codegen25/)」。
* CodeGen2.5 分成三種版本：
    * [Multi](https://huggingface.co/Salesforce/codegen25-7b-multi) 使用 StarCoderData 訓練的模型，包含多種程式語言。
    * [Mono](https://huggingface.co/Salesforce/codegen25-7b-mono) 基於 StarCoderData 額外加上更多 Python 程式碼的訓練資料。
    * [Instruct](https://huggingface.co/Salesforce/codegen25-7b-instruct) 基於 Mono 版做 Instruction Tuning 後的模型。
* 除了一般的程式碼接龍以外，CodeGen2.5 同樣具有填充程式碼的能力，格式如下：
    ```txt
    prefix + "<mask_1>" + suffix + "<|endoftext|>" + "<sep>" + "<mask_1>"
    ```
    * 使用此格式進行推論時，最後會生成一個 `<eom>` 的 End-Of-Mask Token，可以根據此 Token 出現與否來停止生成。
* 參考連結：
    * [GitHub](https://github.com/salesforce/CodeGen)
    * [Blog Post](https://blog.salesforceairesearch.com/codegen25/)
    * [HF Models](https://huggingface.co/models?search=salesforce%20codegen)
    * [Paper](https://arxiv.org/abs/2305.02309)

---

## CodeLlama
[CodeLlama](https://github.com/facebookresearch/codellama) 是從已經完成訓練的 Llama 2 繼續訓練而來的，使用 500B Tokens 資料量的程式碼，並加上約 8% 的自然語言資料來維持原本自然語言的能力。並透過**長上下文微調 (Long Context Fine-Tuning, LCFT**) 的技巧，將 Llama 2 原本 4K 的模型長度擴展到了 16K，這個方法類似於**內插法 (Position Interpolation, PI**)，透過改造**旋轉位置編碼 (Rotary Position Embedding, RoPE**) 來提高模型長度。
* 官方宣稱 CodeLlama 實際使用實，可以處理長度達 100K 的程式碼。
* CodeLlama 包含 7B、13B、34B 與 70B 四種大小，在 HF Hub 上[全部開源](https://tinyurl.com/llm-code-llama)。
* CodeLlama 同樣具有填充程式碼的能力，格式如下：
    ```txt
    prompt = f"▁<PRE> {prefix} ▁<SUF> {suffix} ▁<MID>"
    ```
    * 生成的結果最後會附上一個 `▁<EOT>` 的 End-Of-Mask Token，可以根據此 Token 出現與否來停止生成。
* 參考連結：
    * [GitHub](https://github.com/facebookresearch/codellama)
    * [Blog Post](https://ai.meta.com/blog/code-llama-large-language-model-coding/)
    * [Paper](https://arxiv.org/abs/2308.12950)

---

## DeepSeek Coder
[DeepSeek Coder](https://github.com/deepseek-ai/DeepSeek-Coder) 是由 **DeepSeek(深度求索)** 這間中國公司所開發的系列模型，其模型架構基於Llama，包含1.3B、6.7B與33B 三種參數量。
* 訓練資料量達 2T Tokens，包含87%的程式碼、10%的英文自然語言與3%的中文自然語言。
* 因此這個模型的中文能力相對好，很適合用來處理包含中文的程式碼。
* 在開源的 Code LLMs 裡面，DeepSeek Coder 的效果也是相當頂尖。

---

## 連結
* [arXiv Paper: Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)
* [HF: Code Llama Usage](https://huggingface.co/blog/codellama)

## 結論
* 雖然專精程式碼的語言模型不如一般用途的語言模型那麼多，但筆者介紹的這些模型依然也只是冰山一角。
* 這些開源的 Code LLMs 提供給開發者更多本地且輕量部署的選擇。
* 模型的用途也從原本的程式碼接龍、程式碼填空一路走向對話形式，未來甚至還有機會朝著多模態(Multimodal)邁進，成為伴隨開發者更有力的助手!

---