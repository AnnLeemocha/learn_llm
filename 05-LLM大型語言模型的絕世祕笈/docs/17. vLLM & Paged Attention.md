# vLLM & Paged Attention
vLLM 是來自 UC Berkeley 的 Woosuk Kwon 和 Zhuohan Li 所製作的推論框架，使用 Paged Attention 技術實現相當驚人的 Token 吞吐量。

---

## Key Value Cache
在 Transformer Decoder 語言模型進行推論時，每一次的推論都會產生一組 KV Cache，這些 KV Cache 代表注意力運算底下**已經計算過的 Key 與 Values**，前面丟進模型算過的輸入，其 Key 與 Values 在後續的生成中不會改變，因此將其另外存放下來，在後續生成時可以重複利用減少計算量，因此會一直被保留在 GPU 記憶體裡面。
* 然而 KV Cache 累積下來所消耗的記憶體非常龐大，更糟糕的是如果要做批次推論，這個 KV Cache 佔用的記憶體就會翻倍，加上推論過程帶來的記憶體破碎問題，導致模型即便在高級硬體的環境底下，也難以同時進行大量文本生成。
* 因此對 LLM Service 而言，**最大的瓶頸是 GPU 記憶體**。

---

## Paged Attention
**Paged Attention** 這個技術發想自作業系統的**分頁記憶體 (Paged Memory)**， 原本的 KV Cache 是很長很大的一坨記憶體，Paged Attention 將這坨 KV Cache 切成一個一個的**區塊 (Blocks)**，並以此建立**區塊對照表 (Block Table)**。對每個序列而言，看到的會是**邏輯區塊 (Logical Blocks)**，模型可以透過區塊對照表來查詢 KV Cache 的**實體區塊 (Physical Blocks)** 放在哪裡。
* vLLM 基本原理： (完整動畫請參考[此連結](https://tinyurl.com/llm-gif-vllm))（原版圖源：[vLLM](https://vllm.ai/)）
    ![vLLM 基本原理](https://tinyurl.com/llm-gif-vllm)
* 這樣做的好處在於，長序列的 KV Cache 不再需要使用很長一段連續記憶體來表示，因此可以被分散的存放在裝置上，解決記憶體破碎的問題。
* 而且系統是透過查詢區塊表的方式來取得 KV Cache，所以很自然的也獲得了共享記憶體的能力！
* 記憶體的使用效率大大提昇，減少了許多記憶體的浪費。因此可以說 **Paged Attention 透過查表法的方式解決記憶體瓶頸**。
* 兩個序列共享提示： (完整動畫請參考[此連結](https://tinyurl.com/llm-gif-vllm-shared))（原版圖源：[vLLM](https://vllm.ai/)）
    ![vLLM 基本原理](https://tinyurl.com/llm-gif-vllm-shared)
    * 從上圖可見序列 A 與序列 B 的第一個區塊共享了同一塊記憶體。
    * 因此當兩個序列的某個部份是重複的時候，便能透過共享記憶體的方式減少記憶體用量。
    * 當兩個序列接下來的生成內容產生分歧時，便會透過**寫入時複製 (Copy-on-Write)** 的技巧來建立新的區塊以供後續使用。
* [官方部落格](https://vllm.ai/)提供的 GIF 動畫解釋的相當詳細，但如果跟筆者一樣腦速緩慢的朋友，可以考慮用 [ezgif](https://ezgif.com/split) 將 GIF 分解成逐格影像慢慢看。
* 關於更多詳細的技術理論，可以參考[官網](https://vllm.ai/)及[論文](https://arxiv.org/abs/2309.06180)的說明。

---

## 基本用法
1. 首先安裝 vLLM 套件：
    ```bash
    pip install vllm
    ```
2. 基本的 Offline Inference 用法如下：
    ```python
    from vllm import LLM, SamplingParams

    llm = LLM(model="meta-llama/Meta-Liama-3-8B")

    # llm = LLM(
    #     model="casperhansen/llama-3-8b-instruct-awq",
    #     quantization="awq",
    # )   # 使用AWQ量化

    prompts = ["Hello, ", "Hi, ", "Goodbye, "]
    sampling_params = SamplingParams(temperature=0.1)
    outputs = llm.generate(prompts, sampling_params)

    for output in outputs:
        prompt = output.prompt
        generate = output.outputs[0].text
        print (f"Prompt: (prompt!r), Generate: (generate!r)")

    ```
    * 用法相當簡單也相當直觀，多數的生成控制參數都放在 [`SamplingParams`](https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py) 裡面，與之前介紹過的取樣參數大同小異，可以參考[官方文件](https://docs.vllm.ai/en/stable/api/vllm/index.html?h=samplingparams#vllm.SamplingParams)的說明。
* 如果有很多提示需要處理，例如要對客戶的大量文件進行摘要或分析之類的，只要把提示全部放在一個字串列表裡面，vLLM 就會自動調整批次大小來快速處理，不用自己慢慢算 Token 數切批次，相當方便。
* vLLM 支援的量化選項較少，僅有 AWQ、GPTQ、SqueezeLLM 與 Hopper 架構獨享的 FP8 等，大多都是需要進行 Post Training Optimization 的方法，像 BitsAndBytes 這類能直接量化的選擇比較少，也許未來會支援，但現階段來說是稍微麻煩一點。
* 雖然我們不能用 FP8 量化模型權重，但我們能用 vLLM 把 KV Cache 量化成 FP8！
    * 在初始化時加上 `kv_cache_dtype="fp8"` 參數就好了，使用 FP8 KV Cache 可以顯著減少記憶體的使用量，但可能會產生一些精度上的損失。
    * 如果使用的是更先進的硬體架構，可以從 FP8 上得到更多加速的效果，可惜筆者的 RTX 3090 沒有。
* 補充：
    * NVIDIA 定義的 FP8 有兩種，分別是 E4M3 與 E5M2，代表他們指數與尾數的位元大小不同，E4M3 可以表達較精準的小數位數，但可以表示的最大與最小數值範圍較小，因此比較容易溢位，E5M2 則反之。
    * 浮點數格式比較：
    ![浮點數格式比較](images/浮點數格式比較.png)

---

## LLM Service
在 vLLM 裡面，整合了 [**Continuous Batching**](https://tinyurl.com/llm-cont-batch) 的機制，前一個輸入推論到一半時，可以中間插入一個新的輸入一起做推論。如果有多的輸入被放在佇列裡面，也會在任何序列完成時立刻加入生成的行列。
* Continuous Batching 示意圖：

    ![Continuous Batching 示意圖](images/Continuous%20Batching%20示意圖.png)

    * 如圖所示，可以看到序列 C 比較早完成，所以序列 E 的輸入就跟著接在後面，整個批次便能繼續進行生成，減少序列 E 的等待時間。
* 這個機制內建在 vLLM 提供的 API Server 裡面：
    ```bash
    python -m vllm.entrypoints.api_server \
        --model meta-llama/Meta-Llama-3-8B-Instruct
    ```
    * vLLM 不像 TGI 一樣有很多 Token 數量的設定，系統會自動根據當下的狀況來調整。
* 把服務跑起來之後，可以 POST 到 `/generate` 進行生成，主要有 `prompt` 跟 `stream` 以及其他 `SamplingParams` 包含的參數，可以透過以下指令進行測試：
    ```bash
    curl -X POST http://localhost:8000/generate \
        -d '{"prompt": "hello, "}'
    # Output: {"text":["hello, 1 bathroom toilet..."]}
    ```
* 使用串流接收 vLLM 的輸出稍微複雜一點，要使用 `b"\0"` 當作分隔符號：
    ```python
    url = "http://localhost:8000/generate"
    params = {"prompt": "Hello, ", "stream": True, "max_tokens": 256}
    resp = requests.post(url, json=params, stream=True)

    prev = ""
    for chunk in resp.iter_lines(delimiter=b"\0"):
        if not chunk:
            continue
        text: str = json.loads(chunk)["text"][0]

        # 已經顯示過的字串不用再顯示一次
        text = text.replace(prev, "")
        print(end=text, flush=True)
        prev += text

    print()
    ```
* vLLM 決定輸出停止點的方法也相當簡單，只需要給stop 參數一份字串列表即可。最後加上一些取樣參數，完整的程式碼大致如下：
    ```python
    import json
    import requests

    params = {
        "prompt": "Hello, ",
        "max_tokens": 128,
        "temperature": 0.75,
        "top_k": 50,
        "top_p": 0.95,
        "stop": ["\n", "\n\n"],
        "stream": True,
    }
    
    url="http://localhost:8000/generate"
    resp = requests.post(url, json=params, stream=True)

    prev = ""
    for chunk in resp.iter_lines(delimiter=b"\0"):
        if not chunk:
            continue

        text: str = json.loads(chunk)["text"][0]
        text = text.replace(prev, "")
        print(end=text, flush=True)
        prev += text

    print()
    ```
* vLLM 同樣提供與 OpenAI API 相容的服務：
    ```bash
    python-m vllm.entrypoints.openai.api_server \
        --model meta-llama/Meta-Llama-3-8B-Instruct \
        --api-key auth-token-ouo123
    ```

---

## 速度測試
最後與 llama.cpp 和 TGI 來一個綜合大比較：
| 方法        | 1T            | 1T           | 16T           | 16T          |
| ----------- | ------------- | ------------ | ------------- | ------------ |
| 方法        | Prefill (t/s) | Decode (t/s) | Prefill (t/s) | Decode (t/s) |
| llama.cpp   | 4900          | 49           | 4000          | 473          |
| TGI         | 4000          | 49           | 4200          | 686          |
| vLLM        | 4400          | 49           | 4600          | 650          |
| vLLM FP8-KV | 4400          | 49           | 4600          | 650          |
* 在單筆推論上，其實大家的速度都是差不多的！
* 但來到多筆推論的狀況下，vLLM 能夠負荷的吞吐量又更大了一些，雖然 Decode 速度小輸 TGI，但都是非常快的速度了。
* 而 FP8 KV Cache 在筆者的 RTX 3090 上與一般的 KV Cache 並沒有太顯著的差異。
* [詳細code](https://ithelp.ithome.com.tw/articles/10331673)

### 壓力測試
* [詳細code](https://ithelp.ithome.com.tw/articles/10331673)

---

## 連結
* [vLLM](https://vllm.ai/)
* [vLLM Docs](https://vllm.readthedocs.io/en/latest/)
* [GitHub: vLLM](https://github.com/vllm-project/vllm)
* [arXiv Paper: vLLM](https://arxiv.org/abs/2309.06180)
* [GitHub: TGI Paged Attention](https://github.com/huggingface/text-generation-inference/blob/main/docs/source/conceptual/paged_attention.md)
* [Anyscale: Continuous Batching LLM Inference](https://www.anyscale.com/blog/continuous-batching-llm-inference)

---

## 結論
* 最大的硬傷是對量化的支援還不是很廣泛，目前只支援 [AWQ](https://github.com/mit-han-lab/llm-awq) 量化，多數情況我們都只能以 FP16 進行推論，因此單張 24GB 顯卡完全無法操作 13B 參數量以上的模型。雖然量化需求已經被開發團隊納入 Roadmap 裡面，但是這 Issue 從六月底高懸到現在都還沒完成。(現未知)
* 但如果你有多顯卡，那 vLLM 鐵定是個超棒的選擇！請參考[分散式推論](https://vllm.readthedocs.io/en/latest/serving/distributed_serving.html)的文件。
* 使用 FP16 部署模型的成本實在太高了！雖然 vLLM 的 Token 吞吐量實在高的驚人，但是 No Quantization No Life，想要部署大一點的模型勢必需要更多顯卡，或者尋求其他框架。這時，又是那張神秘笑臉出手了！來自 Hugging Face 團隊開發的 Text Generation Inference 套件，支援既有的量化技術，並率先整合了 Paged Attention 機制，拯救了大眾蒼生的 GPU 記憶體與荷包。
* vLLM 是本書介紹的最後一個推論框架，所以來進行一個優缺點綜合大比較吧：
    1. HF Transformers：
        * 優點：著重在訓練模型方面，模型支援度最廣泛。
        * 缺點：推論效率雖然已經進步許多，但難以克服記憶體用量問題。
    2. Ilama.cpp：
        * 優點：量化選擇最多，模型讀取速度最快，單筆推論速度快、C & C++ 實做容易整合到其他作業系統與平台。
        * 缺點：專案編譯有些技術門檻，多筆推論同時進行的效率略差。
    3. TGI：
        * 優點：支援主流量化方法，多筆推論速度快。
        * 缺點：需要手動調整 Token 設定。
    4. vLLM：
        * 優點：離線多筆批次推論效率最佳，能自動調整 Token 數量設定，支援 FP8 KV Cache 減少記憶體用量。
        * 缺點：支援的量化方法偏少。
    * 懶人包：
        1. 單機單筆推論或者跨平台，選 llama.cpp。
        2. 線上多筆推論，需要快速量化用 TGI，不用量化用 vLLM。
        3. 離線大量多筆推論，選 vLLM。
    * 希望以上資訊能幫助大家選擇合適的推論框架！

---