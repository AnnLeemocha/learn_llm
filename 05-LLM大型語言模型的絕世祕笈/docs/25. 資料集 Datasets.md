# 資料集 Datasets
要訓練一個模型，首先要有資料，不僅要有很**大量**的資料，也要有**品質**很好的資料。
* 資料的品質包含：
    * 文句是否通順
    * 格式是否合理
    * 內容是否偏頗
    * 資訊是否有害等等。
* 品質越好的資料，越有機會訓練出更好的模型。
    > Data quality determines model ability. 資料品質決定模型能力。

---

## Hugging Face Datasets
由 Hugging Face 開發的 [Datasets](https://github.com/huggingface/datasets) 套件，除了可以讀寫資料集以外，也能用來下載 HF Hub 上的公開資料集。
* 可以到資料集的[搜尋頁面](https://huggingface.co/datasets)瀏覽有哪些資料集可以用。
    * 左邊的過濾器提供各種選項，包含資料集的任務、子任務、資料集規模或語言等等。
* 如果遇到超大型資料集，但是不想要整份下載下來的話，可以使用套件的串流功能來取樣幾筆資料看看：
    ```python
    import json
    from datasets import load_dataset

    ds = load_dataset(
        "togethercomputer/RedPajama-Data-1T",
        "arxiv",  # 選擇 axXiv 子集
        cache_dir="cache",  # 方便清理
        streaming=True,  # 啟用此選項，避免整份資料集被下載到硬碟裡面
    )["train"]

    # 只取前 10 筆出來看看
    small_ds = [data for _, data in zip(range(10), ds)]

    # 內容通常很長，所以寫進檔案裡面慢慢看
    with open("small.json", "wt", encoding="UTF-8") as fp:
        json.dump(small_ds, fp, ensure_ascii=False, indent=4)
    ```
* 每種資料集都有自己的讀取方式，例如 RedPajama 就需要給子資料集的選項，但其實也有滿多資料集是沒有這個選項的，因此讀取前請先參考該資料集的介紹頁面。
* 在使用 Python 處理資料時，經常會使用 JSON 格式，但 JSON 格式是個沒有對資料進行壓縮的格式，當資料數量很龐大的時候，存放在硬碟上會佔用很巨大的空間。
* 因此有些人會選擇做 Gzip 壓縮，雖然壓縮率很高，但是壓縮與解壓縮的速度非常慢。
* [**Apache Parquet**](https://parquet.apache.org/) 是個專為機器學習任務所設計的檔案格式，Parquet 具有相當程度的資料壓縮率，但是壓縮速度比 Gzip 快的多，因此是個用來存放大型資料的好幫手。
* 想要使用 Parquet 格式，可以透過 Pandas 套件操作：
    ```python
    from datasets import load-dataser
    import pandas as pd

    ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    text = [{"text": data["text"]} for data in ds]
    text = pd.DataFrame(text)

    text.to_parquet("wikitext.parquet") # 儲存資料集
    pd.read_parquet("wikitext.parquet") # 讀取資料集
    ```
* 可以來比較一下 JSON、Gzip 與 Parquet 之間讀寫速度的差異，首先寫一個 `Timer` 類別協助我們測量時間：
```python
import time

class Timer:
    def __init__(self, title):
        self.title = title:
            
    def __enter__ (self):
        self.delta = time.perf_counter()
    def __exit__(self, *_):
        delta = time.perf_counter() - self.delta
        print(f"{self.title}: {delta:.4f}s")
```
* 接著測量各種格式的讀寫速度：
    ```python
    print("=== 寫入速度 ===")
    with Timer("JSON"): text.to_json("text.json")
    with Timer("Grip"is text.to_json("text.gz", compression="gzip")
    with Timer("Parquet"): text.to_parquet("text.parquet")

    print("=== 讀取速度 ===")
    with Timer("JSON"): pd.read_jaon("text.json")
    with Timer("Gzip"): pd.read_json("text.gz")
    with Timer("Parquet"): pd.read_parquet("text.parquet")
    ```
    * 得到以下結果：
    ```md
    === 寫入速度 ===
       JSON: 0.0479s
       Gzip: 0.5839s
    Parquet: 0.0344s

    === 讀取速度 ===
       JSON: 0.0288s
       Gzip: 0.0616s
    Parquet: 0.0340s
    ```
    * 可以看到 Gzip 真的慢非常多，而 Parquet 幾乎跟 JSON 差不多快！
* 那這些格式的文本壓縮率如何呢？
    ```md
    text.json     11.0 MB
    text.gz        3.9 MB
    text.parquet   6.0 MB
    ```
    * 可以看到 Gzip 是壓縮最多的，但 Parquet 可以在幾乎無損的讀寫速度內，達到一定程度的壓縮率，因此當資料規模逐漸增加的時候，Parquet 是個節省硬碟空間又不會損失讀寫速度的好選擇。
* 然而 Parquet 與其他格式的讀取邏輯相當不同，一般的 JSON 是 Row-Oriented，也就是一筆資料讀完接著下一筆，但 Parquet 是 Column-Oriented，這是什麼意思呢？讓我用一小段程式碼來解釋這個概念：
    ```python
    data[i]["text"]  # Row-Oriented 是這樣讀取
    data ["text"][i] # Column-Oriented 卻是這樣讀取
    ```
    * Row-Oriented 的資料，讀取時需要先指定第幾筆，再指定要讀取該筆資料的哪個欄位。
    * 而Column-Oriented 則是先指定要讀取的欄位，再決定要讀取第幾筆資料。
    * 這樣的讀取邏輯其實更適合機器學習的資料集，因為模型不一定會用到每筆資料的所有特徵，所以實驗時只需要讀取會用到的特徵即可。
* 因此，若是要迭代 Parquet 資料集，可能會像是這樣：
    ```python
    text = pd.read_parquet("dataset.parquet")
    for i in range (len(text)):
        text["id"][i]
        text["title"][i]
        text["article"][i]
    ```
    * 雖然稍微不直觀一點，但掌握好核心概念，操作起來還是很簡單的喔！

---

## 預訓練資料集

### Common Crawl
* [**Common Crawl**](https://commoncrawl.org/) 是一個大型公開的網路爬蟲專案，它會把網路上的海量資料給爬下來，並提供該網頁的文本內容，許多預訓練資料集都是以 Common Crawl(CC) 為基礎建立的。

### Project Gutenberg
* [**Project Gutenberg**](https://www.gutenberg.org/) 谷騰堡計畫是從 1971 年開始進行的電子書計畫，目標是提供免費的電子書給大眾閱讀。
* 此計畫的名稱是用來紀念谷騰堡，一位發明活字印刷術並推廣印刷機的人。

### The Pile
* [**The Pile**](https://pile.eleuther.ai/) 是由 Eleuther AI 提供的資料集，其主成份之一 Pile-CC 就是基於 Common Crawl 的資料集。
* 除此之外，還有很多其他的資料，例如書籍資料 Books3、論文 arXiv、維基百科、GitHub 程式碼等等。
* 另外 The Pile 也有包含 Project Gutenberg 的資料，常見的 PG-19 就是指 1919 年之前的經典文學書籍。
![預訓練資料集 - The Pile 組成](images/預訓練資料集%20-%20The%20Pile%20組成.png)

### C4 & mC4
* [**C4**](https://huggingface.co/datasets/allenai/c4) 全名為 Colossal Clean Crawled Corpus，由 Google 所提出的資料集，同樣是基於 Common Crawl，並做了更進一步的資料清洗，而 [**mC4**](https://huggingface.co/datasets/mc4) 資料集則是後來 Google 提出的多語言 (Multilingual) 版本。

### Stack Exchange
* [**Stack Exchange**](https://stackexchange.com/) 是工程師好朋友 [**Stack Overflow**](https://stackoverflow.com/) 的系列問答網站，除了程式設計以外還有軟硬體操作、數學、物理、化學、英文等等的領域。
* 而 [Stack Exchange 資料集](https://github.com/EleutherAI/stackexchange-dataset)就是基於這些網站爬取下來的網頁文本內容所建立的。

### RedPajama & SlimPajama
* Pajama 指的是睡衣，而 [**RedPajama**](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2) 並不是紅色睡衣，而是一份由 [**together.ai**](https://together.ai/) 所統整的資料集，這份資料集主要整合了 Meta 訓練 LLAMA 時所使用到的資料集，包含 Common Crawl、C4、GitHub、Books、arXiv、Wikipedia 與 StackExchange 等，總共近 1.2T Tokens 的資料集，方便大家重現 LLAMA 的訓練結果。
* [**SlimPajama**](https://tinyurl.com/llm-slim-pajama) 則是由 [**Cerebras**](https://www.cerebras.net/) 提出的資料集，對 RedPajama 做了更進一步的**去重複 (Deduplicate)** 處理，得到更精簡的 627B Tokens 的資料集。

### The Stack & StarCoderData
* [**The Stack**](https://huggingface.co/datasets/bigcode/the-stack) 是由BigCode 釋出的程式碼相關資料集，包含 358 種程式語言，有將近 6TB 的資料量。
* 另外還有去重複的版本 [**The Stack Dedup**](https://huggingface.co/datasets/bigcode/the-stack-dedup) 和 GitHub Issue 版的 [**The Stack Issues**](https://huggingface.co/datasets/bigcode/the-stack-github-issues) 這兩種資料集。
* [**StarCoderData**](https://huggingface.co/datasets/bigcode/starcoderdata) 則是一份基於 The Stack 的程式碼資料集，保留了其中 86 種較常見的程式語言，並加入 GitHub Issue 與 Jupyter Notebooks 等等，總共約 250B Tokens 的文本資料。
* 後來 BigCode 又推出了 [**The Stack v2**](https://huggingface.co/datasets/bigcode/the-stack-v2)，包含 600 多種程式語言，資料量更是來到驚人的 67 TB 之多。

### 補充 - 1
為什麼去重複這麼重要呢？
* 因為重複性的資料可能導致模型產生偏差，試想如果資料集裡面包含了 100 句「土豆是Potato」，但只有 1 句「土豆在臺灣是 Peanut」，那模型肯定很容易偏頗在前者上。
* 假設這兩種資料各只有一筆的話，模型就可以比較均衡的學習到更具有多樣性的資訊。

### 補充 - 2
* 一般在訓練 Code LLM 時，並不會真的只拿程式碼的資料去做訓練，而是會加入一些自然語言的資料一起做訓練。
* 在一些文獻中，會以 **PL (Programming Language)** 代稱**程式語言**，並使用 **NL (Natural Language)** 代稱**自然語言**。

---

## 對話資料集

### Databricks Dolly
* [**databricks-dolly-15k**](https://huggingface.co/datasets/databricks/databricks-dolly-15k) 是由 [**Databricks**](https://www.databricks.com/) 提出的資料集，包含一萬五千筆對話問答的資料，而且這些資料是由 Databricks 上千名員工**人工手動**寫出來的！
* 是當代相當少見的純人工資料集，可惜只有英文。

### OASST
* [**OASST**](https://huggingface.co/datasets/OpenAssistant/oasst2) 是 [**Open Assistant**](https://open-assistant.io/) 提出的資料集，有十多萬筆資料，涵蓋多種語言，其中就包含中文。
* 但沒有特別說是簡體中文還是繁體中文，所以高機率就是...
* [論文](https://arxiv.org/abs/2304.07327)

### ShareGPT
[**ShareGPT**](https://sharegpt.com/) 在之前的文章有介紹過，是一個用來分享 ChatGPT 對話內容的瀏覽器外掛。
* 後來 Vicuna 團隊在 ShareGPT 的網頁上進行爬蟲獲得的 ShareGPT 資料集。
* 但現在 ShareGPT 已經關閉爬蟲入口了，只剩別人整理好的 [ShareGPT 資料集](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)可以用了。

### LMSYS Chat
* 沒有 ShareGPT 沒關係，由 Vicuna 團隊釋出的 [LMSYS Chat](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) 資料集，共有一百萬組對話資料，是從 [FastChat Demo](https://chat.lmsys.org/) 頁面蒐集而來，因此這些資料都是真實的對話資料。

---

## 中文資料來源

### 政府資料開放平台
* [**政府資料開放平台**](https://data.gov.tw/)提供了很多來自政府提供的資料集，例如前面章節用到的全國路名資料就是來自這裡。
* 對於挖掘臺灣潛在應用而言，是個相當有價值的資料來源。

### 維基文庫
* [**維基文庫**](https://zh.wikisource.org/zh-hant/)收集了許多自由文本的內容，例如一些古詩詞和散文經典等等，像是三國演義、西遊記之類的。
* 這些文本大多沒有版權問題，所以拿來訓練是沒問題的，不過這裡面使用文言文的資料居多，缺少白話文資料。

### 開放文學
* [**開放文學**](http://open-lit.com/)是另外一個收集自由文本的網站，類似維基文庫，雖然文本量可能稍微少一些，但是對爬蟲相對友善，可以直接分析它的 API 呼叫來取得純文本。

### 漢籍全文資料庫
* [**漢籍全文資料庫**](https://hanchi.ihp.sinica.edu.tw/ihp/hanji.htm)是中研院建立的文本資料庫，以經、史、子、集分類，其中史書的資料相對多一點。

### TAIDE
* 國科會的 TAIDE 專案使用了大量的[繁體中文訓練資料](https://taide.tw/index/training-data)，且來源多與臺灣相關，例如縣市政府的旅遊網站、各大新聞媒體或教育部的字典等等。
* 雖然不是每份檔案都有結構化的資料可以下載，但也相當具有參考價值。
* 根據 TAIDE 介紹所說，目前這些資料大約是 40B Tokens 左右，可能不算很多，但至少是個不錯的開始！

---

## 評估資料集
完成模型的開發或微調之後，要如何確定模型有沒有進步呢？
* 這時就會需要透過**基準測試 (Benchmark)** 來評估與比較模型的好壞，因此需要**評估資料集**來做測試，不同的資料集會有不同的評估方法、領域、面向與語言等等。
* 生成式語言模型的評估難度相對高一些，像分類或辨識模型，通常都有標準答案可以計算準確率，但是語言模型通常是生成一段文字，諸如摘要、翻譯或對話等等，似乎都不是有標準答案的任務，那要怎麼評估呢？
* 最簡單的方法就是把題目變成**選擇題**，給模型數個選項，請它選出正確的選項，這種資料集包含 MMLU、CMMLU 與 TMMLU+ 等都是這樣的設計。
* 實際上要如何確認模型會選哪個選項呢？
* 可以透過比較 Loss 的方法來進行：
    ```python
    choices = [
        "A banana is red.",
        "A banana is yellow.",
        "A banana is blue.",
        "A banana is green.",    
    ]

    for text in choices:
        input_ids = tk.encode(text, return_tensors="pt")
        with torch.no_grad():
            outputs = model.forward(input_ids=input_ids, labels=input_ids)
        print(outputs.loss, text)
    ```
    * 這段程式在比較哪句話的 Loss 比較低，最後輸出結果如下：
    ```md
    5.9140 A banana is red.
    4.9455 A banana is yellow.
    5.8184 A banana is blue.
    5.7485 A banana is green.
    ```
    * 「香蕉是黃色的」Loss 最低，所以可以判定模型傾向於選擇這個選項。
    * 但 Loss 低未必代表模型真的會生成這個答案，因此僅能在自動評估上做個參考而已。
* 除了選擇題以外，也能評估答案與生成之間的**文本相似度**，例如 BLEU 或 ROUGE 分數，通常用在問答、摘要或翻譯任務上，例如 WMT 翻譯就是用這種方法評估。
* 最近還很流行**請更強的模型來評分**，例如問 ChatGPT 這篇文章的品質如何？或者這兩篇不同模型生成的文章，誰的品質比較好？
* 在 HF Hub 上可以找到相當多評估資料集來用，但是這麼多的評估資料集，每個資料集的格式不盡相同，評估手法也不一樣，該怎麼辦呢？
* 這時可以透過 Eleuther AI 製作的評估套件 `lm-evaluation-harness` 來幫我們，安裝方法如下：
    ```shell
    git clone https://github.com/EleutherAI/lm-evaluation-harness
    cd lm-evaluation-harness
    pip install -e .
    ```
*  首先查看目前套件支援哪些評估資料集：
    ```shell
    lm-eval --tasks list
    ```
* 以目前繁體中文最常用的評測資料集 TMMLU+ 為例，用法如下：
    ```shell
    export HF_DATASETS_CACHE=cache

    lm-eval --model_args pretrained=meta-llama/Meta-Llama-3-8B \
        --model hf --device cuda:0 --batch_size auto \
        --output_path results --num_fewshot 5 \
        --tasks tmmluplus --limit 0.01
    ```
    * 透過 `--tasks tmmluplus` 參數指定使用 TMMLU+ 進行評測。
    * 設定 `--limit 0.01` 先做少量測試。
    * 通常評估會給少量樣本，因此給 `--num_fewshot 5` 來做 5-Shot 評估。
    * 將批次大小設定為 `auto`，套件會自行判斷可以設定到多大的批次大小，藉此來提昇評估的速度。
* 評估完之後，會在 `results` 資料夾底下長出一份 `.json` 檔，裡面存放評估測試的結果，其中 `acc` 就是準確率，而 `acc_norm` 則做了一些正規化，使不同資料集之間的評估結果可以互相比較。
* 不同的評測也會產生不一樣的指標，可以到[官方 GitHub](https://github.com/EleutherAI/lm-evaluation-harness) 上查看。

---

## 模型生成資料
在沒有太多人力資源的情況下，透過模型來蒐集資料是個不錯的方法，像是 GPT-3.5 API 每生成 1000 Tokens 的資料只需要 0.0015 美金，約新台幣 0.045 塊錢。
* 收集一百萬 Tokens 約莫只需要 60 新台幣而已，其實超便宜的！
* 但是透過模型輸出來蒐集資料，需要注意模型是否存在偏頗的狀況。
* 像是由 Stanford 提出的 [**Alpaca**](https://github.com/tatsu-lab/stanford_alpaca) 專案，裡面就包含了五萬多筆模型生成的資料集。
* Alpaca 是最常見的一種做法，首先設定一些種子任務：
    ```md
    Seed 1: 請把「星爆氣流斬」翻譯成英文
    Seed 2: 請問桐人的著名招式是什麼
    Seed 3: 請介紹刀劍神域這部動畫
    ```
* 然後請 ChatGPT 根據這些種子任務，改寫出更多問句：
    ```md
    Task 1-1: 請把「星爆氣流斬」翻譯成英文
    Task 1-2: 請把「大玉螺旋丸」翻譯成英文
    Task 1-3: 請把「龜派氣功波」翻譯成英文

    Task 2-1: 請問桐人的著名招式是什麼
    Task 2-2: 請問鳴人的著名招式是什麼
    Task 2-3: 請問悟空的著名招式是什麼

    Task 3-1: 請介紹刀劍神域這部動畫
    Task 3-2: 請介紹火影忍者這部動畫
    Task 3-3: 請介紹七龍珠這部動畫
    ```
* 最後再拿著這些問題直接去問 ChatGPT 來得到答案，這樣就能獲得完整的問答資料集了。
* 除了一般的問答以外，有些時候也會設計一些不能回答或答不出來的問題，藉此訓練模型拒絕回答的能力。

---

## 連結
* [arXiv Paper: The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027)
* [arXiv Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
* [arXiv Paper: mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)
* [GitHub: T5](https://github.com/google-research/text-to-text-transfer-transformer#datasets)
* [TF Datasets: C4](https://www.tensorflow.org/datasets/catalog/c4)
* [HF Datasets: The Stack](https://huggingface.co/datasets/bigcode/the-stack)
* [HF Datasets: StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata)

---

## 結論
* 本章節介紹了一些常見的資料集，以及一些中文相關的資料。
* 在繁體中文資料方面，可用於語言模型訓練的資料還是相對稀少一些。
* 作為一個相對資源稀缺的語言使用族群，開發出符合在地語言與文化的資料集需要相當大的努力，希望在這波人工智慧的浪潮下，可以讓大家更加重視這個領域的耕耘。

---