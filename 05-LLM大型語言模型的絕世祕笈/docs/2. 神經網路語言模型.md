# 神經網路語言模型

## 文字向量 (Word Embedding)
一種廣義的語言模型，==目的是為了將離散的文字轉換成實數的向量==，有了向量我們才能使用機器學習模型進行運算，因此可以把 Embedding Layer 視為一種把文字變成數字的過程。
* 建構的方式有很多種
    * 傳統:
        * 有 Word2Vec 的 Skip-Gram 與 CBOW，以及 GloVe 和 fastText 等等。
    * 目前主流
        * 在 Transformer 架構前面加上一個 Embedding Layer 並透過大量文本 Pretraining 訓練而來。
* 本身通常是上下文無關的，這是一個相當重要的特性。
    * 也就是說這個詞的前後是什麼並不重要，他出來的 Embedding 都是固定的。
    * 因此訓練一個 Word Embedding 時，就是在訓練每個 Word 其獨立的語意。
* 文句向量 (Sentence Embedding):
    * 這些獨立語意的向量，透過特定模型（例如 Transformer 之類的）的處理，這些原本離散的語意向量得以融入上下文資訊。
    * 在資訊檢索領域相當有用，也是支撐現在 Knowledge-Based 生成式 IR 相當核心的技術之一。

---

## 遞歸神經網路 (Recurrent Neural Network, RNN)
一種典型==序列模型==的代稱，被認為是用來==處理具有時間相依性資料==相當有效的模型，例如股市或氣象等領域。
* 在深度學習剛開始盛行的時候，也很常見到使用 RNN 處理 NLP (自然語言處理)領域的任務，將每個字詞視為一個時間單位來處理。
* 常見 RNN 架構: 有 LSTM, GRU 等。
    * 通常會依序拜訪時間序列上的每個元素，並在每個時間點決定要「**記憶**」或「**遺忘**」某些前面的資訊，並不斷往下累積下去。
    * 但如此一來，前面的元素顯然參照不到什麼過往的資訊。
    * 透過**雙向 (Bidirectional)** 的技巧，不只從頭到尾拜訪一次，也從尾到頭拜訪一次，使模型對整體資料都有充足的理解。
* 缺點: 
    * 容易梯度爆炸或消失，因此要將 RNN 疊到很多層，其難度是很高的。
    * 雖然 LSTM 與 GRU 能夠減緩梯度的問題，但因為時間相依的特性，導致他很難進行平行運算，因此在訓練效能上有很大的瓶頸。
    * 尤其是 Transformer 模型出現之後，在 NLP 領域就越來越少見到 RNN 相關的研究了。
* RNN + Transformer:
    * 這種架構保留了兩者的優點，例如更好的上下文理解能力與無限的文本長度等，也減少了傳統 Transformer 帶來的超膨脹記憶體消耗。
    * 模型: [RWKV](https://github.com/BlinkDL/RWKV-LM)、[Mamba](https://arxiv.org/abs/2312.00752)、[Jamba](https://arxiv.org/abs/2403.19887)

---

## Transformer
現在語言模型的主流架構，筆者喜歡將 Transformer 比喻為把 Linear Layers 疊的很華麗的模型。
* 重要==注意力機制 (Attention)==
    * 本質上就是把好幾個 Linear Layer 拿來互相運算。
    * 可以視為一種比較華麗的查表法，在 Transformer 裡面，所有的元素就是在瘋狂互相查表。
    * 比喻成大家所熟悉的 Python 字典 (dict) 類別，我們將資訊存成鍵值對應 (Key Value Pairs) 來建構字典，然後在字典裡面根據查詢 (Query) 尋找一樣的 Key 並得到對應的 Value。
        | Attention                                                      | Python                        |
        | -------------------------------------------------------------- | ----------------------------- |
        | Query 與 Key 是否相符，是透過模型訓練學得                      | 須要 Query 與 Key 完全相符合  |
        | 一個 Query 會找到一堆 Value，而且這些 Value 各自會有各自的權重。 | 一個 Query 只會找到一個 Value |
        * 這些權重分佈代表的是這個 Query 需要放多少注意力在這個 Value 上面。
        * 這些 Value 的目的，其實就是為了理解上下文對語境的影響。
* 本身其實並非真的是單純的線性模型。
* 實際上其運作原理還是涉及了很多複雜的機制
    * 例如不同的激勵函數 (Activation)、正規化 (Normalization)、位置向量 (Positional Embedding) 等等。
    * 這些機制不僅考量了應用階段的可用性，也考量了訓練階段的可行性。[進一步了解，推薦研究論文連結](https://arxiv.org/abs/1706.03762)
* 缺點: 需要克服模型輸入長度與膨脹的記憶體消耗。
    * 在文字領域上達到很好的效果，但其架構也帶來相當巨量的 GPU 記憶體消耗。除了會與批次大小 (Batch Size) 成線性成長以外，因為 Attention 會算出一個二維注意力權重的關係，所以隨著模型輸入長度越長，其記憶體消耗會呈現平方性的成長。

---

## Transformer LM 架構種類
粗淺的分為以下三類：
1. 純編碼模型 (Encoder-Only, AutoEncoder)
1. 解編碼模混合型 (Encoder-Decoder)
1. 純解碼模型 (Decoder-Only, AutoRegressive)

### Encoder LM
* ==專注於分類型任務== (分類專家)
    * 例如情感分類、意圖偵測或實體辨識等等。
* 在許多自然語言理解任務上，這類模型都有相當不錯的效果。
* 建模方法:
    * Masked Language Modeling (MLM)
        * 使用雙向 Transformer 的架構，能夠對上下文進行充分的理解，然後進行分類等。
        * 首先隨機遮蔽 (Mask) 文本中的文字，並要求模型預測該文字為何。
        * 可以達到非監督式訓練的效果。只要寫個程式隨機遮蔽幾個字就好，完全不需要人工標記，也能讓模型學出相當理想的語言資訊。
    * Next Sentence Prediction (NSP) 
    * Sentence Order Prediction (SOP)
    * ... 進一步了解，推薦研究從 [BERT](https://arxiv.org/abs/1810.04805) 或 [ALBERT](https://arxiv.org/abs/1909.11942) 的論文連開始

### Encoder-Decoder LM
* 一種==序列到序列== (Seq2Seq) 的模型架構，利用了 Encoder 能夠充分理解上下文的優點，結合擅長文本生成的 Decoder 形成的架構。
* 常用於機器翻譯、自動摘要等任務上。
* 著名的模型:
    * Google 的 [T5](https://arxiv.org/abs/1910.10683)
    * Facebook 的 [BART](https://arxiv.org/abs/1910.13461)

### Decoder LM
* 一種==專注於文本生成==的語言模型架構。 (文字接龍專家)
* 著名的模型:
    * ChatGPT
    * [Llama](https://arxiv.org/abs/2307.09288)
* 建模方式:
    * Causal Language Modeling (CLM) 因果建模
        * 透過不斷要求模型去預測下一個字詞是什麼來建模。(Next Token Prediction 建模法)
        * 這也是一種非監督式學習，僅需要大量文本即可訓練。

---

## 參考
* [Wikipedia: Word Embedding](https://w.wiki/7UD2)
* [Wikipedia: Recurrent Neural Network](https://w.wiki/3sAW)
* [Wikipedia: Transformer](https://w.wiki/7MW8)

---

## 結論
* 以上是神經網路語言模型的一個概覽。
* 從傳統的 Word Embedding 和 RNN 模型，到現在百花齊放的 Transformer 模型，不同的模型與架構都有其適用的場景。
* 因為本系列文主題為 LLM 大型語言模型，所以未來的章節主要著重在 Decoder LM 上面。


---

