# 量化
量化 (Quantization) 是我們這些平民 LLM 玩家最好的夥伴，一般模型在訓練時多使用 32-Bit 或 16-Bit 的浮點數，即便是 7B 16-Bit 的模型，也要消耗掉 13 GiB 以上的 GPU 記憶體，更不用說是 13B 以上的模型，一般單張 24GB 消費級顯卡根本放不下。
* 這個技術可以將模型轉換成 8-Bit 或 4-Bit 甚至更低，大幅減少模型佔用的 GPU 記憶體，使我們能夠操作 13B 甚至 30B 以上的模型。

---

## 基本概念
「**量化 (Quantization)**」一詞有多個用法，其中一個是**把抽象的概念給「數字化」**。

* 舉例來說：
    > @penut：你今天有多努力？
    > @me：我今天寫了 30 行程式碼我超認真
    * 「多努力」就是一種抽象的概念，而我們用 30 行程式碼這種具體的數字與單位，來量化一個人今天的工作量。
    * 但是這樣的量化方法，會忽略掉許多細節。
    * 也許對方今天做了很多寫程式以外的事情，而在這個量化方法下都被無視了，因此產生了誤差。
* 所以選擇一個能夠減少誤差的量化方法，是這個領域最關注的重點。

### 簡單推論
在機器學習領域裡面，量化則被用來進行**值域的轉換**。
* 例如，將 16 位元的浮點數 (FP16) 量化成 8 位元的整數 (INT8)，這種將高位元的資料轉換為低位元的過程。
1. 量化技術的初始概念相當單純，假設我們有以下 FP16 數列：
    ```python
    [1.2, 0.3, -0.1, -0.8]
    ```
    * 觀察這個數列，他的值域介在 `[-0.8, 1.2]` 之間。若我們要用 INT8 來表示，那我們需要將這個值域對應到 `[-128, 127]` 之間。
    * 最簡單的做法就是透過**正規化 (Normalization)** 將這些數值正規化到那個值域裡面。
    * 正規化是機器學習裡面相當重要的技巧，透過將輸入正規化到 `[-1.0, 1.0]` 之間來減少偏移量，使模型更容易收斂。
2. 若將正規化的作法套到量化上，首先將整組數列減去**最小值** `-0.8` 得到：
    ```python
    [2.0, 1.1, 0.7, 0.0]
    ```
3. 接著計算原本值域的**間距** `(1.2 - -0.8) = 2.0` 並將新的數列除以間距：
    ```python
    [1.0, 0.55, 0.35, 0.0]
    ```
4. 最後縮放到 INT8 的值域 `x * 255 - 128` 並四捨五入：
    ```python
    [127, 12, -39, -128]
    ```
    * 完成 FP16 到 INT8 的**量化 (Quantize)** 動作。
    * 在存放這組數列時，最小值(-0.8)與間距(2.0)也要一起被存下來，可以反向回推原本的資料型態。
5. 實際進行矩庫運算時，通常會先將數列**反向量化 (Dequantize)** 回來，也就是按照量化的流程反向計算回去：
    ```python
    (x + 128) / 255 * 2.0 + (-0.8)
    ```
    反向量化後的數列如下:
    ```python
    [1.2, 0.298, -0.102, -0.8]
    ```
    * 可以看到反量化後的數列並沒有被精準的還原，數值內容是有誤差的。
        * 因此量化技術探討的不僅只是將數值壓縮至低位元，更重要的目標在於如何減少量化所產生的誤差。
    * 最小值與間距通常會用原本的資料型態存放。
    * 除了精準度的考量以外，也要考慮極值的問題，避免間距或最小值因為過大過小而發生溢位的問題。
    * 但也因此需要花費額外的空間來存放最小值與間距。
        * 以上例而言，四個數值省下了 `8 * 4 = 32` 個位元，但是因為額外存了兩個 FP16 的資訊，所以兩者就抵銷了。
        * 雖然一般而言不會只拿著四個數值就在那邊做量化，但還是可以使用其他方法來進一步減少額外資訊。
6. 例如我們可以先對數列**取絕對值**再做量化。
    * 這樣的好處在於最小值必定為零，以上例而言，數列的值域就被縮減到 `[0.0, 1.2]` 之間。
    * 按照正規化的概念，應該先將數列除以 `1.2` 來正規化到 `[-1.0, 1.0]` 之間，並乘上 `127` 來得到 INT8 數列：
    ```python
    (x / 1.2) * 127
    ```
    * 透過一些簡單的數學原理，可以將算式整理如下:
    ```python
    (x / 1.2) * 127
      = x * (1 / 1.2) * 127  # 改寫為乘法
      = x * 127 * (1 / 1.2)  # 根據交換律
      = x * (127 / 1.2)      # 根據結合律
    ```
    * 這個 `127 / 1.2` 就是所謂的**量化常數 (Quantization Constant)**，以此例而言為 `127 / 1.2 = 105.83` ，這也是這個量化方法裡面唯一一個額外資訊。
7. 將此量化常數與原本的數列相乘並四捨五入，得到以下數列：
    ```python
    [127, 32, -11, -85]
    ```
8.  而反向量化也只要除以量化常數即可：
    ```python
    [1.2, 0.302, -0.104, -0.803]
    ```
    * 這樣的做法誤差會稍大一些，但是實做上也比較方便。
    * 這種做法也稱為**絕對最大值量化(AbsMax Quantization)**，需要計算最小值與間距的方法則被稱為**最大最小值量化(MinMax Quantization)**，兩者都是概念最單純的量化方法。
    * 依照這個思維，其實也能用相同的方法達成 INT4 量化，但是誤差就更大了! 所以一般 8-Bit 以下的量化並不是單純的線性量化而已。
    * 補充：
        * 像絕對最大值量化這類**以零為中心點**的量化也被稱為**對稱量化(Symmetric Quantization)**，反之則為**非對稱量化(Asymmetric Quantization)**。
        * 這兩種量化各有優缺點，在不同情況下也有各自的最佳化場景。
        ![對稱量化與非對稱量化的差異](https://hackmd.io/_uploads/ry6kBa0Wge.png)
* 透過 Python 簡單實做這種量化方法：
    ```python
    def quantize_fp16_to_int8(fp16_weights):
        # 找到權重的絕對最大值
        abs_max_val = np.max(np.abs(fp16_weights))

        # 計算縮放係數 Scaling Factor
        scale = 127 / abs_max_val

        # 將 FP16 權重轉換為 INT8
        weights = fp16_weights * scale
        int8_weights = np.round(weights).astype(np.int8)

        return int8_weights, scale


    def dequantize_int8_to_fp16(int8_weights, scale):
        # 將 INT8 權重轉換回 FP16
        return int8_weights.astype(np.float16) / scale
    ```
    * 可以參考以下筆者的 Colab 程式碼觀察 INT8 的實際效果如何，這份 Colab 裡面同時也示範了如果直接將這個方法套用到 INT4 上誤差會有多大。
* 📝 範例程式碼
    * [筆者程式碼](https://tinyurl.com/llm-note-09)
    * 自己嘗試: .../LLM/project/quantization

---

## 量化方法
量化的方法主要分為**訓練時量化 (Quantization-Aware Training, QAT)** 與**訓練後量化 (Post-Training Quantization, PTQ)**。
* 訓練時量化需要在模型一開始進行訓練時，就把量化的誤差考慮進去，通常會透過 [Fake Quantize](https://tinyurl.com/lim-fake-quant) 來達成。
    * 但是 QAT 實在太慢了!
* 因此多數用在語言模型上的量化方法都是訓練完成之後才進行量化的，此章節接下來要介紹的方法也都是 PTQ 的方法!

### BitsAndBytes
[BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) 簡稱 BNB，是由 [Tim Dettmers](https://github.com/TimDettmers) 開發的專案，在 HF Transformers 套件裡面做 8-Bit 或 4-Bit 量化的後端技術之一。
* 8-Bit 的做法與上節所述大致相同，但是 BNB 更著重在**隔離異常值 (Isolate Outlier)** 上
    * 因為作者觀察到如果將一些數值過大或過小的異常值也量化掉，會造成很大的誤差，但如果將少量的異常值隔離出來，維持他們原本的精度，便能大幅減少誤差值。
1. 首先安裝套件：
    ```bash
    pip install bitsandbytes
    ```
2. 透過 `BitsAndBytesConfig` 來將模型轉成 BNB 8-Bit 的格式：
    ```python
    from transformers import BitsAndBytesConfig
    from transformers import AutoModelForCausalLM

    bnb_config = BitsAndBytesConfig(load_in_8b=True)
    model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Meta-Llama-3-8B",
        quantization_config=bnb_config
    )
    ```
    * 也可以將 8-Bit 模型透過 `model.save_pretrained` 存下來，這樣下次再讀取就會直接是 8-Bit 的型態，是個節省硬碟空間的手段。
    * 但是通常不太建議這樣做，因為變成 8-Bit 格式之後，就不能再轉回 16-Bit 或者降成 4-Bit 之類的，而且也會強制讀進 GPU 而不能在 CPU 裡使用，因此保留原始權重為 16-Bit 的格式是比較彈性的選擇。
* 除了 8-Bit 以外，BNB 也有提供 4-Bit 格式。BNB 4-Bit 所使用的資料型態為 Normal Float 4 (NF4)，是 [QLoRA](https://arxiv.org/abs/2305.14314) 論文裡面提出的資料型態。
    * 要使用 BNB 4-Bit 的話，只需要把 `BitsAndBytesConfig` 裡的 `load_in_8bit` 改成 `load_in_4bit` 就可以了。
    * 除此之外還有一些其他參數可以設定：
        ```python
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        ```
    * 被量化的模型在實際進行矩陣運算時，是會被反向量化回浮點數的，預設是用 FP32 進行運算，也可以透過 `bnb_4bit_compute_dtype` 參數將其設定為 FP16 會算的比較快。
    * 預設 4-Bit 的資料型態是一般的 FP4，將 `bnb_4bit_quant_type` 設定為 NF4 就可以使用論文裡面提到的資料型態。
    * 最後如果開啟 `bnb_4bit_use_double_quant` 可以進一步減少模型的大小。
* 將模型壓縮到 4-Bit 不僅能讓我們順利在單張 24 GB 消費級顯卡上使用 30B 的模型進行推論，也可以結合 LoRA 訓練來微調 30B 的模型。

### Quanto
[Quanto](https://github.com/huggingface/optimum-quanto) 是由 Hugging Face 所開發的量化套件，支援 FP8、INT8、INT4、 INT2 等量化層級。
1. 首先安裝套件：
    ```bash
    pip install quanto
    ```
2. 透過 QuantoConfig 類別來對模型進行量化：
    ```python
    from transformers import LlamaForCausalLM
    from transformers import QuantoConfig

    #可以是 int2, int4, int8, float8
    quanto_config = QuantoConfig(weights="int4")
    model = LlamaForCausalLM.from_pretrained(
        "meta-llama/Meta-Llama-3-8B-Instruct",
        device="auto",
        quantization=quanto_config,
    )
    ```
* 使用 Quanto 的優點就是量化過程非常簡單，但目前 Quanto 的運算速度並不是很快，除了 INT4 以外都會慢兩倍以上。

### GPTQ
[GPTQ](https://github.com/IST-DASLab/gptq) 是個相當受歡迎的量化技術。前面介紹的 BNB 量化雖然可以快速的將模型量化到低位元，但是其準確率與運算效率是個經常受到挑戰的部份。 GPTQ 與 BNB 不同的地方在於，GPTQ 使用了校準資料集進行 **Post Training Optimization** 的方法，來對每個權重逐步進行量化。
* 因為是使用實際資料進行校準，所以模型的失真度會相對較低一些，但也因此需要多花一些時間進行量化。
* 補充：
    Post Training Optimization：
    * 當量化至極低的位元時，每個量化值所代表的實際值之間差距會非常大，可能你是 -4 我是 -3，但我們的實際值相差其實很大。
    * 因為模型的參數量很大，若每個數值都有這樣不小的誤差，累積起來就會發生很嚴重的錯誤。
    * 若有個量化值在四捨五入前為 -3.5，那應該變成 -4 還是 -3 呢？因此這類的量化方法會透過實際的資料觀察哪個四捨五入的方向可以產生最小的誤差。
* [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) 是個能夠簡單進行 GPTQ 量化的套件，此套件已被整合在 Transformers 裡面。
1. 若要使用 GPTQ 量化，需要先安裝相關套件：
    ```bash
    pip install optimum auto-gptq
    ```
2. 透過 `GPTQConfig` 就可以對模型進行量化：
    ```python
    from transformers import GPTQConfig
    from transformers import LlamaForCausalLM as ModelCls
    from transformers import LlamaTokenizerFast as TkCls

    model_id = "meta-llama/Meta-Llama-3-8B"

    tk: TkCls = TkCls.from_pretrained(model_id)
    gptg_config = GPTQConfig(
        bits=4,             # 可以是2, 3, 4, 8
        tokenizer=tk,
        dataset="wikitext2",
        desc_act=False,
        # 提高 damp_percent 可以避免 torch._C._LinAlgError
        damp_percent=0.1,
        model_seqlen=2048,  # 影響 GPU 使用量
    )

    model: ModelCls = ModelCls.from_pretrained(
        model_id,
        device_map="cpu",   # 設定 CPU Offload
        low_cpu_mem_usage=True,
        quantization_config=gptg_config,
    )

    output_dir = "Models/Llama3-8B-Inst-GPTQ"
    model.save_pretrained(output_dir)
    tk.save_pretrained(output_dir)
    ```
    * 參數 `bits` 為量化的位元大小，可以是 2、3、4 或 8 位元。
    * 因為 GPTQ 是逐層量化的演算法，所以可以設定 `device_map="cpu"`，在量化的過程中把模型權重都先放在 CPU 記憶體裡面，只有在某一層需要量化時，再拿出來放進 GPU 記憶體裡面做量化，完成量化後再放回 CPU 記憶體。
        * 如此一來，再大的模型都不怕了，只要確保其中一層權重能放的下，就能對整個模型進行量化。
    * 使用 RTX 3090 將 Llama 3 8B 量化至 4-Bit， 大約只需要花費 7 分鐘左右。
    * GPTQ 格式的模型能直接存在硬碟裡面，原本 Llana 3 8B 佔用了 15 GB 的硬碟空間，進行 GPTQ 量化後縮小到了 5.4 GB 而已!
    * 注意：
        * `model_seqlen` 會影響 GPU 記憶體的使用量，如果會發生 CUDA OOM 的話可以試著調低這個數字。
        * 如果會遇到 `torch._C._LinAlgError` 錯誤的話，可以嘗試提高 `damp_percent` 的值。
        * 若是會遇到 `RuntimeError: shape '...' is invalid for input of size...` 的錯誤，則可以將 `desc_act` 設為 True 避免此問題。
3. 也可以使用自己的文本進行 GPTQ 量化，方法相當簡單：
    ```python
    dataset = ["Hello, GPTQ!", "Goodbye, GPTQ!"]
    gptq_configm= GPTQConfig(bits=4, tokenizer=tk, dataset=dataset)
    ```
    * 只要將文本列表傳進去就可以啦!
        * 建議使用與目標任務領域相近的文本，例如模型是個中文的泛用模型，那可能可以選擇中文維基當作文本。
        * 如果目標是氣象知識問答之類的，那可以用氣象相關的文本等等。
    * 在 GPTQ 原論文裡面，是從預訓練資料裡面，隨機挑選 128 筆固定長度的文本做校準。
    * 注意：
        * 雖然 GPTQ 可以透過校準資料集提昇量化的精準度，但也因此產生了過度擬合於校準資料集的問題。所以慎選校準資料集，以及量化後的完整評估相當重要。
4. 讀取已經做過 GPTQ 量化的模型時，不用再給一次 `GPTQConfig`，直接讀取即可：
    ```python
    from transformers import LlamaForCausalLM as ModelCls

    model: ModelCls = ModelCls.from_pretrained(
        "Models/Liama3-8B-Inst-GPTQ",
        device_map="auto",
    )
    ```
    * 透過 `nvidia-smi` 觀察 GPU 記憶體，就會發現使用量大幅減少囉!
    * 如果你不想花時間自己進行 GPTQ 量化，可以選擇直接去 HF Hub 上面搜尋 GPTQ，就可以找到許多別人已經做完量化的權重。
* 📝 範例程式碼
    * 參考量化大神 TheBloke 的 [Script](https://github.com/PanQiWei/AutoGPTQ/issues/179#issuecomment-1611257490)
    * HF Hub 上面已經做完 GPTQ 訓練，例如 TheBloke 大神就上傳相當多[使用 GPTQ 量化過的模型](https://huggingface.co/models?search=thebloke%20gptq)。
    * 自己嘗試: .../LLM/project/quantization

### AWQ
[Activation-Aware Weight Quantization (AWQ)](https://github.com/mit-han-lab/llm-awq) 也是一種 Post Training Optimization 的量化方法，同樣也是透過少量的校準資料集來進行更精準的量化。
* 相較於 GPTQ 而言，作者宣稱 AWQ 更不容易發生校準資料集的過度擬合問題，而且 AWQ 量化的模型在使用時可以有更快的推論速度。
    * 作者指出，像是 GPTQ 這種透過**反向傳播 (Backward)** 來進行量化的方法，可能會使模型在校準資料集上發生過度擬合的情形。
    * 因此AWQ並不仰賴反向傳播，只透過**前向運算 (Forward)** 來觀察**輸出分佈 (Activation Distribution)** 並藉此尋找**重要的模型權重 (Salient Weights)**。
    * 作者指出，只要將其中 1% 的重要權重保留為 FP16 的資料型態，就能大幅降低量化帶來的誤差。
* 但如果為了保護這些分散在不同地方的重要權重，而使得整份權重量化完後是混精度的，則會使計算效率降低。
    * 因此作者引入 **Activation-Aware Scaling** 的技巧來對每組權重做量化，使得重要權重能夠獲得保護的同時，也能夠避免混精度權重以提昇硬體友善度，降低運算瓶頸，在實際推論上可以有相當快的速度。
* 在社群上也有類似 AutoGPTQ 的 [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) 這種方便的工具可用，AWQ 量化的過程是一層一層的量化，所以 AutoAWQ 可以將模型權重本身放在 CPU 裡面，只有在該層需要量化時，才把它的權重拿出來放在 GPU 記憶體裡面，因此記憶體的使用非常有效率。
    * 單張 RTX 3090 24 GB 就能量化 70B 規模的模型。
1. 若要使用 AWQ 量化，首先需要安裝 AutoAWQ 套件：
    ```bash
    pip install autoawq
    ```
2. 安裝好 AutoAWQ 後，可以使用 `AutoAWQForCausalLM` 類別讀取原始模型：
    ```python
    import torch

    from awq import AutoAWQForCausalLM

    model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

    model = AutoAWQForCausalLM.from_pretrained(
        model_dir,
        torch_dtype=torch.float16,
        device_map="cpu",   # 權重可以放在 CPU 就好
    )
    ```
    * 這裡將模型放在 CPU 記憶體裡面，避免模型權重佔用太多 GPU 記憶體導致 AutoAWQ 沒有空間進行量化運算。
3. 接下來會需要準備一份校準資料集：
    * 預設會使用原作者提供的 [`mit-han-lab/pile-val-backup` 這份資料集](https://huggingface.co/datasets/mit-han-lab/pile-val-backup) 進行校準。
    * 但我們也可以自行準備一份字串列表當作資料集：
    ```python
    # 此為示範用的校準資料集，請根據自身應用替換成適當資料集
    # 只要是 list[str] 的型態且 Token 數量夠多就能量化

    calib_data = ["Hello, World! "  *  8] * 16
    ```
    * 注意：
        * 如果給的文本太短太少的話， 進行量化時就會發生 `RuntimeError: torch.cat() expected a non-empty list of Tensors` 的錯誤。
        * 這是因為在套件裡面的 `awq/utils/calib_data.py`，作者放了一個 Token 數量必須大於 512 才會拿來用的條件，可能是因為[原本 AWQ 的實做](https://github.com/casper-hansen/AutoAWQ/issues/191)就是如此的關係。

4. 接下來放上相關設定，並開始進行量化：
    ```python
    from transformers import AutoTokenizer as TkCls

    tokenizer = TkCls.from_pretrained(model_dir)

    # 相關設定
    quant_config = dict(
        zero_point=True,
        q_group_size=128,
        w_bit=4,
        version="GEMM",
    )

    # 進行量化
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calib_data,
    )

    # 儲存量化結果
    output_dir = "Models/Lama3-8B-Inst-AWQ"
    model.save_quantized(output_dir)
    tokenizer.save_pretrained(output_dir)
    ```
    * 筆者實測使用 RTX 3090 24 GB 量化一個 7B 模型，大約也是七八分鐘左右，速度上與 GPTQ 是差不多的，而且過程中所使用的 GPU 記憶體非常少量。 
    * 如果 CPU 記憶體夠大，也能拿來量化 70B 的模型，大概要花費 100 分鐘左右。 
    * 不過 70B 的模型即便量化到 4-Bit 也放不進一張 24GB GPU 裡面就是了。
5. 這些量化完的模型，可以直接讀取進來使用：
    ```python
    from transformers import LlamaForCausalLM as ModelCls

    model_dir = "Models/Llama3-8B-Inst-AWQ"

    model: ModelCls = ModelCls.from_pretrained(
        model_dir,
        device_map="auto",
    )   # 約 5.8 GiB
    ```
    * 不需要任何額外的設定就能讀取，且 AWQ 模型在讀取速度上快滿多的。

---

## 困惑度、混淆度 (Perplexity, PPL)
在量化的過程中，會相當重視模型的損失程度，也就是說量化完之後的誤差有多大，這在語言模型裡面通常以**困惑度 (Perplexity, PPL)** 來表示，PPL 是用來描述模型預測下一個字詞的時候其不確定性如何。
* 因為語言模型每次推論時會產生一份機率表：
    * 如果這個機率表顯示「下個字是 X 的機率為 100% 」那就代表模型非常**明確**，這時困惑度就會相對較低。
    * 但如果模型覺得「嗯……好像每個字都有可能，那就代表模型相當**困惑**，這時困感度就會較高。
* 一般語言模型都是以**交叉熵 (Cross Entropy)** 當作**損失函數 (Loss Function)** 而困惑度就是對**損失值 (Loss)** 取**指數函數 (Exponential Function)** 的結果。

### 那實際上到底要如何計算困惑度呢？
1. 首先，要先來瞭解如何取得單次推論時的 Loss：
    ```python
    model = AutoModelForCausalLM.from_pretrained(...) 
    tk = AutoTokenizer.from_pretrained(...)
    input_ids = tk.encode(...)

    with torch.no_grad():
        outputs = model.forward(input_ids=input_ids, labels=input_ids) 

    print(outputs.loss)
    ```
    * 只需要多指定一個 `labels` 參數，並且代入原本的 `input_ids` 就能得到 Loss 了。
2. 接下來對 Loss 取指數函數就能得到困感度：
    ```python
    ppl = torch.exp(outputs.loss)
    ```
### 最簡單的評測方法
**固定序列長度**來計算困惑度，以 Wikitext 資料集為例：
1. 首先透過分詞器進行分詞：
    ```python
    from datasets import load_dataset

    tk = AutoTokenizer.from_pretrained(...)

    dataset = load_dataset(
        "wikitext",
        "wikitext-2-raw-vl",
        split="test",
    )

    Input_ids = list()
    for item in dataset:
        text = item["text"] + "\n"
        tokens = tk.encode(text, add_special_tokens=False)
        input_ids.extend(tokens)
    ```
2. 這裡先設定序列長度為 4096 來進行評估：
    ```python
    import torch

    seqlen = 4096
    data_size = len(input_ids) // seqlen # 計算序列數量
    input_ids= input_ids[: data_size * seqlen] # 捨棄最後一筆
    ```
    * 其中長度不足 4096 的最後一筆資料會被捨棄。
3. 然後將 `input_ids` 轉入 `Tensor`，並在每個序列的開頭加上 BOS Token，最後把 `Tensor` 移動到對應的裝置上：
    ```python
    input_ids = torch.LongTensor(input_ids).view(data_size, seqlen)
    bos_token = torch.full(
        (data_size, 1),
        tk.bos_token_id,
        dtype=torch.int64,
    )

    input_ids = torch.concat((bos_token, input_ids), dim=1)
    input_ids = input_ids.to(model.device)
    ```
    * 雖然這裡長度設定為 4096，但是因為加上額外的一個 BOS Token，所以填型的實際推論長度是 4097，不過計算 Loss 時會位移一格，因此最終得到的困惑度依然是以序列長度 4096 算出來的。
4. 接下來開始對測試資料進行推論：
    ```python
    nlls = list()
    for i in range(data_size):
        batch = input_ids[i : i+1]
        with torch.no_grad():
            outputs = model.forward(batch, labels=batch)
        nlls.append(outputs.loss)
    ppl = torch.exp(torch.stack(nlls).mean())
    print(ppl)
    ```
    * `n11s` 代表 Negative Log-Likelihood，與 Cross Entropy 基本上是等價的概念。
5. 因為評估的過程可能會花上一段時間，所以可以借助 `tqdm` 套件來顯示進度：
    ```python
    from tqdm import trange

    batch_size = 1
    nlls = list()
    with trange(0, data_size, batch_size) as prog:
        for i in prog:
            batch = input_ids[i : i + batch_size]
            outputs = model.forward(batch, labels=batch)
            nlls.append(outputs.loss)
            ppl = torch.exp(torch.stack(nlls).mean())
            prog.desc= f"ppl: {ppl:.4f}"
    ```
    * 筆者實測 Llama 3 8B Instruct 的困惑度在序列長度 4K 時為 7.7494，而序列長度 2K 時則為 8.3272，由此可見序列長度對困惑度也有些微的影響。
    * 一般而言測試的序列長度在模型的訓練長度之內時，序列越長困惑度越低，但如果序列長度超過訓練長度太多時，困惑度就會開始無情暴漲。
* 補充：
    * 固定長度的評估方法雖然簡單方便，但可能會有**文本邊界不自然**的問題。
    * 因此可以改成採用非固定長度的文本進行評估，必須特別注意**填充**與**標籤**問題，有 Padding Token 的位置其 Label 應設為 `-100` 才會算出正確的 Loss。
    * 另外，因為無論文本長度為何，都只會算出一份 Loss，所以一些比較細微的評估可能會根據文本長度做加權。
6. 有了困惑度之後，就可以用來比較量化前後的模型了！這裡拿 Llama 3 8B Instruct 簡單比較一下 Float16、BFloat16 與 BitsAndBytes 之間困惑度的差異：
    | 類型      | 困惑度 |
    | --------- | ------ |
    | Float16   | 7.7494 |
    | BFloat16  | 7.7570 |
    | BNB 8-Bit | 7.7730 |
    | BNB 4-Bit | 8.1899 |
    * 可以看到 FP16 與 BF16 幾乎沒有差異，BNB 8-Bit 也與 FP16 十分相近，但 BNB 4-Bit 就稍微有點距離了，不過這個差距還算可以接受。
    * 根據筆者的經驗， 通常困惑度的差距到達 1~2 點左右就會開始感受到實際輸出品質在下降了。
* 困惑度並不是個絕對性的指標，在同個模型內做比較是滿有參考性的，但如果是不同模型或者不同訓練資料時，用困惑度進行互相比較就需要更加謹慎一些。
    * 例如分詞器的不同，可能會造成同一份文本對不同模型而言有不同的長度，在困惑度的比較上也會因此產生差異。
![困惑度與模型大小關係圖](https://hackmd.io/_uploads/H1fiiSbzlx.png)
* 在合理的量化下，模型的參數量通常會比模型的大小來的重要。
    * 就算用 16-Bit 去跑一個 7B 的模型，其效果也不會比 4-Bit 的 13B 模型還要好，即使 7B 16-Bit 的模型在賣際大小上比 13B 4-Bit 還要大的多。
    * 因此模型的能力主要取決於參數量的多寡，而資料型態僅影響運算的精準度而已。
* 即便模型變小了，但模型的總計算量原則上是不變的，因此並不會因為模型從 16-Bit 變成 8-Bit 後，推論速度就快上兩倍。
* 真正有影響的是每個權重的位元數縮小後，在運算上可以減少傳輸頻寬，使**吞吐量 (Throughput)** 提高，也就是每個**批次 (Batch)** 能容納的 Token 變多了。
* 或者從軟硬體上針對特殊設計的資料型態進行加速運算，但是基本上計算量還是一樣的。

---

## 連結
* [ONNX Runtime: Quantization](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)
* [GitHub: bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
* [GitHub: EETQ](https://github.com/NetEase-FuXi/EETQ)
* [HF Blog: QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
* [HF Blog: Quantization](https://huggingface.co/blog/merve/quantization)
* [HF Blog: Quantization Overview](https://huggingface.co/blog/overview-quantization-transformers)
* [HF Blog: bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
* [HF Blog: AutoGPTQ](https://huggingface.co/blog/gptq-integration)
* [arXiv Paper: LLM.int8()](https://arxiv.org/abs/2208.07339)
* [arXiv Paper: 8-Bit Optimizer](https://arxiv.org/abs/2110.02861)
*[ arXiv Paper: QLoRA](https://arxiv.org/abs/2305.14314)
* [Reddit: llama.cpp Quantization](https://www.reddit.com/r/LocalLLaMA/comments/140nzqr/2_to_6_bit_quantization_coming_to_llamacpp/)

---

## 結論
* 本章節介紹了量化的基礎觀念，與一些常用的量化方法，這些量化技術大幅降低了遊玩語言模型的門檻，即便只有一張 24GB 的 GPU 也能輕鬆玩轉 30B 規模的模型。
* 關於更多量化的方法，推薦可以參考 [HF Quantization](https://tinyurl.com/llm-hf-quant) 的介紹指南。
* 雖然量化這門學問已經發展了一段時間，但依然是個快速成長中的技術。技術迭代的速度也是相當之快，量化框架層出不窮，對筆者這種貧窮玩家而言是一大利多。
    * 例如筆者今天查看 [TGI](https://github.com/huggingface/text-generation-inference) 的參數說明時才發現他們居然要將 BNB 棄用了
    * Text Generation Inference, TGI 是 Hugging Face 製作的一個推論框架，未來也會仔細介紹這個專案。
    * TGI 表示接下來將會使用 [EETQ](https://github.com/NetEase-FuXi/EETQ) 套件來取代 BNB 框架。

---