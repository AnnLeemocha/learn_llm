# 檢索 (Retrieval) & 生成 (Generation)
雖然 ChatGPT 已經有相當豐富的知識含量，但還是難免會產生一些事實錯誤或偏差。
* 為了解決這個問題，結合檢索模型 (Retrieval Model) 的做法相當受歡迎。
* **檢索模型**
    * 像是一個搜尋引擎，系統根據使用者的問題去尋找相關的文章，並將這些文章一起放進模型輸入裡面，然後要求語言模型根據文章回答問題。
    * 機器學習裡面也被稱為機器閱讀理解 (Machine Reading Comprehension, MRC) 的問題。

---

## 生成式資訊檢索 (Generative Information Retrieval) 
透過==文字生成模型==來進行資訊檢索，主要著重在如何提高檢索的準確度，是一種資訊檢索的分支做法。
分成封閉式 (Close Domain) 與開放式 (Open Domain) 兩種。

### 封閉生成式資訊檢索 (Close Domain Generative IR)
將一個特定領域的知識，例如法律、醫學的相關文章作為訓練資料，將這些訓練資料放進文字生成模型裡面做訓練，並要求模型根據使用者的問題來回答。
* 現在 ChatGPT 的知識性問答能力，就像是一種非常大型且泛用的封閉式資訊檢索系統。
* 問題:
    1. 幻覺
        * 回答錯誤的狀況，俗稱「一本正經的胡言亂語」。
    2. 解釋性不好
        * 當模型產生錯誤的答案時，會很直接的認為「也許是訓練資料有問題吧！」。
    3. 難驗證答案的正確性
        * 難以將生成的答案與參考的來源做配對，也就是說系統通常不知道這個答案到底來自哪裡。
    4. 知識受限於訓練資料
        * 沒有即時性的最新資訊，需要更新資料。

### 開放生成式資訊檢索 (Open Domain Generative IR) 
挑戰多個領域的知識問答，通常會結合檢索模型或搜尋引擎來回答問題。這類做法的關鍵有兩個。

1. 如何正確的**檢索**相關文章？
    * 由 IR 系統來解決，系統的基本檢索能力、檢索的粒度、是否能跨語言檢索等，都會大幅影響整個問答系統的效果。
    * 其次是檢索的速度如何、會不會很佔用記憶體等等，這些相對實務上的問題。
    * 
2. 如何正確的**解讀**這些文章？
    * 考驗文字生成模型的能力。
        * 型能夠理解問題嗎？
        * 模型知道可以回答問題的文章段落在哪裡嗎？
        * 模型能夠產生回答問題的格式嗎？
        * 如果資訊不足，模型知道拒絕回答嗎？還是也會亂答一通呢？

* 在 ChatGPT 問世之前，第一點已經有了非常多很好的解法。
    * 例如各種文字向量模型像是 Google 的 [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175) 與微軟的 [E5](https://arxiv.org/abs/2212.03533) 等等，不僅效果很好，模型權重也有開源，使用方法也滿簡單的。
* 而在 ChatGPT 問世之後，第二點問題也迎刃而解。
    * 其強大的上下文理解能力與自然流暢的文字生成，使開放生成式資訊檢索再度成為知識問答的新寵兒。
* 大家常用 **RAG (Retrieval Augmented Generation)** 來稱呼這種結合檢所與生成的系統，出自於 [2020 年 Facebook AI Research 的一篇論文](https://arxiv.org/abs/2005.11401)。

## 論文閱讀問答機器人實作
* 以 Latex 論文閱讀為題目，設計一個問答機器人。
* 這裡使用 [GPT-4](https://arxiv.org/abs/2303.08774) 的論文進行示範。
* 註：筆者使用的開發環境為 **Ubuntu 22.04 + Python 3.10** 版本。
* 使用到的套件:
    `pip install tiktoken==0.6.0 openai==1.28.1 faiss-cpu==1.8.0`
* 嘗試分析一篇比較新的論文 ([LongLoRA](https://arxiv.org/abs/2309.12307)) 以確保 ChatGPT 完全沒有看過這篇論文。
* 📝 範例程式碼
    * [筆者程式碼](https://github.com/penut85420/LatexPaperQA)
    * 自己嘗試: .../LLM/project/latex_paper_qa
### 資料蒐集
在 arXiv 上面有相當大量的論文文本，部份論文甚至會上傳 Latex 原始碼。
* 可以在論文介紹頁面的右上角點擊 "Other Formats" 查看是否有提供:

* 如果有提供的話，則會有個 Download Source 的連結可以按:
下載下來如果沒有副檔名的檔案，自己手動加上 `.tar.gz` 即可。

### 索引階段 Index Phase
GPT-4 論文的 Latex 原文有近三萬多個 Tokens，無論是基於模型輸入大小的考量，還是荷包的考量，都不太可能將整份文章直接放進模型裡面當成輸入。
* 因此我們會在索引階段 (Index Phase) 將文章切成一塊一塊的**區塊 (Chunk)**，我們將這些區塊索引起來，在使用階段時方便搜尋。
* 📝 範例程式碼
    * [筆者程式碼](https://github.com/penut85420/LatexPaperQA/blob/main/step01.index.py)
    * 自己嘗試: .../LLM/project/latex_paper_qa

1. 拜訪資料夾中所有的 Latex 文件 (.tex)
2. 讀取每份文件的內容並且開始切割區塊
    * 先簡單依照換行符號進行切割，並且將排版用的雙空格換成單空格。
3. 處理 Special Token 問題，並計算每個 Chunk 有多少 Tokens
    * 使用 `tiktoken` 套件提供的 Tokenizer 來計算
    * 注意 Tokenizer 內建的 Special Token，如果直接 Encode 會跳錯誤，所以我們要設定 `disallowed_special=()` 的參數
        * 這個機制是為了避免使用者透過 **提示注入攻擊 (Prompt Injection)** 來「越獄」，並破壞語言模型原本的行為。
    * 觀察並處理:
        1. 零散的 Chunk 會造成上下文語意被嚴重截斷，效果通常不會很好。
            * 將零散的小 Chunk 合併為一個大的 Chunk，但又不會大到模型塞不下。
        2. 如何決定一個 Chunk 有多大呢？
            * 根據**模型能力與可負擔的成本**而有變化。 
            * 另外太多或太長的 Chunk 模型未必處理的來，有時反而會因為雜訊太多而影響回答的品質。
            * 筆者希望每個 Request 平均只需要消耗 2000 個 Tokens :
              * 預留 500 Tokens 給模型輸出。
                剩下 1500 給搜尋到的 Chunks。
              * 假設我們每次取 5 個 Chunks 來用。
                這樣每個 Chunk 就是 `1500 / 5 = 300` 個 Tokens 可用。
4. 觀察處理完的 Chunk 是否正確
    * 確認格式正常沒有變亂碼
5. 將這些切好的 Chunks 投入 OpenAI Embedding API
    * 注意 OpenAI API 的 Rate Limit。
6. 將 Chunks 與 Embeddings 存起來，完成我們的索引階段
    * 因為文章是固定的，所以切 Chunk 以及取 Embedding 的動作就不需要再做一次。
    * 簡單將 Chunks 用 JSON 格式存起來，Embeddings 的部份也是用 Numpy 隨便存。但這種做法是相對沒有效率的，實務上建議使用一些資料庫系統來管理這些資料。

### 查詢階段 Query Phase
Faiss 是 Facebook 製作的一個向量搜尋引擎，可以很有效率的處理大規模向量搜尋的問題，並且用法相當簡單。
* 📝 範例程式碼
    * [筆者程式碼](https://github.com/penut85420/LatexPaperQA/blob/main/step02.query.py)
    * 自己嘗試: .../LLM/project/latex_paper_qa
1. 將方才存下來的 Chunks 與 Embeddings 讀取出來，並且透過 Faiss 套件建立 Embedding 的索引
    * 需要先透過 Embedding Size 來初始化 faiss 的 IndexFlatL2 類別，然後將這些 Embedding 加進去做索引。
2. 透過 Embedding API 取得使用者輸入的 Embedding
3. 根據 Query Embedding 尋找前五個相似的 Embeddings，並根據 Index 來建立 Prompt
    * 可以透過 `IndexFlatL2.search` 完成
    * 指定 `k=5` 代表只需要取前五名相似的向量。
    * 最有可能包含答案的文章，通常是相似度最高（也就是距離最短）的文章。
        * 讓最相近的文章最靠近問題，所有 Chunks 之間以雙換行隔開。
4. 把這個 Prompt 整份投入 ChatGPT API 

### 分析
詳細的問答結果可以參考[這份連結](https://github.com/penut85420/LatexPaperQA/blob/main/RESP.md#longlora-%E8%AB%96%E6%96%87%E5%95%8F%E7%AD%94)。
* 文章內包含的資訊：
    > Q: GPT-4 能夠理解圖片嗎？
    > A: ...
* GPT-4 能夠理解圖片嗎？
    > Q: GPT-4 的模型參數量多少？
    > A: 文件中並未提到 GPT-4 的模型參數量。
* 詢問與位置相關的問題:
    > Q: 請問第二頁的內容是什麼？
    > A: 抱歉，我無法回答這個問題，因為提供的資訊不足以確定第二頁的內容是什麼。請提供更多的資訊。
    * 如何解決:
        * 讓 Chunk 附帶 Metadata 資訊，告知 ChatGPT 這個 Chunk 的所在位置，而且也要能讓 Embedding 查詢時可以查到這個段落。
* Latex 為自動編排章節，所以透過 Latex 原始碼也無從得知章節次序
    > Q: 請摘要第四章節的內容
    > A: 抱歉，文件中並不存在第四章節的內容。最後一個標題為「Authorship, Credit Attribution, and Acknowledgements」。
    * 沒有加入太多 Chunk 的 Metadata 在裡面，例如這個 Chunk 來自哪份文件？位在文件的哪個位置？
    * 如何解決:
        * 加入 Latex Parser 來推論章節的次序是什麼，並且一同加入 Metadata 裡面。

---

## 參考
* [iThome News: 生成式 IR](https://www.ithome.com.tw/news/158297)
* [Meta AI - Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)
* [Amazon SageMaker: Retrieval Augmented Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)
* [IBM: What is retrieval-augmented generation?](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)
* [Prompting Guide: Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)

---

## 結論
* 探討了生成式 IR 的原理，瞭解現在 IR 系統如何與 LLM 做結合，並且簡單實做了一個 Open Domain 的論文問答系統。
* 因為這個系統相對簡單，有許多細節沒有處理，因此也有不少侷限，例如無法詢問頁數資訊、章節資訊相關的問題等等。但至少開發者可以專注在優化檢索的環節，而不再需要煩惱生成的部份了。

---