# LangChain
前面幾個章節提到的框架，大多屬於底層推論類，但上層的應用開發類也是許多人關注的焦點，其中最炙手可熱的非 [LangChain](https://www.langchain.com/) 莫屬。
* 開始實際使用語言模型進行應用開發時，會面臨到許多雜七雜八的提示樣板、重複的生成程序以及固定瑣碎的串接流程等等。
* 多數情況下，這些樣板、流程切開來看，都不是很複雜的程式，自己動手寫也都不難寫。
* 但是當不同的流程開始堆疊出更龐大、更複雜的應用時，這一切就會顯得十分混亂。
* 這時就能借助 LangChain 來幫我們進行更好的程式碼管理。
* 如果想看 LangChain Pro 的可以參考 Ted Chen 大哥的[系列文](https://ithelp.ithome.com.tw/users/20154415/ironman/6008)。

---

## 題目
中二技能翻譯系統。

---

## 資料來源
想要製作一個中二技能翻譯系統，勢必需要一些中二的翻譯資料，但是哪裡可以獲得這種中二的資料呢？
* 這時筆者就把歪腦筋動到英雄聯盟的 [Riot API](https://tinyurl.com/llm-riot-api) 上了！英雄聯盟有上百位英雄，每個英雄的技能都是由真人精心翻譯的，因此是個翻譯品質相當不錯的選擇！
* 筆者先手動取一些樣本，來展示這個中二系統的核心概念：
    ```!
    請根據以下範例，產生一個中二的技能翻譯，只需要回答翻譯就好。

    Source: The Darkin Blade
    Target: 冥血邪劍

    Source: Infernal Chains
    Target: 冥府血鏈

    Source: Umbral Dash
    Target: 冥影衝鋒

    Source: World Ender
    Target: 劍魔滅世

    Source: Lightning Slash In The Dark
    Target:
    ```
    * 將這份提示丟進 ChatGPT 測試，得到一個「**冥雷暗破**」的翻譯，有那麼一丟丟要成為影之強者的感覺了！
    * 筆者也另外實測了一下Llama 38B 的結果，得到「**暗電劈冥**」的翻譯，哇！感覺心中有某種神奇的東西在燃燒了！

---

## 資料爬取
官方文件其實描述的滿詳細，我們需要先獲取全部的英雄列表，然後根據英雄名稱一一抓取各自的資料。
* 我們先手動抓取全部的英雄列表，可以從以下連結獲得：
    ```!
    https://ddragon.leagueoflegends.com/cdn/14.10.1/data/en_US/champion.json
    ```
    * 網址裡面的 `14.10.1` 是遊戲版本，如果懷念菲歐拉的舊版大絕名稱，可以考慮回溯到 `5.14.1` 之前的版本去抓取。
    * 而 `en_US` 代表語言是英文的意思，因為所有英雄的索引都是使用英文，所以這個步驟只需要英文資料。
* 接下來開始爬取英雄的技能資料，每個英雄資料的 API 格式如下：
    ```!
    http://ddragon.leagueoflegends.com/cdn/13.19.1/data/<lang>/champion/<champ>.json
    ```
    * `<lang>` 為語言代號，而 `<champ>` 為英雄的英文名字。
    * 我們會分別爬取英文 `en_US` 與繁體中文 `zh_TW` 兩種語言的資料。
* 首先定義爬取的函式：
    ```python
    import json
    import os
    import requests

    def get_data(champ, lang, ver="14.10.1"):
        # 取得資料
        cdn_url = f"http://ddragon.leagueoflegends.com/cdn/{ver}/data"
        url = f"{cdn_url}/{lang}/champion/{champ}.json"
        resp = requests.get(url)

        # 寫入檔案
        fn = f"data/{lang}/{champ}.json"
        os.makedirs(f"data/{lang}", exist_ok=True) 
        with open(fn, "wt", encoding="UTF-8") as fp:
            fp.write(resp.text)
    ```
* 爬蟲的過程，可以善用 Python 內建的 `concurrent` 套件來減少等待時間：
    ```python
    from tqdm import trange
    from concurrent, futures import ThreadPoolExecutor, as_completed

    with open("champion.json", "rt", encoding="UTF-8") as fp:
        data: dict = json.load(fp)["data"]

    with ThreadPoolExecutor(max_workers=16) as tpe:
        futures = [
            tpe.submit(get_data, champ, lang)
            for champ in data.keys()
            for lang in ("en_US", "zh_TW")        
        ]

        with trange(len(futures), ncols=100) as prog:
            [prog.update() for _ in as_completed(futures)]
    ```
    * 這樣三百多筆資料不用 10 秒鐘就可以全部下載完囉！
* 若想要其他語言可參考[這篇資訊](https://developer.riotgames.com/docs/lol#data-dragon_data-assets)做調整。 
* 補充：
    * 如果你經常覺得 `tqdm` 套件的進度條太寬太佔版面，除了透過 `ncols` 參數設定以外，也可以透過環境變數 `TQDM_NCOLS` 來設定所有 `tqdm` 進度條的寬度，但是如果該進度條有設定 `dynamic_ncols` 的話就會失效。
* 接著依序拜訪造型外觀名稱、主動技能名稱、被動技能名稱，這些都是重要的中二要素來源，建立中英技能資料集的程式碼如下：
    ```python
    import json

    def load_json(file_path):
        with open(file_path, "rt", encoding="UTF-8") as fp:
            return json.load(fp)

    def create_item(source, target):
        return {"source": source, "target": target}

    datasets = list()
    data: dict[str, dict] = load_json("champion.json")

    for champ in data["data"].keys():
        # 讀取英雄資料
        en_data = load_json(f"data/en_US/{champ}.json")
        zh_data = load_json(f"data/zh_TW/{champ}.json")

        # 外觀造型名稱
        en_skins = en_data["data"][champ]["skins"]
        zh_skins = zh_data["data"][champ]["skins"]

        # 跳過經典造型
        for en_sk, zh_sk in zip(en_skins[1:], zh_skins[1:]):
            en_sk_name = en_sk["name"]
            zh_sk_name = zh_sk["name"]
            datasets.append(create_item(en_sk_name, zh_sk_name))

        # 主動技能名稱
        en_spells = en_data["data"][champ]["spells"]
        zh_spells = zh_data["data"][champ]["spells"]
        for en_sp, zh_sp in zip(en_spells, zh_spells):
            en_name, zh_name = en_sp["name"], zh_sp["name"]
            datasets.append(create_item(en_name, zh_name))

        # 被動技能名稱
        en_pass = en_data["data"][champ]["passive"]["name"]
        zh_pass = zh_data["data"][champ]["passive"]["name"]
        datasets.append(create_item(en_pass, zh_pass))

    with open("datasets.json", "wt", encoding="UTF-8") as fp:
        json.dump(datasets, fp, ensure_ascii=False, indent=4)
    ```
    * 到這裡完成資料集的建立，接下來正式進入 LangChain 的部分！

---

## LLM
LangChain 其實很講究**元件間的互動**，在搞懂互動之前，鐵定要先搞懂元件！
* 在 LangChain 裡面的元件多數可以獨立運作，先來安裝相關套件：
    ```bash
    pip install langchain langchain-community langchain-openai
    ```
* LangChain 的特色之一在於整合了相當多相關的套件框架，讓我們可以用統一的介面去操作，例如LLM類別就有整合 OpenAl、ggml、vLLM 或 TGI 等語言模型後端框架。
* 但是框架這麼多，有選擇障礙怎麼辦？
    * 還記得前面的章節有提到，這些框架其實都有提供相容 OpenAI API 的用法嗎？

### 選擇 vLLM 當作主要的後端框架來進行推論：
在 LangChain 裡面，一樣可以把它當成 OpenAI API 來用。
* 首先啟動一個 vLLM 服務：
    ```bash
    python -m vllm.entrypoints.openai.api_server \
        --model meta-llama/Meta-Llama-3-8B-Instruct \
        --api-key auth-token-ouo123
    ```
* 在 LangChain 裡面 LLM 的基本用法如下：
    ```python
    from langchain.callbacks import StreamingStdOutCallbackHandler
    from langchain_openai import OpenAI

    llm  = OpenAI(
        api_key="auth-token-ouo123", 
        model="meta-llama/Meta-Llama-3-8B-Instruct", 
        base_url="http://localhost:8000/v1", 
        callbacks=[StreamingStdOutCallbackHandler()], 
        streaming=True,
    )

    llm.invoke("hello, i am", stop=("\n"))
    ```
    * 將 `base_url` 參數換成 vLLM Server 的路徑，然後填入啟用 vLLM 時設定的 API Key，並指定使用的模型名稱就可以囉！
    * 另外設定 `streaming=True` 的參數之後，就可以搭配上面的 Callback 進行串流輸出。

### 選擇 Hugging Face Text Generation Inference 當作主要的後端框架來進行推論：
* 在此之前，需要先將 TGI Service 跑起來，可以參考筆者的 [TGI 介紹文章](https://ithelp.ithome.com.tw/articles/10332065)。 (同上)
* 模型選擇的部份，可以考慮 [Taiwan Llama](https://github.com/MiuLab/Taiwan-LLaMa) 以及最近出的 [CKIP Llama](https://github.com/ckiplab/CKIP-Llama-2-7b) 等模型，另外像是 [Vicuna](https://huggingface.co/lmsys/vicuna-7b-v1.5) 也是可以。
* 因為這只是個趣味性質的應用，所以對模型的效能沒有太大的講究。
* 在 LangChain 裡面 LLM 的基本用法如下：
    ```python
    from langchain.llms import HuggingFaceTextGenInference
    from langchain.callbacks import StreamingStdOutCallbackHandler

    cb = StreamingStdOutCallbackHandler()

    llm = HuggingFaceTextGenInference(
        inference_server_url="http://localhost:8080/",
        max_new_tokens=64,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.5,
        truncate=512,  # 截斷 512 Tokens 以前的輸入
    )

    # llm("### USER: 什麼是語言模型？\n### ASSISTANT: ", stop=["，"])
    # # Output: '語言模型是一種人工智慧模型'

    llm("### USER: 什麼是語言模型？\n### ASSISTANT: ", callbacks=[cb])
    ```
    * 搭配 Callback 進行 Streaming 輸出，這個 `callbacks` 參數其實也能放在 LLM 初始化裡面。

---

## Prompt Template
* 在 LangChain 裡面可以使用各種 Prompt Template 類別來對不同的樣板進行管理，其用法如下：
    ```python
    from langchain.prompts import PromptTemplate

    template = "### USER: {query}\n### ASSISTANT: "
    prompt = PromptTemplate.from_template(template)

    prompt.format(query="什麼是語言模型？")
    print(repr(prompt))
    # Output: '### USER: 什麼是語言模型？\n### ASSISTANT: '
    ```
* 經由樣板產生的提示，雖然可以當成一般字串丟進 LLM 使用，但是在 LangChain 裡面其實有更 "**Chain**" 的做法：
    ```python
    from langchain.prompts import PromptTemplate

    template = (
        "### SYSTEM:\n使用繁體中文回答。\n\n",
        "### USER:\n{query}\n\n",
        "### ASSISTANT:\n",
    )
    template = PromptTemplate.from_template(template)
    chain = template | llm
    chain.invoke({"query":"什麼是語言模型？"})
    # Output: 語言模型是一種人工智慧模型...
    ```
    * 很酷吧！居然用 OR 運算子 `|` 來描述元件間的互動，是個非常特別的做法！
* 如果是檢索類的應用，那樣板可能會長的像這樣：
    ```python
    template = (
        "### INST: 請根據 REF 回答 USER 的問題\n\n",
        "### REF: {reference}\n\n",
        "### USER: {query}\n\n",
        "### ASSISTANT: "
    )
    prompt = PromptTemplate.from_template(template)

    chain = prompt | llm.bind(stop-["###"])

    params = {
        "reference": "小明每天從板橋通勤到新店工作",
        "query": "請問小明下班會回去哪裡？",
    }

    chain.invoke(params)
    # Output: 小明從板橋通勤到新店工作,因此下班後他將會返回板橋。
    ```
    * 其實這些基本的提示樣板跟一般的格式化字串用起來沒什麼太大的不同，但是透過這個類別就能跟其他元件 "Chain" 起來用。
* 補充：
    * 在這裡 `|` 是一種 Bitwise Operator，此運算子會在位元層級進行 OR 運算，例如：
        ```python
        x = 0b1100 | 0b1010
        print(f"0b{x:b}") # Output: 0b1110
        ```
    * 在 Python 中，可以定義類別的 `_or_` 方法來自訂 `|` 運算的行為，例如：
        ```python
        class Hello:
            def init (self, value):
                self.value = value

            def _or_(self, other):
                print (f"or self={self}, other={other}")

            def _repr_(self):
                return f"Hello({self.value})"

        hl = Hello(1)
        h2 = Hello(2)
        h1 | h2 # Output: or self-Hello(1), other-Hello(2)
        ```
    * 而在 LangChain 框架裡面 `chain = template llm` 的這段敘述，其實更像是指令操作裡面 Pipeline 的概念，將前者的內容當作後者的輸入。

---

## Embedding Model
Embedding Model 也是 LangChain 另外一個整合相當豐富的部分，[各式各樣的 Embedding 框架](https://python.langchain.com/docs/integrations/text_embedding)都被整合在裡面了。
* 根據筆者的經驗，在跨語言且短文本的檢索上，Google 的 Universal Sentence Encoder 效果是不錯的，而且模型尺寸也比較小，相當適合我們這種搞笑應用：
    ```python
    from langchain.embeddings import TensorflowHubEmbeddings

    url = (
        "https://tfhub.dev/google/",
        "universal-sentence-encoder/tensorFlow2/",
        "multilingual-large/2"
    )
    embeddings = TensorflowHubEmbeddings(model_url=url)
    ```
* 基本用法大致如下：
    ```python
    import numpy as np

    texts = ["dog", "狗", "cat", "貓"]
    embs = embeddings.embed_documents(texts)
    np.inner(embs, embs)
    ```

---

## Vector Store
有了向量之後就可以存在向量儲存空間裡面，這些存放向量的地方稱為 Vector Store。
* 其用途不僅是存放向量，更重要的功能在於**快速搜尋向量**。
* 例如 Faiss 就是個可以存放與搜尋向量的套件，搭配剛才的 Embedding Model 一起使用：
    ```python
    from langchain.vectorstores import FAISS

    texts = ["dog", "狗", "cat", "貓"]
    faiss_store = FAISS.from_texts(
        texts=texts,
        embedding=embeddings,
    )
    ```
* Vector Store 會自動透過 Embeddings 類別將文字轉為向量並存起來，這樣就可以直接查詢並取得語意相似的結果：
    ```python
    results = faiss_store.search(
        query="고양이",  # 韓文的「貓」
        search_type="similarity",
        k=2,  # 取兩筆結果
    )

    for res in results:
        print(res.page_content)

    # Outputs: 貓, cat
    ```
    * 參數 `k` 用來指定系統要搜尋幾筆結果，然後 Faiss 就會依照相似度排序並回傳結果。
    * 說到搜尋就要提一下我們的老朋友 BM25 好夥伴：
        ```python
        from langchain.retrievers import BM25Retriever

        bm25 = BM25Retriever.from_texts(texts)
        bm25.get_relevant_documents("cat")
        ```
* 在 LangChain 裡面 BM25 屬於 Retriever，而 Faiss 屬於 VectorStores，但兩者都具有搜尋的功能。該如何一起使用呢？
    * 這時可以透過 `EnsembleRetriever` 類別來處理，首先需要先將 Falss 轉換成 `Retriever` 類別：
        ```python
        faiss = faiss_store.as_retriever()
        ```
    * 然後再將 BM25 與 Faiss 一起丟進 Ensemble 裡面：
        ```python
        from langchain.retrievers import EnsembleRetriever

        ensemble = EnsembleRetriever(
            retrievers=[faiss, bm25],
            weights=[0.75, 0.25]
        )

        ensemble.get_relevant_documents("고양이")
        ```
    * 這樣在搜尋時就可以同時參考兩種檢索系統的結果了。
* Vector Store 的種類相當繁多，請根據自身應用的場景做選擇。因為筆者的資料其實沒很多，所以這個應用只需要 Faiss 就足夠了。

---

## Few-Shot Prompt Template
在這個應用裡面會使用到 Few-Shot 的技巧來建立完整的提示，因此需要用到 `FewShotPromptTemplate` 類別。
* 在使用此類別之前，需要使用基本的提示樣板來定義每個範例的格式：
    ```python
    examples = [
        {"source": "hello", "target": "哈囉"},
        {"source": "goodbye", "target": "再見"},
    ]

    example_prompt = PromptTemplate(
        input_variables=["source", "target"],
        template="Source: {source}\nTarget: {target}",
    )

    print(example_prompt.format(**examples[0]))

    """
    Source: hello
    Target: 哈囉
    """
    ```
* 接著以這個樣板為基礎來建立 Few-Shot Prompt Template 的內容：
    ```python
    from langchain.prompts.few_shot import FewShotPromptTemplate

    prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        prefix="請將以下單字翻譯成中文",
        suffix="Source: {input}\nTarget: ",
        example_separator="\n===\n",
        input_variables=["input"],
    )

    print(prompt.format(input="Starburst Stream"))
    ```
    * 輸出結果：
    ```
    請將以下單字翻譯成中文
    ===
    Source: hello
    Target: 哈囉
    ===
    Source: goodbye
    Target: 再見
    ===
    Source: Starburst Stream
    Target:
    ```
    * 這樣就完成 Few-Shot Prompt Template 的設定囉！

---

## Example Selector
我們已經知道如何將 Prompt 與LLM串在一起了，現在需要煩惱的部份是`使用者輸入 => 檢索 => 少量樣本提示`這段流程該如何串接起來。
* 這裡就要出動 Example Selector 來幫忙：
    ```python
    import json

    from langchain.prompts.example_selector import (
        SemanticSimilarityExampleSelector as Selector
    )
    from langchain_community.embeddings import TensorflowHubEmbeddings
    from langchain_community.vectorstores import FAISS

    with open("datasets.json", "rt", encoding="UTF-8") as fp:
        datasets: dict[str, str] = json.load(fp)

    url = (
        "https://www.kaggle.com/models/google/",
        "universal-sentence-encoder/tensorFlow2/",
        "multilingual-large/2"
    )
    embeddings = TensorflowHubEmbeddings(model_url=url)

    example_selector = Selector.from_examples(
        examples=datasets,
        embeddings=embeddings,
        vectorstore_cls=FAISS,
        k=10,
    )
    ```
* Example Selector 把提取和存放向量的動作都封裝起來了，查詢方法如下：
    ```python
    result = example_selector.select_examples({"query": "Dark"})
    print(result)

    """
    查詢結果：
    [{'source': 'Darkness Rise', 'target': '暗崛'},
     {'source': 'Dark Matter', 'target': '黑暗物質'},
     {'source': 'Looming Darkness', 'target': '闇黑迫近'},
     {'source': 'Dark Binding', 'target': '暗影禁錮'},
     {'source': 'Shroud of Darkness', 'target': '夜幕庇護'},
     {'source': 'Dark Sphere', 'target': '黑暗星體'},
     {'source': 'Black Shield', 'target': '黑暗之盾'},
     {'source': 'Dark Passage', 'target': '鬼影燈籠'},
     {'source': 'Shadow Dash', 'target': '影襲'},
     {'source': 'Piercing Darkness', 'target': '刺骨幽闇'}]
    """
    ```
    * OK，到這邊萬事俱備只欠東風，接下來就要看看如何將這些元件全部 Chain 在一起啦！

---

## Chain !
讀取資料集的部份與上個小節提供的程式碼是一樣的，這裡不再重複一次。
* 讀取完資料集之後，就要來建立樣板：
    ```python
    from langchain.prompts import PromptTemplate
    from langchain.prompts.few_shot import FewShotPromptTemplate

    # 建立 Example Template
    example_prompt = PromptTemplate(
        input_variables=["source", "target"],
        template="Source: {source}\nTarget: {target}",
    )

    # 建立 Few-Shot Template
    prompt = FewShotPromptTemplate(
        example_selector=example_selector,
        example_prompt=example_prompt,
        suffix="Source: {query}\nTarget: ",
        prefix="請根據以下範例，產生一個中二的技能翻譯。",
        input_variables=["query"],
    )
    ```
* 最後把LLM串上去：
    ```python
    from langchain_openai import OpenAI

    llm = OpenAI(
        api_key="auth-token-ouo123",
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        base_url="http://localhost:8000/v1",
    )

    chain = prompt | llm.bind(stop=["\n"]) # 遇到換行停止生成
    print(chain.invoke("Starburst Stream"))
    ```
* 順便使用 Gradio 建立一個簡單的 Demo 網頁：
    ```python
    import gradio as gr

    def send(source):
        return chain.invoke({"query": source})

    with gr.Blocks() as app:
        source = gr.Textbox(label="Source")
        target = gr.Textbox(label="Target")
        source.submit(send, source, target)
    app.launch()
    ```
* 整個應用使用起來的效果如下：
    ![中二技能翻譯使用範例](images/中二技能翻譯使用範例.png)
* 📝 範例程式碼
    * [筆者程式碼](https://tinyurl.com/llm-note-13)
    * 自己嘗試: .../LLM/project/langchain

---

## 連結
* [LangChain](https://www.langchain.com/)
* [GitHub: LangChain](https://github.com/langchain-ai/langchain)
* [LangChain: Few-Shot Prompt](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples)
* [LangChain: Debugging](https://python.langchain.com/docs/modules/chains/how_to/debugging)
* [LangChain: BM25](https://python.langchain.com/docs/integrations/retrievers/bm25)
* [LangChain: QA Retriever](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa)

---

## 結論
* LangChain 這個框架的確將語言模型應用中的每個元件與行為都封裝的很好，而且使用了相當有趣的 Chain 概念來將元件串聯起來，是個非常獨特的思維。
* 幸好 LangChain 真的很簡單好上手，才沒讓筆者在這篇文章開天窗。
* LangChain 的易用性與靈活性加上活躍的社群，未來發展相當值得關注！
* 其他值得參考的應用框架包含 [Guidance](https://github.com/guidance-ai/guidance), [LMQL](https://lmql.ai/), [Semantic Kernel](https://github.com/microsoft/semantic-kernel) 等等，給大家參考。

---