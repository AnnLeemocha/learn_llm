# ggml & llama.cpp
[ggml](https://github.com/ggerganov/ggml) 是 [ggerganov](https://github.com/ggerganov) 開發的一個機器學習框架，主打純 C 語言、輕量化且可以在 Apple 裝置上執行等功能。

大概 2022 年底的時候，就常常看到 ggml 登上 GitHub Trending 趨勢榜。在 Meta 推出 LLaMA 之後，作者順勢製作了一個 [llama.cpp](https://github.com/ggerganov/llama.cpp) 專案，其高速讀取模型、低硬體需求、極低位元量化等功能深深吸引各大 LLM 使用者，並在 GitHub 上迅速竄紅。

---

## ggml
[ggml](https://github.com/ggerganov/ggml) 是個泛用型的機器學習框架，可以應用在很多模型上。從 ggml 的 GitHub 首頁可以看到除了 [GPT-2](https://github.com/ggerganov/ggml/tree/master/examples/gpt-2) 與 [Llama](https://github.com/ggerganov/llama.cpp) 等純文字的語言模型以外，甚至連 [CLIP](https://github.com/monatis/clip.cpp) 、 [Whisper](https://github.com/ggerganov/ggml/tree/master/examples/whisper) 跟 [Stable Diffusion](https://github.com/leejet/stable-diffusion.cpp) 這些圖片語音相關的模型都有。
* 缺點是 ggml 遇到新的模型架構時，必須自行實做沒看過的模組，通常必須等待作者或其他熱心人士投入開發。
* 補充：
    * 常見的 `.gguf` 就是 ggml 專案衍生出來的模式格式。

---

## llama.cpp
[llama.cpp](https://github.com/ggerganov/llama.cpp) 是 ggml 作者實做 LLaMA 模型架構的專案，但因為 LLaMA 實在太紅了，這份專案甚至開始喧賓奪主，很多 ggml 相關的改動都是從 llama.cpp 流過來的。
* 也因此，有很多 llama.cpp 有的功能或工具，在原生的 ggml 可能都還沒開始支援。
* 因為 llama.cpp 是針對特定模型架構運行的專案，因此並不是所有模型都能透過這個專案操作，但時至今日支援的模型架構已是相當廣泛，除非是非常新穎的架構，不然基本上都是有支援的。
* 接下來我們以 llama.cpp 為主，介紹其環境建置與相關工具的使用。
    * 在此之前，建議可以先下載一個 [Llama 3 GGUF 格式](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF)的模型下來方便做測試。
    * 如果只是想稍微試試可以下載 [Q2_K](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf) 版本，而如果想要效果好一點，建議使用 [Q8_0](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q8_0.gguf) 版本。
    * 如果想使用中文的話，則可以考慮使用 [Breeze](https://huggingface.co/YC-Chen/Breeze-7B-Instruct-v1_0-GGUF) 、 [Vicuna](https://huggingface.co/TheBloke/vicuna-7B-v1.5-GGUF) 或唐鳳上傳的 [Taiwan Llama](https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGUF) GGUF 版模型。

---

## 環境建置
llama.cpp 支援使用 CUDA 在 NVIDIA GPU 上進行運算，因此需要準備一個有包含編譯器 `nvcc` 的環境，才能使用 GPU 進行運算，一般來說只要安裝 CUDA 就會有了。
* 注意：
    * CUDA 的版本可以參考 llama.cpp 專案底下 `.devops` 資料夾內與 CUDA 相關的 Dockerfile，目前是使用 CUDA 11.7.1 版。
    * 但需要注意自身 `gcc` 的版本與 `nvcc` 是否相容。
        * 例如筆者的 `gcc` 版本為 13，那 `nvcc` 就要裝到 12.4 才能用，同時也別忘了GPU 驅動程式的支援版本！
* 除了 CUDA 以外還需要 CMake 編譯工具。
* 以下示範使用 Conda 建立環境：
    ```bash
    conda create -yn ggml python=3.11
    conda activate ggml
    conda install -y nvidia/label/cuda-11.7.1::cuda
    pip install cmake
    conda activate ggml # 確保 cmake 路徑有被系統正確讀取
    ```
* 透過 Git 將 llama.cpp 專案的原始碼複製下來：
    ```bash
    git clone https://github.com/ggerganov/llama.cpp --depth 1
    cd llama.cpp
    ```
* 接著使用 CMake 進行建置：
    ```bash
    cmake -B build -DGGML_CUDA=ON --fresh
    ```
    * 使用 `-DLLAMA_CUBLAS=ON` 參數啟用 CUDA 函式庫，若只想使用 CPU 的話把這個選項移掉就好。
    * 筆者這邊加上 `--fresh` 來強制 CMake 重新建立編譯資訊。
* 在 cmake 執行的過程，確保以下訊息有出現：
    ```txt
    -- Found CUDAToolkit: /home/user/.miniconda3/envs/ggml/include (found version "11.8.89")
    -- cuBLAS found
    -- The CUDA compiler identification is NVIDIA 11.8.89
    ```
    * 如果沒有出現這些訊息的話，可能要確認一下 CUDA 環境是否設置正確，否則建置出來的程式會因為不能使用 GPU 而跑得很慢。
* 最後進行完整建置：
    ```bash
    cmake --build build --config Release -j
    ```
    * 建置過程約需一兩分鐘左右，完成建置後，相關的主程式與工具都會放在 `llama.cpp/build/bin` 底下。
* 補充：
    * llama.cpp 也支援 Windows 及 macOS 作業系統，除了參考 GitHub 的說明自行編譯以外，也可以在 GitHub 的 Releases 頁面找到作者預先編譯好的版本。

---

## 格式轉換
可以用 llama.cpp 執行的模型文件格式被稱為 gguf 格式。
* 先安裝相關套件：
    ```bash
    pip install numpy sentencepiece
    ```
* 使用 llama.cpp 提供的 `convert_hf_to_gguf.py` 工具，將 Hugging Face 的模型轉換成 gguf 格式：
    ```bash
    python convert_hf_to_gguf.py /path/to/llama3-8b-inst \
        --outfile llama3-8b-inst.gguf --vocab-type bpe
    ```
    * 這樣就可以輕鬆做轉換了，而且速度相當快。
    * 另外可以設定 `--outtype` 參數決定資料型態，這個階段支援 `f32`, `f16` 與 `q8_0`，推薦使用 `f16` 比較平衡一點。
* 注意：
    * 在對 Breeze 模型做轉換時，可能會遇到 `ValueError: Vocab size mismatch` 的問題，這是因為 Breeze 模型參數的字典維度與分詞器實際的字典數量不符合的關係，只要加上 `--pad-vocab` 參數來填充字典即可。

---

## 主程式 llama-cli
`llama-cli` 是整個專案最基本的用法，絕大多數的指令參數都放在這裡。
* 首先是 `-m` 指定 gguf 格式的模型路徑，並使用 `-ngl` 指定 GPU 讀取層數：
    * 建議直接設定為 `-ngl 99` 就好，就算超過 GPU 記憶體上限，系統也會自動改放到 CPU 記憶體裡面。
    ```bash
    ./build/bin/llama-cli --help # 顯示參數說明
    ./build/bin/llama-cli -m /path/to/model.gguf -ngl 99
    ```
    * 在沒有其他參數的情況下，程式會快速讀取完模型，然後自己開始瘋狂輸出，這通常是用來測試運作的速度有多快。
* 調用 GPU 時，要確認讀取訊息有出現類似以下的文字：
    ```md
    llm_load_tensors: offloading 32 repeating layers to GPU
    llm_load_tensors: offloading non-repeating layers to GPU 
    llm_load_tensors: offloaded 33/33 layers to GPU
    ```
    * 請確保最後一行寫的是 `33/33` 而不是 `30/33` 之類的，否則就代表有些模型參數沒有被放進 GPU 裡面，而是放在 CPU 裡面做運算，速度會因此大打折扣。
* 如果想要進入互動模式，可以加上 `-if` 參數：
    ```md
    <|begin_of_text|>

    > hi 
    Hello! It seems like you forgot to provide any instructions. Please provide the instructions you'd like me to follow, and I'll do my best to assist you.
    ```
    * 但這樣並不是 Llama 3 標準的聊天模式。
* 可以透過參數 `-cnv` 來啟用聊天模型，並搭配參數 `-p` 來指定系統提示：
    ```bash
    ./build/bin/llama-cli -m /path/to/model.gguf -ngl 99 -cnv \
        -p "You are a helpful assistant. Reply in Traditional Chinese."
    ```
    * 用起來大概會像這樣：
    ```md
    <|begin_of_text|><|im_start|>system
    You are a helpful assistant. Reply in Traditional Chinese.<|im_end|>
    > 你好!
    你好!
    <|im_end|><|eot_id|>

    > 什麼是語言模型?
    語言模型(Language Model)是一種算法，用於模擬人類語言的生成和理解能力。它能夠根據線入的文字或語句，生成相對應的語句或回答，並且可以根據上下文進行理解和生成。
    ```
* 在非互動模式下，會用 `-n` 指定生成多少 Tokens，並搭配 `--ignore-eas` 來評測模型或硬體的速度。
* 透過 `-c` 參數可以指定模型的輸入長度，預設只有 512，但目前的模型都支援到 4096 或 8192 以上了，所以如果有相關應用要消耗大量 Token 的話，這個參數記得一定要開大，不然模型的輸出會看起來很奇怪。
* 在純 CPU 運算時，參數 `--threads` 對速度的影響滿大的，建議開系統總核心數的一半，例如你是 20 核的 CPU，那就指定 `--threads 10` 這樣。
* 另外能透過 `-fa` 來啟用 Flash Attention 加速生成。
* 其他像是 `--top-k`、`--top-p`、`--temp` (也就是Temperature) 等都是老面孔的取樣參數了，除此之外 llama.cpp 還額外支援相當豐富的取樣參數，可以自行研究看看。
* 補充：
    * 其中把 `--temp` 設定為 0.0 並且把 `--repeat-penalty` 設定為 1.0，會最貼近在 HF Transformers 套件裡面設定 `do_sample=False` 的結果。

---

## 量化工具 llama-quantize
量化是 llama.cpp 最受歡迎的功能，llama.cpp 支援的量化方法相當廣泛，從 8-Bit、6-Bit 甚至到 2-Bit、1-Bit 都有，同時也支援混精度、線性、非線性等量化方法。
* 基本的量化方法只要使用 `llama-quantize` 這個工具便可以進行操作：
    ```bash
    ./build/bin/llama-quantize \
        /path/to/model-fp16.gguf \
        /path/to/output.gguf Q4_K_M
    ```
    * 以上指令會將模型量化為 4-Bit 格式，這裡的 `Q4_K_M` 代表「以 `Q4_K` 為基本型態的 Medium 大小」，意思是大部分的模型權重都是 4-Bit 的 `Q4_K` 型態，但是有一部分的權重會用 6-Bit 的 `Q6_K` 型態，因此最後的**平均權重位元 (Bits Per Weight, BPW)** 其實會將近 5.0。
        * 另外也有 `Q4_K_S` 是混合 `Q4K` 與 `Q5K` 的格式，其 BPW 會更貼近 4.0 一些。
    * 透過 `./build/bin/llama-quantize --help` 可以看到，還有許多 `IQ` 開頭的量化方法。
        * 例如極低位元數的 `IQ2_XXS` 與 `IQ1_S` 等。
* 透過另外一個程式 `llama-imatrix` 的協助來使用這些量化方法。
    * `llama-imatrix` 指的是**重要性矩陣 (Importance Matrix)** ，這個程式會透過一份校準資料集來評估哪些模型權重是重要的，這些重要的權重需要保護起來，才能在極低位元的量化下維持模型的表現。
    * 在此之前，需要先蒐集一份純文字資料：
        ```python
        from datasets import load_dataset

        ds_path = "bigscience-data/roots_zh-tw_wikipedia"
        ds = load_dataset(ds_path, split="train", streaming=True)
        text = [item["text"] for _, item in zip(range(128), ds)]
        text = "\n".join(text)
        with open("zh-wiki.txt", "wt", encoding="UTF-8") as fp:
            fp.write(text)
        ```
        * 這裡使用中文維基當作範例，只需要取大概 128 筆資料即可。
    * 接著丟入 `llama-imatrix` 裡面：
        ```bash
        ./build/bin/llama-imatrix \
            -ngl 99 -c 8192 -f zh-wiki.txt \
            -m /path/to/model-fp16.gguf
        ```
        * 大約兩三分鐘的時間就能跑完，並產生一份 `imatrix.dat` 檔案。
* 將這份檔案用在量化工具上，便能產生極低位元的量化模型了：
    ```bash
    ./build/bin/llama-quantize \
        --imatrix imatrix.dat \
        /path/to/model-fp16.gguf \
        /path/to/output.gguf IQ2_XXS
    ```
* 以 Breeze-7B 實測，原本 FP16 為 14 GB，量化成 `Q4_KM` 只剩下 4.3 GB，到了 `IQ2_XXS` 只剩下 2 GB，最極端的 `IQ1_S` 更是只剩下 1.7 GB！但 7B 的模型量化到這個程度，其實已經接近無法使用的狀態了。
* 根據筆者的經驗，8-Bit 和 6-Bit 幾乎不會有損失的感覺，到了 4-Bit 以下才會有明顯感受到不同，而參數量越大的機型，受到量化的影響則越小。
    * 不過這是個人的主觀感受，最後選擇使用多少位元的量化，還是要根據實際應用的狀況進行過充分的評測後再來決定。

---

## 伺服器 llama-server
`llama-server` 是筆者相當喜歡的一個程式，他可以將模型變成一個服務，使用者可以透過 HTTP API 來存取模型。
* 參數與 llama-cli 大致相同，例如：
    ```bash
    ./build/bin/llama-server \
        -m /path/to/model.gguf \
        -ngl 99 -c 8192 -fa -cb -np 4 \
        --host 0.0.0.0 --port 8080
    ```
    * 比較大的不同在於參數 `-cb` 與 `-np`：
        * `-cb` 指的是 Continuous Batching，也就是說使用者的輸入會不斷加入批次裡面，而不需要等整個批次都結束了才能處理下個輸入。
        * `-np` 則是指能夠同時處理的輸入數量，這裡設定 `-np 4` 就代表系統最多能同時處理四個輸入。
* 啟動之後可以在 `http://127.0.0.1:8080/` 打開網頁介面進行互動，這個介面只是用來簡單測試，一般開發通常還是以 API 呼叫居多：
    ```bash
    curl -X POST http://localhost:8080/completion \
        -d '{"prompt": "你好!", "n_predict": 16}'
    # Output: {"content": "今天我們要為大家介紹的是...
    ```
* 可以撰寫一個 Python 程式用串流的方式接收模型輸出：
    ```python
    import json
    import requests

    url = "http://127.0.0.1:8080/completion" 
    prompt = "[INST] 什麼是語言模型? [/INST]"

    params = {
        "prompt": prompt,
        "stream": True,
        "stop": ["\n", "\n\n"],
    }

    resp = requests.post(url, json=params, stream=True) 
    for chunk in resp.iter_lines():
        if not chunk:
            continue
        # 會有固定的 "data:" 前級，需要跳掉 5 個字元
        content = json.loads(chunk[5:])["content"]
        print(end=content, flush=True)
    print()
    ```
    * 這裡透過 `stop` 參數就能指定模型輸出的停止點。
* 為了避免使用者輸入太長的提示，可以透過 `tokenize` 與 `detokenize` API 來截斷使用者的提示，例如：
    ```python
    url = "http://127.0.0.1:8080/tokenize"
    params = {"content": "hello, llama.cpp!"}
    resp = requests.post(url, json=params)
    tokens = json.loads(resp.text)["tokens"]
    print(tokens) # [6312, 28709, 28725,...1
    ```
* 假設我們只需要最後面三個 Tokens 的話：
    ```python
    url = "http://127.0.0.1:8080/detokenize"
    params = {"tokens": tokens[-3:]}
    resp = requests.post(url, json=params)
    content = json.loads(resp.text)["content"]
    print(content) # .cpp!
    ```
    * 這樣就完成了截斷提示長度的組合操作囉！
* 除了以上這些 API 以外，還有使用 LLM 做檢索時能透過 `/embedding` 取得文句向量，以及 Code LLM 常用的 `/infill` 程式碼填充，也有與 OpenAl API 容的 `/v1/chat/completions` 可以使用。

---

## Python Binding
[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) 是 llama.cpp 的 Python 介面。
* 透過以下指令安裝：
    ```bash
    CMAKE_ARGS="-DGGML_CUDA=ON" pip install llama-cpp-python
    ```
* 以下是個簡單的 Streaming 範例：
    ```python
    from llama_cpp import Llama

    llm = Llama(
        model_path="/path/to/model.gguf",
        n_gpu_layers=99,
        verbose=True,
    )

    output = llm(
        "[INST] 什麼是語言模型? [/INST]",
        max_tokens=128,
        stop=["\n", "\n\n"],
        stream=True,
    )

    for token in output:
        print(end=token["choices"][0]["text"], flush=True)
    ```
    * 若想要隱藏原本 llama.cpp 的訊息紀錄，將 verbose 參數設定為 False 即可。
* 用法原則上大同小異，詳細資訊可以參考[官方文件](https://llama-cpp-python.readthedocs.io/en/stable/)。

---

## 速度比較
這個章節要來比較一下 HF Transformers 與 llama.cpp 之間的推論速度。
* 在這邊先附上筆者的硬體設備資訊：
    * CPU： 12th Gen Intel(R) Core(TM) i7-12700K
    * GPU： NVIDIA GeForce RTX 3090
* 測量速度的時候，主要考量以下因素:
    * **預填充 (Prefilling)**： 初始輸入階段，觀察第一個 Token 生成的延遲。
    * **解碼 (Decoding)**： 後續輸出階段，觀察 Token 與 Token 之間的延遲。
    * **延遲 (Latency)**： 只考慮單筆處理的速度。
    * **吞吐量 (Throughput)**： 考慮同時處理多筆的速度。
* 通常會用每秒可以處理多少 Tokens 來當作衡量速度的單位，可表示成單位 `tokens/s` 或簡寫為 `t/s`。
* 以下使用的 HF Transformers 版本為 4.41.1，使用的 llama.cpp 版本為 commit 9b8247，評測模型皆為 Llama 3 8B。
1. Prefill 設定在 4K 左右，而 Decode 設定為 128，首先在 Transformers 上測單筆推論的速度：
    | 方法            | Prefill (t/s) | Decode (t/s) |
    | --------------- | ------------- | ------------ |
    | CPU             | 0.25          | 0.13         |
    | Eager           | 3200          | 41           |
    | SDPA            | 3600          | 42           |
    | FlashAttention2 | 3700          | 43           |
    * 因為 CPU 實在太慢了，4 秒才處理 1 個Token，所以筆者只測了 16 個 Tokens 就放棄了。
    * 整體上來說，GPU 上預填充速度約在 3200 t/s 到 3700 t/s 左右，解碼速度約 40 t/s。
2. 接下來看看 llama.cpp 吧，因為是測單筆，所以不開 `-cb` 跟 `-np` ：
    | 方法     | Prefill (t/s) | Decode (t/s) |
    | -------- | ------------- | ------------ |
    | CPU      | 2.57          | 1.27         |
    | GPU      | 4300          | 48           |
    | GPU + fa | 4900          | 49           |
    * llama.cpp 的 CPU 速度比 HF 快了 10 倍之多！
    * 在 GPU 方面也都比 HF 快了不少。
3. 接下來測試看看不同量化方法的單筆速度：
    | 方法   | Prefill (t/s) | Decode (t/s) |
    | ------ | ------------- | ------------ |
    | IQ1_S  | 3800          | 144          |
    | IQ2_M  | 3600          | 129          |
    | Q2_S   | 3800          | 129          |
    | Q4_K_M | 3800          | 125          |
    | Q8_0   | 3800          | 85           |
    | FP16   | 4900          | 49           |
    * 可以看到有量化的模型在預填充方面明顯比 FP16 慢了些，但是生成解碼就快很多了。
4. 最後簡單比較一下多筆推論的速度差異：
    | 方法             | Prefill (t/s) | Decode (t/s) |
    | ---------------- | ------------- | ------------ |
    | HF Batch Size 1  | 3800          | 42           |
    | HF Batch Size 2  | 4000          | 83           |
    | HF Batch Size 4  | OOM           | 162          |
    | HF Batch Size 8  | OOM           | 313          |
    | HF Batch Size 16 | OOM           | 585          |
    | llama.cpp -np 4  | 4000          | 163          |
    | llama.cpp -np 8  | 3800          | 293          |
    | llama.cpp -np 16 | 4000          | 473          |
    * 如果只看 Decode 這欄的話你可能會覺得「什麼，介紹了老半天的llama.cpp 居然輸了嗎?」
    * 其實這裡的比較方式是不太公平的：
        * 在 HF Transformers 這邊，是預先湊好完整的批次讓模型去做推論 (俗稱 Offline Inference)。
        * 但 llama.cpp 這邊，是分別接收 HTTP Requests 再去動態進行批次推論 (俗稱 Online Inference)。
    * 因此算上網路傳輸及一些零零總總的小開銷，Online 會輸給 Offline 其實是滿合理的，但因為 HF Transformers 沒有 Online Inference 的機制所以也只能這樣比較。
    * 不過最大的問題在於 HF Transformers 推到批次大小為 4 的時候就超出記憶體上限了！
    * 因此實際上也沒辦法開到這麼大，在這情況下選擇 llama.cpp 會相對理想一點。
* llama.cpp 只做單筆推論時，Prefill 可以達到 4900 t/s，但是多筆推論時大概都只有 4000 t/s 出頭，可能是因為早期 llama.cpp 比較著重在單筆推論的開發上，而多筆推論的功能相對較新一點，也許未來的更新還會繼續提昇 llama.cpp 的速度，但現階段來說 llama.cpp 還不算是個擅長多筆推論的框架，主要還是偏重在單機運行上。
* 📝 範例程式碼
    * [筆者速度評測程式碼](https://tinyurl.com/llm-note-11)
    * 自己嘗試: .../LLM/project/ggml_llama_cpp

---

## 連結
* [GitHub: ggml](https://github.com/ggerganov/ggml)
* [GitHub: llama.cpp](https://github.com/ggerganov/llama.cpp)
*[ GitHub: llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

---

## 結論
* 本章節介紹了ggml 框架與熱門的 llama.cpp 專案。
* 雖然 llama.cpp 在推論速度上與 HF Transformers 似乎旗鼓相當，但是 llama.cpp 在 gguf 格式轉換、讀取速度與量化表現都相當優秀，是個相當適合部署在單機本地端上運行的框架。
* 像是 [LM Studio](https://lmstudio.ai/) 與 [llamafile](https://github.com/Mozilla-Ocho/llamafile) 都是基於 llama.cpp 的專案，目標也都是為了在單機本地端上跑語言模型。
* 除了本章節介紹的程式以外，在 llama.cpp 裡面還提供了相當多工具可以使用，例如 `llama-perplexity` 可以用來計算模型困惑度，甚至還有能用來進行模型訓練的 `llama-finetune` 與 `llama-train-from-scratch` 等等。
* 如果想要更深入瞭解 llama.cpp 的程式碼流程，推薦可以先看看 `11ama-simple` 的程式碼。
* 其他更多的說明可以到 `examples` 資料夾底下探索。

---