# OpenAI API
能透過 OpenAI API 調用 ChatGPT 的功能，讓我們能夠在自己的開發應用裡面借助 ChatGPT 的力量。

---

## Token
是==切割文本的最小單位==，有些中文翻譯成「詞元」，可以想像是字或詞的概念。
* 一句話會如何被**分詞 (Tokenize)** 取決於**分詞器 (Tokenizer)** 如何被訓練，並不一定是依照空格或字元邊界切開。
    * "photography" ➜ "photo" 和 "graphy" ➜ 兩個 Subwords ➜ 佔用兩個 Tokens
* 在 UTF-8 裡面中日韓表意文字 (CJK Characters) 通常由 3 個 Bytes 所組成。因為中日韓的文字相當多，通常不會全部包含在 BPE (Byte-Pair Encoding) Tokenizer 的字典裡面，因此經常需要將沒見過的 UTF-8 字元 Fallback (倒退) 拆解成 Byte 來表示。
    * 一般情況下，中文（或中日韓文字）消耗的 Token 用量會比英文多的多。
    * 例如在英文中 "apple" 通常只需要 1 個 Token 來表示，而蘋果則可能需要 4 ~ 6 個 Tokens 來表示。

---

## Tiktoken
Tiktoken 是 OpenAI 的 Tokenizer 套件 (估計是致敬 TikTok 的)，同樣也是使用 BPE Tokenizer。([套件連結](https://github.com/openai/tiktoken))
* 在操作 ChatGPT API 的過程，我們需要透過此套件來==計算 Token 用量==，來精準控制以避免超出模型的輸入長度，或者預防使用者任意輸入過長的句子。
* 📝 範例程式碼
    * 完整程式碼 (無)
    * 自己嘗試: .../LLM/project/tiktoken
* 在 BPE Tokenizer 的訓練過程中，為了減少字典大小，部分不常見的 UTF-8 字元會被拆解成 Bytes。
    * 在某個程度上避免了傳統 NLP 上的困擾"中文斷詞錯誤"引起的問題。
    * 也具有處理未知詞的能力
    * 缺點: 代價是更龐大的參數量與更難收斂的模型。
        * 因此，一些以中文為主的語言模型，可能會考慮擴大其字典，納入更多中文字，這樣可以減少中文 Token 的使用量。
* 比較 GPT-3.5 與 GPT-4 Tokenizer 的差異
    * 兩個模型根本使用相同的 Tokenizer，而這個 Tokenizer 的名稱是 cl100k_base。
    * 因此這部份與 tiktoken.get_encoding("cl100k_base") 的操作是等價的。

    | 編碼名稱            | OpenAI 模型                                      |
    | ------------------- | ------------------------------------------------ |
    | cl100k_base         | gpt-4, gpt-3.5-turbo, text-embedding-ada-002     |
    | p50k_base           | Codex models, text-davinci-002, text-davinci-003 |
    | r50k_base (or gpt2) | GPT-3 models like davinci                        |
    * 詳細的 Tokenizer 名稱可以參考[官方範例](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
* 若要相當精準的考慮 Chat Format 對 Token 數量的影響，可以參考[官方的 ChatGPT Prompt 格式範例](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)。
    * 其關鍵在於每次 ChatGPT 回覆都會增加 <|start|>assistant<|message|> 這三個固定前綴 Tokens，因此 Token 消耗量需要額外 +3 上去。

---

## 定價
為什麼這個 Token 這麼重要呢，除了對模型效能的影響以外，ChatGPT API 本身的計價方式，就是以 Token 數量計價的。
* 詳細的計價方式，請參考:
    * [官方網站](https://openai.com/pricing)
    * [OpenAI API 價格計算器](https://invertedstone.com/calculators/openai-pricing?ref=producthunt)
* 以 2024 年 5 月份的價格的價格為例
    | 模型 (Model)           | 上下文長度上限 (Context) | 輸入 (Input)        | 輸出 (Output)       |
    | ---------------------- | ------------------------ | ------------------- | ------------------- |
    | gpt-3.5-turbo-0125     | 16K                      | US$0.0005/1K Tokens | US$0.0015/1K Tokens |
    | gpt-4-turbo-2024-04-09 | 128K                     | US$0.01/1K Tokens   | US$0.03/1K Tokens   |
    * 上下文長度上限越高，收費越貴。
    * 假設中文平均占用 2 ~ 3 個 Tokens，那 1000 個 Tokens 約 300 ~ 500 個中文字，輸出價格則貴三倍。
    * 專案開發請記得==估算成本==。
* 特別注意：
    * 根據[官方所述](https://platform.openai.com/docs/deprecations/instructgpt-models) GPT-3 已經是即將被棄用的模型，其效果與 GPT-3.5 差距是很大的，價格差距也很大。
    * 根據[官方文件](https://platform.openai.com/docs/deprecations/2023-07-06-gpt-and-embeddings)所述，建議使用 gpt-3.5-turbo-instruct 取代 Completions API 的 text-davinci-003 模型。
    * 在套用別人的專案時，請格外注意，使用的模型是否已被棄用。

---

## ChatGPT API
在使用 API 之前，需要先註冊帳號並取得 API 金鑰。使用 API 時，建議多多參考[官方文件](https://platform.openai.com/docs/api-reference/introduction)。
* Python 版的 OpenAI API 套件
    ```python!
    pip install openai
    ```

### 認證
* api_key (OPENAI_API_KEY)
    1. 程式碼裡面寫死
    1. 設定成環境變數
    1. 指定檔案路徑
* organization (OPENAI_ORG_ID), project (OPENAI_PROJECT_ID)
    * 有加入公司組織之類的，記得要設定 Organization 資訊，不然帳單就不是報公帳，而是算在你頭上
* 📝 範例程式碼
    * 自己嘗試: .../LLM/project/chatgpt_api

### 基礎用法 Non-Streaming
開始串接 ChatGPT API
* 參數
    * `model`: 指定要用哪個模型。
    * `messages`: 傳送給模型處理的訊息，接收一個陣列，陣列裡面每個元素代表每個回合的對話內容。
        * `role`: 該訊息的角色，主要有 `system`, `user`, `assistant` 三種角色。
* 傳送訊息的角色 `role`
    * `system`: 前面提過的 System Prompt 概念
    * `user`: 使用者的輸入
    * `assistant`: 模型的輸出
* 額外參數來控制生成結果
    * `max_tokens`: 設定最多輸出幾個 Tokens。
    * `stop`: 模型輸出遇到什麼字串需要停下來。
    * 取樣參數: `temperature`, `top_p`
* 📝 範例程式碼
    * 自己嘗試: .../LLM/project/chatgpt_api

### 串流用法 Streaming
加上參數 `stream=True` 即可用串流的方式接收輸出，而 `response` 也會因此變成一個 Generator 物件。
* 這樣就會看到模型輸出一個字一個字的跑出來，這種顯示輸出方式，較能感受到模型跟連線都還「活著」的感覺。
* 建議
    * 如果使用者會直接接觸到這些輸出，那有 Streaming 會讓使用者比較感受的到回饋。
    * 如果單純只是應用端的邏輯處理，那只需要使用 Non-Streaming 即可。
* 📝 範例程式碼
    * 自己嘗試: .../LLM/project/chatgpt_api

### CLI Chat Demo (文字介面聊天範例)
可以將使用者輸入與模型輸出不斷加到 messages 裡面來達到==多輪對話==的效果。
* 但如果訊息太長或者要做成本控制，我們就需要結合 Tiktoken 套件來截斷輸入。
    * 在截斷的過程記得保留系統提示，並且從最舊的訊息開始截斷。
* 📝 範例程式碼
    * [筆者程式碼](https://github.com/penut85420/LLM-Note-Labs/blob/main/OpenAI-API/CLI-Chat-Demo.py)
    * 自己嘗試: .../LLM/project/chatgpt_api

### 文字向量 (Embedding) API
將文本轉成文字向量 (Sentence Embedding) 來使用。
* Embedding 在 NLP 裡面扮演相當重要的角色，尤其在==資訊檢索 (Information Retrieval)== 領域裡面相當實用。
* Embedding API 可以同時處理很多個句子，減少傳送處理的時間，因此會比一句一句傳送快的多。
* Embedding 不僅能用來比較文本之間的相似度，也具有跨語言的能力，在未來提到 Retrieval-Based 應用時會相當重要。
* 常見用法:
    * 比對文本之間的相似度
      例如透過計算 Embedding 之間的 [歐式距離 (Euclidean Distance)](https://w.wiki/YdM) 來觀察跨語言文本之間的相似度。
        * 歐式距離數字越小代表越相近，也可以換成 [餘弦相似度 (Cosine Similarity)](https://w.wiki/neY) 或 [向量內積](https://w.wiki/A3Jz) 之類的評估公式。
* 📝 範例程式碼
    * 自己嘗試: .../LLM/project/chatgpt_api
        * [text-embedding-3-small](https://platform.openai.com/docs/guides/embeddings/embedding-models) 這個模型最大輸入長度可以達到 8192 個 Tokens，每個向量的維度是 1536 維。

### Rate Limits
使用 Open API 時，其用量除了受限於開發者的荷包以外，官方也有限制 ==API 的存取速率==。
[官網連結](https://platform.openai.com/docs/guides/rate-limits/overview)
* 文字模型常用單位:
    * Requests Per Minute (RPM) : 每分鐘可以存取 API 的次數上限。
        * 如果發送大量短訊息，那就有可能先踩到 RPM 的上限。
    * Tokens Per Minute (TPM) : 每分鐘可以要求 API 處理的 Token 數量上限。
        * 如果每次 Request 都發送很大量的文本，那就有可能先踩到 TPM 的上限。
* 用量達上限官方並不會把你封鎖起來，而是回傳一個達到速率上限的錯誤訊息，所以記得在程式碼中捕捉超速錯誤，並延遲一段時間再發送一次。
    * 例如有些實驗可能會透過 Multi-Threading 的方式發送 Requests。
    * 超速錯誤:
      `openai.error.RateLimitError` ➜ `Error: Rate limit reached for ...` 。
* 📝 範例程式碼
    * 自己嘗試: .../LLM/project/chatgpt_api
        * 範例程式碼中的作法僅供參考，也可以使用其他有 retry 功能的套件來處理，這部份可以參考[官方文件](https://platform.openai.com/docs/guides/rate-limits/error-mitigation)的推薦。


### API Usage
開發者可以到 OpenAI 的 [Usage 頁面](https://platform.openai.com/account/usage)查看 API 的使用量與目前累積的花費。
如果整合好 API 的應用正式上線後，可以透過此頁面監控使用量與計費，並分析應用的成本。

---

## Bonus
OpenAI 其實不只有提供語言模型的服務，還有 Whisper 語音辨識以及 DALL-E 圖像生成等功能。

### Whisper
[Whisper](https://openai.com/research/whisper) 是 OpenAI 訓練的一個語音轉文字 (Speech To Text, STT) 模型，採用 Encoder-Decoder 的 Transformer 架構。
* 具有跨語言辨識的能力，使用方式為上傳音檔。
* 音檔格式支援廣泛，最高上傳 25 MB 大小。
    * [官網資訊連結](https://platform.openai.com/docs/guides/speech-to-text/introduction)
        * 筆者建議轉成 Mp3 格式，比較節省流量也能減少網路傳輸時間。
        * 可以透過 FFmpeg 轉換格式。
* 辨識速度相當的快，600 秒的音檔只要 30 秒就能完成辨識，是 Real-Time 的 20 倍快。筆者實際使用 RTX 3090 做辨識，約莫也是 8 ~ 10 倍快而已。
* 除了能做傳統的語音辨識以外，還能直接將語音翻譯成其他語言。
* 計價方式: [以官方網站公佈的價目為主]
    * 每辨識一分鐘的音檔便收取 $0.006 鎂的費用，辨識十分鐘的音檔約莫就是新台幣 1 ~ 2 塊錢。
    * Whisper 的模型權重是有開源的
        * 相比於 ChatGPT 這種語言模型而言，語音辨識的權重要來的小很多。
        * 需要佔用的 GPU Memory 約 6 GB 左右，若擁有不錯的 GPU 可以嘗試自己運行 Whisper 模型，在需求量不大的情況下會相對經濟一點。
* 可以參考的資源：
    * [HuggingFace](https://huggingface.co/openai/whisper-large-v2)
        * OpenAI 官方公佈的模型權重。
    * [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
        * 使用 GGML 框架的 Whisper 實做，主要使用 C++ 串接。
    * [Whisper Desktop](https://github.com/Const-me/Whisper)
        * 基於 whisper.cpp 製作的程式，推薦給 Windows 使用者。
    * [Faster Whisper](https://github.com/guillaumekln/faster-whisper)
        * 基於 CTranslate2 的 Whisper 實做，可以用 Python 串接。

### DALL-E
[DALL-E](https://openai.com/research/dall-e) 是 OpenAI 的圖像生成模型，雖然知名度可能不如 Stable Diffusion 之類的，但也是個可供參考的服務。
* 生成一張 256 x 256 大小的圖片，大約需要 5 ~ 8 秒左右。
* 除了一般的圖像生成以外，還有圖片編輯 (Edits) 與變體 (Variations) 等功能，詳細用法請參考[官方文件](https://platform.openai.com/docs/guides/images)。
* 計價方式: [以官方網站公佈的價目為主]
    * 按圖收費，每張 256x 大小的圖片收取 0.016 鎂，而最大的 1024x 每張圖片則收取 0.02 鎂。

---

## 參考
* [OpenAI Docs: API Reference](https://platform.openai.com/docs/api-reference)
* [OpenAI Docs: Rate Limits](https://platform.openai.com/docs/guides/rate-limits)

---

## 結論
引自 [InstructGPT](https://arxiv.org/abs/2203.02155) 的論文。
> 如果將如此龐大的語言模型直接開源，將會很難限制使用者的不當用途。但如果語言模型的存取限制太高，這樣一來，普羅大眾將很難受益於這項先進科技。因此其中一個可行的方法是透過 API 的形式提供服務，由 API Provider 來負責監控不當使用的情況。
* 缺點
    * 無法連網的裝置、使用者的隱私問題等等。
* 優點
    * 低廉的價格與存取的便利性。
    * 可以專注在上層的應用開發，而不需要煩惱硬體設備不足的問題，只要保持網路暢通。

---