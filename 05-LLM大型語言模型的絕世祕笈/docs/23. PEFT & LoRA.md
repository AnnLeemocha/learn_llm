# PEFT & LoRA
在單張消費級顯卡上**全微調 (Full Fine-Tuning, FFT)** 一個 7B 參數量以上的模型是相當困難的，這時神秘的笑臉再次出手拯救了我們。
* 由 Hugging Face 開發的 **[PEFT](https://github.com/huggingface/peft) (Parameter-Efficient Fine-Tuning)** 套件集合了許多具有**參數效率**的微調方法，讓我們可以不做 FFT 也能對一個高參數量的模型進行訓練。
* 其中 [LoRA](https://arxiv.org/abs/2106.09685) 就是個相當受歡迎的方法，本章節將以 LoRA 為主，介紹 PEFT 的訓練方式。

---

## LoRA簡介
* 在原本的 HF Transformers 套件中，一般的訓練流程大致如下：
    ```python
    # 讀取模型
    model = ModelCls.from_pretrained(...)

    # 設定參數
    train_args = TrainingArguments(...)
    trainer = Trainer(model, ...)

    # 開始訓練
    trainer.train()
    trainer.save_model()
    ```
* PEFT 的訓練方法可以很輕鬆的整合進去：
    ```python
    # 讀取模型
    model = ModelCls.from_pretrained(...)

    # 使用 PEFT LoRA
    peft_config = LoraConfig(...)
    model = get_peft_model(model, peft_config)

    # 設定參數
    train_args = TrainingArguments(...)
    trainer = Trainer(model, ...)

    # 開始訓練
    trainer.train()
    trainer.save_model()
    ```
* 只需要加上兩個操作，就能輕鬆使用 PEFT 進行訓練。完整讀取模型的程式碼如下：
    ```python
    import torch
    from peft import LoraConfig, TaskType, get_peft_model
    from peft.tuner.lora import LoraModel
    from transformers import LlamaForCausalLM as ModelCls

    # 讀取 Model
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
    model: ModelCls = ModelCls.from_pretrained(model_name)

    # 讀取 Peft Model
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules["q_proj", "k_proj"],
        modules_to_save=["embed_tokens"],
    )
    model: LoraModel | ModelCls = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # all params: 8,033,669,120
    # trainable params: 3,407,872
    # trainable%: 0.0424
    ```
    * 最後的 `model.print_trainable_parameters()` 可以告訴我們這個模型總共有多少參數，其中會拿來訓練的參數量又有多少。
可以看到原本擁有8B參數量的模型，在 LoRA 的加持下只剩下 3.4M 的參數需要訓練，僅佔原本模型的 0.042% 而已，不到千分之一的比例。
* 原本訓練一個大型語言模型，會消耗的 GPU 記憶體為：
    ```md
    大模型權重 + 超巨大梯度
    ```
* 對一個 7B 模型做 FFT 少說也要 40~80 GB 左右！權重本身僅佔 15GB 左右，因此權重以外的東西至少多佔了兩倍左右。
* 而 LoRA 會凍結原本的模型，並在旁邊放一個小模型來訓練，因此套上 LoRA 之後就變成：
    ```md
    大模型權重 + 超迷你模型 + 超迷你梯度
    ```
* 因為實際上只需要訓練這個 LoRA 小模型，所以需要計算的梯度也就跟著變小。

---

## LoRA 原理
LoRA 是由 Microsoft 提出的一種訓練方法，全名為 Low-Rank Adaptation， 其核心概念是**將大模型的權重凍結起來**不去訓練它，並在旁邊放一個小模型。
* 進行運算時**將大模型與小模型的輸出合併**，但更新權重時**只更新小模型的權重**，因此只需要算小模型的小梯度就好，大幅減少記憶體需求。
* LoRA 架構示意圖：

    **(中文版)**
    ![LoRA 架構示意圖](images/LoRA%20架構示意圖.png)
    
    [**HF Blog: TRL-PEFT**](https://huggingface.co/blog/trl-peft)
    ![LoRA 架構示意圖 (HF Blog: TRL-PEFT)](images/LoRA%20架構示意圖%20(HF%20Blog%20-%20TRL-PEFT).gif)

---
## FFT 運算
* 先來看看原本的 FFT 如何進行運算：
    1. 假設我們的 Batch Size 為 `4`
    2. 假設原本的模型參數為 `100x100` 的矩陣 `W`
    3. 那輸入便是 `4x100` 的矩陣 `I`
    4. 推論時計算 `I(4x100) x W(100x100) = O1(4x100)`
    5. 結果會是一個 `4x100` 的輸出矩陣 `O1`
    * 這個過程中，需要更新的矩陣 `W` 有 `100x100 = 10,000` 的參數量。

---

## LoRA 運算
* LoRA 會根據原本的權重，額外產生兩個矩陣 A 跟 B，這兩個矩陣被稱為 **Adapter**，運算時會將原本權重的輸出與 Adapter 的輸出相加，合併成新的輸出：
    1. 設定一個參數 `r` 為 `8`
    2. 根據此參數，將原本 `100x100` 的矩陣拆成：
        a. `100x8` 的矩陣 `A`
        b. `8x100` 的矩陣 `B`
    3. 推論時計算變成 `IxAxB`
        a. `I(4x100) x A(100x8) = C(4x8)`
        b. `C(4x8) x B(10x100) = O2(4x100)`
    4. 結果一樣是一個 `4x100` 的輸出矩陣 `O2`
    5. 將兩個輸出矩陣 `O1(4x100)` 與 `O2(4x100)` 相加，同樣是 `4x100` 的矩陣
* 因此將矩陣 `M` 拆成 `A`, `B` 兩矩陣在理論上是可行的，且需要更新的參數量變成：
    1. `A(100x8) = 800`
    2. `B(8x100) = 800`
* 加起來總共 1,600 的參數量，比原本的 10,000 少了超過八成的參數量！
* 能夠減少的參數量會根據一開始設定的 `r`，而有所不同，`r` 設定的越低，Adapter 的參數量就越小，需要更新的權重就越少，反之亦然。

---

## 合併運算
LoRA 還有一個很重要的特性，就是將 Adapter 與原始模型權重合併之後，並不會增加模型整體的參數量！
* (線性代數) 為什麼將一個大模型與小模型合併卻不會增加參數量呢？又為什麼形狀不一樣的兩個 `AB` 小矩陣可以跟大矩陣 `W` 合併呢？
    * 讓我們來仔細分析：
        1. 套上 LoRA 進行推論時，計算為 `IxW + IxAxB`
        2. 根據分配律將 `I` 提出來，改成 `Ix(W + AxB)`
            a. `A(100x8) x B (8x100) = W' (100x100)`
            b. `W(100x100) + W' (100x100) = W" (100x100)`
        3. 合併運算變成 `IxW"`
    * 所以 `AB` 兩矩陣就是這樣跟大矩陣 `W` 合併在一起的，而且 `W` 與 `W"` 的形狀是一樣的，因此最後整體參數量並不會增加。

---

## 回顧 LoRA 程式碼
* 瞭解 LoRA 的運作原理之後，再來重新回顧一次 LoRA 的設定檔：
    ```python
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules["q_proj", "k_proj"],
        modules_to_save=["embed_tokens"],
    )
    ```
    * 透過 `task_type` 可以指定不同的任務類型，LoRA 不只能用在 Decoder 的訓練上，包含 Encoder 跟 Seq2Seq 模型都是可以使用 LoRA 訓練的。
    * `LoraConfig` 裡面的 `r` 就是剛才介紹的 `r` 參數，用來決定 Adapter 的大小， 也會一定程度的影響訓練效果。
        * 但其實滿多研究指出，這個 `r` 的影響並不是很大，影響最大的還是訓練資料的品質，所以通常就選個 8、16 或 32 之類順眼的數字用就好。
    * `lora_alpha` 則會決定 Adapter 的影響程度，也就是說 Alpha 值越高，越容易把大模型既有的能力給覆蓋掉。
    * 而 `lora_dropout` 就與一般的 Dropout 概念相同，訓練過程中隨機拋棄部份輸出，是用來對抗過度擬合用的參數。
    * `target_modules` 可以選擇要進行 LoRA 訓練的權重模組，例如 `q_proj` 與 `k_proj` 就是注意力層裡面 Query 與 Key 線性轉換層的權重。
    * 而 `modules_to_save` 則可以設定要額外存放的權重，例如除了 Adapter 本身以外，可能還會把 Embedding Layer 打開來訓練，那就可以用這個選項讓權重跟著 Adapter 一起存下來。
* 補充：
    * 通常，模型權重就是一份字串對應到 `torch.Tensor` 的字典喔！
    * 所以如果你想要檢查 LoRA 是否有正確訓練在目標權重上，或者額外存放的權重是否正確，可以將檢查點 (Checkpoint) 底下的 `adapter_model.safetensors` 讀出來看看：
        ```python
        from safetensors.torch import load_file

        adapter = load_file("path/to/adapter_model.safetensors")

        for k in adapter.keys():
            print(k)
        ```
    * `safetensors` 是 Hugging Face 官方主推的模型格式，目的是增加權重儲存格式的安全性，避免程式碼注入攻擊並提昇讀寫效率，同時支援 Numpy、PyTorch、Tensorflow 等眾多框架。
* 接下來就能用 LoRA 對模型進行訓練囉！筆者這裡使用 Llama 38B 進行訓練，提醒大家因為Llama 3 與 TinyLlama 使用的分詞器並不一樣，所以資料要記得重新斷詞喔！

---

## LoRA 推論
* 完成 LoRA 訓練後，會得到一份 Adapter 權重，透過 HF Transformers 套件進行推論時，使用 PEFT 讀取模型的方式如下：
    ```python
    import torch
    from peft.peft_model import PeftModelForCausalLM as PeftCls
    from transformers import LlamaForCausalLM as ModelCls

    base_model_path = "meta-llama/Meta-Llama-3-8B-Instruct"
    lora_model_path = "Models/TwAddr-LoRA"

    base_model = ModelCls.from_pretrained(
        base_model_path,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )

    lora_model: PeftCls = PeftCls.from_pretrained(
        base_model,
        lora_model_path,
        torch_dtype=torch.bfloat16,
    )
    ```
* 進行文本生成的用法與原本的 CausalLM 大致相同：
    ```python
    from transformers import LlamaTokenizerFast as TkCls
    from transformers import TextStreamer

    tk: TkCls = TkCls.from_pretrained(base_model_path)
    ts = TextStreamer(tk)

    inputs = tk("Hello, ", return_tensors="pt").to("cuda")

    lora_model.generate(**inputs, max_new_tokens=128, streamer=ts)
    ```

---

## QLoRA
[**QLoRA**](https://github.com/artidoro/qlora) 是由 BitsAndBytes [作者 Tim Dettmers 提出的方法](https://arxiv.org/abs/2305.14314)，它的原理很簡單，就是把原本參數凍結的模型量化到 4-Bit 大小，這樣就能進一步減少 GPU 記憶體的消耗，在單張 24GB 的顯卡上甚至能微調參數量 30B 的模型！
* 因此在 HF Transformers 套件裡面使用 QLoRA 的方式也相當簡單，只要在讀取原始模型時放上 `BitsAndBytesConfig` 即可：
    ```python
    from peft import get_peft_model
    from transformers import LlamaForCausalLM as ModelCls
    from transformers import BitsAndBytesConfig

    # 設定量化選項
    bnb_config = BitaAndBytesConfig(load_in_8bit=True)

    # 讀取模型
    model_id = "meta-llama/Llama-2-13b-chat-hf"
    model: ModelCls = ModelCls.from_pretrained(
        model_id,
        device_map="auto",
        quantization_config=bnb_config,
    )

    model = get_peft_model(model, ...)
    ```
* 後面照正常訓練流程就可以囉！以 24GB 的 GPU 來說，通常 13B 的模型用 8-Bit 就能訓練，而 30B 的模型要用 4-Bit 才能訓練，但 70B 以上可能就沒辦法了。
* 補充：
    * 除了 BitsAndBytes 加 LoRA 以外，其實也可以用 GPTQ 或 AWQ 加 LoRA 做訓練喔！
    * 但是在後續做權重合併的時候，還沒有一個很方便的方法可以將已量化的模型與 LoRA Adapter 做合併。
    * 真的想這麼做的朋友，可以考慮先將量化後的模型反量化回 FP16 的格式後再去做合併，但是 GPTQ 跟 AWQ 反量化的方法都太硬派了，本書就不多做介紹了。

---

## 權重合併
完成 LoRA 訓練之後產生的 Adapter 權重非常小，通常只有 20MB 左右而已。
* 如果要評估使用 LoRA 訓練的模型，可以使用 vLLM 的 LoRA 推論功能：
    ```python
    from vllm import LLM, SamplingParams
    from vllm.lora.request import LoRARequest

    model_id = "meta-llama/Llama-3-8B-Instruct"
    llm = LLM(model_id, dtype="float16", enable_lora=True)

    sampling_params = SamplingParams(...)
    lora = LoRARequest("TwAddr", 1, "Models/TwAddr-LoRA")
    outputs = llm.generate(
        prompts,
        sampling_params,
        lora_request = lora,
    )
    ```
    * 這樣就能輕鬆評估帶有 LoRA Adapter 的模型了！
* 但是這裡會遇到一個問題，如果訓練時你有使用 `modules_to_save` 這個設定的話，vLLM 會拒絕你使用這種 Adapter 來進行推論，因此會需要把 Adapter 與原始模型做合併：
    ```python
    import torch
    from peft import PeftModel
    from transformers import LlamaForCausalLM as ModelCls
    from transformers import LlamaTokenizerFast as TkCls

    # 指定模型路徑
    orig_model_path = "meta-llama/Llama-3-8B-Instruct"
    lora_model_path = "Models/TwAddr-LoRA"
    output_dir = "Models/TwAddr-Merged"

    # 讀取原本的模型
    base_model = ModelCls.from_pretrained(
        orig_model_path,
        torch_dtype=torch.float16,
    )

    # 讀取 Peft 模型
    lora_model: PeftCls = PeftModel.from_pretrained(
        base_model,
        lora_model_path,
        torch_dtype=torch.float16,
    )

    # 將 LoRA 權重合併到原本的模型裡面並存下來
    lora_model = lora_model.merge_and_unload()
    lora_model.save_pretrained(output_dir)

    # Tokenizer 也要跟著另外存一份
    tk: TkCls = TkCls.from_pretrained(orig_model_path)
    tk.save_pretrained(output_dir)
    ```
    * 如此一來，就能獲得完整權重的合併模型了！
* 合併完之後的模型就可以跟一般的模型一樣操作，例如轉換成 .gguf 格式或是進行 GPTQ 訓練等等。
* 注意：
    * 要特別注意，即便你是使用 QLoRA 的方式進行訓練，在合併權重時也必須用 FP16 或者 BF16 來讀取原本的模型，因為 LoRA Adapter 是不能合併到一個 INT8 模型上的。
* 那這個使用 LoRA 微調的 Llama 38B 效果如何呢？
    ```md
    Accuracy: 100.00%
    ```
* 居然全對啦！這時上個章節過度擬合的恐懼浮現心頭，於是來自由測試看看：
    ```md
    > 你好！
    你好！
    > 使用繁體中文解釋什麼是語言模型？
    語言模型(Language Model)是一種人工智慧 ...        
    ```
    * 好險，看來原本的能力並沒有被破壞！
* 由此可見，使用 FFT 去訓練一個模型，受限於 GPU 記憶體只能訓練到 1B 或 3B 的參數量，但效果是不如我們用 LoRA 去訓練一個 7B 模型的。
* 雖然 LoRA 在同參數量下，訓練效果沒辦法比 FFT 好，但是可以在硬體成本與模型效能之間達到一個理想的平衡點。
* 除此之外，使用 LoRA 也比較不容易過度擬合，但也有人認為 LoRA 學的較少，所以忘的較少，不過這也未必是個壞事！

---

## 訓練細節
* 通常參數量越大，學習率會設定的越低，但是因為 Adapter 的參數量很少，所以使用 LoRA 訓練時，學習率會設定的比較高。
* 筆者通常會從 `1e-4` 或 `4e-5` 開始嘗試。
* 除此之外，把 Alpha 值調高也是個提昇學習效果的方法。
* 雖然有很多人指出 LoRA 的訓練效果不如 FFT 來得好，但 FFT 除了訓練成本高以外，對於剛接觸訓練的開發者而言，也是比較容易因為過度擬合而導致訓練失敗的方法，必須多累積一些實戰經驗才會比較知道如何避開。
* 而 LoRA 訓練成本低、速度快，且既有的模型權重依然保留，還能發揮一部分的影響力，所以 LoRA 訓練比較不容易失敗，有時更容易得到相對理想的結果。
* 不過這也只是筆者的自身經驗談而已，到底使用哪個方法比較好，取決於任務的設計、難度與硬體環境。
* LoRA 本身也有很多變種做法，可以透過設定檔去嘗試。
* 而且在 PEFT 裡面其實也不只收錄了 LoRA 這種方法而已，另外還有 P-Tuning 和 (IA)3 之類的訓練手法，大家也都可以去嘗試看看。

---

## 連結
* [GitHub: PEFT](https://github.com/huggingface/peft)
* [HF Docs: LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)
* [HF Blog: TRL-PEFT](https://huggingface.co/blog/trl-peft)
* [HF Blog: FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp)
* [arXiv Paper: LoRA](https://arxiv.org/abs/2106.09685)
* [arXiv Paper: QLoRA](https://arxiv.org/abs/2305.14314)

---

## 結論
* 本章節介紹了偉大笑臉製作的 PEFT 套件以及 LoRA 訓練方法，使得高參數量模型的訓練更加平易近人，對於推動整個開源社群參與大型語言模型的開發相當有幫助。
* 加上量化技術加持的 QLoRA 訓練法，更是進一步減少了訓練時的記憶體消耗，使得單卡訓練更大規模的語言模型不再是空談。
* 即便如此，到這裡依然會遇到另外一個問題。雖然使用 LoRA/QLoRA 能夠跑得動這個玩具實驗，但是如果把訓練資料長度拉長時，記憶體又再度爆開了。
    * 筆者實測用 Llama 2 13B 4-Bit 訓練到 1K Tokens 以上就不行了，但 1K、 2K 的長度對現在的應用而言侷限性實在太大。
    * 若想要再往上訓練，該怎麼做呢？下個章節就來探討如何訓練長文本的模型。

---