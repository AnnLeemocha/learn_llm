# Text Generation Inference (TGI)
[Text Generation Inference](https://github.com/huggingface/text-generation-inference) ç°¡ç¨± TGIï¼Œæ˜¯ç”± Hugging Face é–‹ç™¼çš„èªè¨€æ¨¡å‹æ¨è«–æ¡†æ¶ã€‚å…¶ä¸­æ•´åˆäº†ç›¸ç•¶å¤šæ¨è«–æŠ€è¡“ï¼Œä¾‹å¦‚ Flash Attentionã€Paged Attentionã€ Continuous Batching ç­‰ç­‰ï¼ŒåŒæ™‚ä¹Ÿæ”¯æ´ BitsAndBytesã€GPTQã€AWQ ç­‰é‡åŒ–æ–¹æ³•ï¼ŒåŠ ä¸Š Hugging Face åœ˜éšŠå¼·å¤§çš„é–‹ç™¼èƒ½é‡èˆ‡æ´»èºçš„ç¤¾ç¾¤åƒèˆ‡ï¼Œä½¿ TGI æˆç‚ºéƒ¨ç½²èªè¨€æ¨¡å‹æœå‹™çš„æœ€ä½³é¸æ“‡ä¹‹ä¸€ã€‚

---

## æ¶è¨­æœå‹™
TGI æä¾›æœ¬åœ°å®‰è£èˆ‡ Docker Image å…©ç¨®ç”¨æ³•ï¼Œæ ¹æ“šç­†è€…çš„ç¶“é©—ï¼Œæœ¬åœ°å®‰è£è¦èŠ±ä¸Šéå¸¸ä¹…çš„æ™‚é–“ä¾†ç·¨è­¯èˆ‡å®‰è£ï¼Œè€Œä¸”ä¹Ÿå¾ˆå®¹æ˜“é‡åˆ°ç’°å¢ƒé…ç½®çš„å•é¡Œï¼Œé™¤éè¦å®¢è£½åŒ–æŸäº›åŠŸèƒ½ä¹‹é¡çš„ï¼Œä¸ç„¶ä¸€å¾‹æ¨è–¦ä½¿ç”¨ Docker Image æ“ä½œã€‚
* è£œå……ï¼š
    * æœ¬ç« ç¯€ä¸»è¦ä½¿ç”¨ TGI çš„ Online Serving åŠŸèƒ½ï¼ŒçœŸçš„éœ€è¦åš Offline æ¨è«–çš„æœ‹å‹ï¼Œæ¨è–¦ä½¿ç”¨ Docker Image æ­é… VS Code çš„ Container Development åŠŸèƒ½ï¼Œå°±èƒ½åœ¨ Docker è£¡é¢æ“ä½œ Offline çš„ TGI å›‰ï¼
    * ä½†å› ç‚ºTGI ä¸»è¦é¢å‘ Online Serving çš„é—œä¿‚ï¼ŒOffline çš„ä»‹é¢ä¸¦æ²’æœ‰å¾ˆå¥½ç”¨ï¼Œç›¸é—œçš„èªªæ˜æ–‡ä»¶ä¹Ÿåå°‘ã€‚
* å¯ä»¥å¾å®˜æ–¹ [GitHub Packages](https://tinyurl.com/lim-tgi-pkgs) æŸ¥çœ‹ TGI çš„ Docker Image æœ‰å“ªäº›ç‰ˆæœ¬ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ `latest`ï¼š
    ```bash
    docker pull ghcr.io/huggingface/text-generation-inference:latest
    ```
    * ç­†è€…ç›®å‰ä½¿ç”¨çš„æ˜¯ `sha-f871f11` ç‰ˆï¼Œå› ç‚ºé€™å€‹ Docker Image å¤§å°ç´„ 10 GB å·¦å³ï¼Œæ‰€ä»¥ä¸‹è¼‰éœ€è¦èŠ±ä¸Šä¸€æ®µæ™‚é–“ã€‚
* å®Œæˆä¸‹è¼‰ä¹‹å¾Œå¯ä»¥é€é--help æŸ¥çœ‹åƒæ•¸èªªæ˜ï¼š
    ```bash
    docker run --m \
        ghcr.io/huggingface/text-generation-inference:latest --help
    ```
* å¯ä»¥å…ˆç”¨ GPT-2 ç°¡å–®æ¸¬è©¦ä¸€ä¸‹ï¼š
    ```bash
    docker run --gpus all --shm-size lg \
        -p 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-generation-inference \
        --model-id openai-community/gpt2
    ```
    * ç­‰åˆ°è¨Šæ¯ç´€éŒ„å‡ºç¾ `Connected` å°±ä»£è¡¨æœå‹™å•Ÿå‹•å®Œæˆï¼Œå› ç‚º TGI æ˜¯ç”¨ FastAPI å»ºæ§‹çš„ï¼Œæ‰€ä»¥èƒ½åœ¨`http://localhost:8080/docs/` åº•ä¸‹æŸ¥çœ‹ç›¸é—œçš„ API Endpoint èˆ‡åƒæ•¸ã€‚
* æ¥ä¸‹ä¾†é€é `curl` åšæ¸¬è©¦ï¼š
    ```bash
    curl -X POST 127.0.0.1:8080/generate \
        -d '{"inputs":"Hello, "}' \
        -H 'Content-Type: application/json'
        
    # å¾—åˆ°é¡ä¼¼ä»¥ä¸‹çš„è¼¸å‡º
    # {"generated text": "I'm a new player and ..."}
    ```
* TGI æœƒé€éç¶²è·¯ä¸‹è¼‰æ¨¡å‹ï¼Œä¸¦ä¸”æŠŠæ¨¡å‹è½‰æ›æˆ Safetensors çš„æ ¼å¼ï¼Œç„¶å¾Œå°‡æ¨¡å‹æ¬Šé‡å­˜åœ¨å®¹å™¨å…§éƒ¨çš„ `/data` è·¯å¾‘è£¡é¢ï¼Œæ‰€ä»¥ä½¿ç”¨åƒæ•¸ `-v $PWD/data:/data` å°‡æ­¤è·¯å¾‘å°æ‡‰å‡ºä¾†ï¼Œé€™æ¨£å°±ä¸ç”¨æ¯æ¬¡åŸ·è¡Œçš„æ™‚å€™ï¼Œéƒ½è¦é‡æ–°ä¸‹è¼‰ä¸€éŠæ¨¡å‹æ¬Šé‡ã€‚åéä¾†èªªï¼Œä¹Ÿå¯ä»¥å…ˆå°‡æ¨¡å‹ä¸‹è¼‰å¥½ï¼Œç„¶å¾Œå†æ›è¼‰é€²å®¹å™¨è£¡é¢ï¼Œä¾‹å¦‚ï¼š
    ```bash
    MODEL_PATH="./models/opt-125m"

    git clone https://huggingface.co/facebook/opt-125m $MODEL_PATH

    docker run --gpus all-shm-size lg \
        -p 8080:80 - SPWD/SMODEL PATH:/SMODEL PATH
        ghcr.io/huggingface/text-generation-inference \
        --model-id /$MODEL_PATH
    ```

---

## åŸºæœ¬ç”¨æ³•
TGI çš„ API è©³ç´°ç”¨æ³•å¯ä»¥åƒè€ƒ[å®˜æ–¹æ–‡ä»¶](https://huggingface.github.io/text-generation-inference/)ï¼Œèˆ‡ Transformers ä¸€æ¨£æœ‰ `do_sample`ã€`temperature`ã€`top_k`ã€`top_p` çš„å–æ¨£åƒæ•¸å¯ä»¥ä½¿ç”¨ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå°‡ `decoder_input_details` å’Œ `details` è¨­ç‚º `true` æœƒå›å‚³ä¸€äº›è©³ç´°è³‡è¨Šï¼Œä¾‹å¦‚è¼¸å…¥æˆ–è¼¸å‡ºçš„ Token ç¸½æ•¸ã€‚çµ¦ `stop` åƒæ•¸ä¸€å€‹å­—ä¸²åˆ—è¡¨å¯ä»¥æ§åˆ¶è¼¸å‡ºåœæ­¢é»ã€‚
* TGI æœ€æ–¹ä¾¿çš„æ˜¯å¯ä»¥è¨­å®š `truncate` åƒæ•¸ï¼Œè®“ç³»çµ±å¹«ä½ æŠŠå¤ªé•·çš„æç¤ºæˆªæ–·ï¼Œä¾‹å¦‚è¨­å®šç‚º 500 çš„è©±ï¼Œå°±æœƒæŠŠæç¤ºå‰é¢è¶…é 500 å€‹ Tokens çš„å…§å®¹å…¨éƒ¨åˆ‡æ‰ï¼Œåªç•™æœ€å¾Œé¢çš„ 500 Tokens ç•¶è¼¸å…¥ã€‚
* è£œå……ï¼š
    * é€™åœ¨é™åˆ¶è¼¸å…¥é•·åº¦å¾ˆå¥½ç”¨ï¼Œä¾‹å¦‚åš Retrieval-Based In-Context Learning æ™‚ï¼Œå¯ä»¥åœ¨æª¢ç´¢éšæ®µå–éå¸¸å¤šçµæœå‡ºä¾†ï¼ŒæŠŠç›¸ä¼¼åº¦è¼ƒä½çš„æ“ºåœ¨å‰é¢ï¼Œè—‰ç”± `truncate` åƒæ•¸è‡ªç„¶çš„æŠŠç›¸ä¼¼åº¦å¤ªä½çš„ç¯„ä¾‹çµ¦åˆ‡æ‰ï¼Œé€™æ¨£å°±èƒ½åœ¨ç¶­æŒè¼¸å…¥é•·åº¦ä¸æœƒè¶…å‡ºç³»çµ±é™åˆ¶çš„åŒæ™‚ï¼Œç›¡å¯èƒ½çš„åŠ ä¸Šæ›´å¤šç¯„ä¾‹åœ¨æç¤ºè£¡é¢ã€‚
* æœ€å¾Œé€é Python å‘¼å«ç”Ÿæˆ API çš„ç¨‹å¼ç¢¼å¤§è‡´å¦‚ä¸‹ï¼š
    ```python
    import json
    import requests

    url = "http://localhost:8080/generate"
    params = {
        "inputs": "This is a long story maybe, ",
        "parameters": {
            "best_of": 1,
            "details": True,
            "return_full_text": True,
            "decoder_input_details": True,
            "truncate": 4, #åªä¿ç•™æœ€å¾Œå››å€‹ Tokens
            "max_new_tokens": 128,
            "stop": ["\n", "."],
            "do_sample": True,
            "temperature": 0.5,
            "top_k": 10,
            "top_p": 0.95,
        },
    }

    resp = requests.post(url, json=params)
    result = json.loads(resp.content)
    print(result)
    ```
* é€™è£¡è¨­å®š `truncate=4` ä¾†è§€å¯Ÿå¯¦éš›æ•ˆæœï¼Œå› ç‚ºæœ‰æ‰“é–‹ `return_full_text` çš„é¸é …ï¼Œæ‰€ä»¥å›å‚³çµæœæœƒåŒ…å«æç¤ºèˆ‡ç”Ÿæˆï¼Œé›–ç„¶çµæœè£¡é¢çœ‹èµ·ä¾†æç¤ºä¸¦æ²’æœ‰è¢«è¼‰æ–·ï¼Œä½†è‹¥æ˜¯ä»”ç´°è§€å¯Ÿ `details` è£¡é¢çš„ `prefill` å°±å¯ä»¥çœ‹åˆ°å¯¦éš›çš„æç¤ºå…¶å¯¦æ˜¯å¾ `"maybe"` é–‹å§‹çš„ã€‚
* é™¤äº†åŸºæœ¬çš„ `/generate` ä»¥å¤–ï¼ŒTGI ä¹Ÿæœ‰æä¾› `/v1/chat/completions` é€™é¡èˆ‡ OpenAI API ç›¸å®¹çš„ç”¨æ³•ã€‚

---

## é‡åŒ–é¸é …
TGI æ”¯æ´çš„é‡åŒ–æ–¹æ³•ç›¸ç•¶å¤šå…ƒï¼Œå¯ä»¥é€é `--quantize` ä¾†æŒ‡å®šè¦ä½¿ç”¨å“ªä¸€ç¨®ï¼Œç›®å‰ TGI æ”¯æ´ [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) 8-Bit/4-Bitã€[GPTQ](https://github.com/PanQiWei/AutoGPTQ)ã€[AWQ](https://github.com/mit-han-lab/llm-awq)ã€[EETQ](https://github.com/NetEase-FuXi/EETQ)ã€FP8 ç­‰é‡åŒ–æ–¹æ³•ï¼š
* èªªæ˜ï¼š
    * `bitsandbytes`ï¼š 8-Bit é‡åŒ–ï¼Œé›–ç„¶æ¨è«–é€Ÿåº¦åæ…¢ï¼Œä½†æ”¯æ´ç›¸å°å»£æ³›èˆ‡ç©©å®šã€‚
    * `bitsandbytes-nf4`ï¼š 4-Bit é‡åŒ–ï¼Œå¤§éƒ¨åˆ†çš„æ¨¡å‹éƒ½å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤é¸é …ï¼Œè³£æ–™å‹æ…‹ç‚º Normal Float 4.
    * `bitsandbytes-fp4`ï¼š 4-Bit é‡åŒ–ï¼Œèˆ‡ `bitsandbytes-nf4` é¡ä¼¼ï¼Œä½†ä½¿ç”¨æ¨™æº–çš„ 4-Bit æµ®é»æ•¸è³‡æ–™å‹æ…‹ã€‚
    * `gptq`ï¼š 4-Bit é‡åŒ–ï¼Œéœ€è¦å…ˆé€²è¡Œ GPTQ é‡åŒ–æ‰èƒ½ä½¿ç”¨æ­¤é¸é …ã€‚å¯ä»¥åˆ° HF Hub ä¸Šæœå°‹ï¼Œä¾‹å¦‚ [TheBloke æä¾›çš„ GPTQ æ¨¡å‹](https://huggingface.co/models?search=thebloke%20gptq)ã€‚
    * `awq`ï¼š 4-Bit é‡åŒ–ï¼ŒåŒæ¨£ä¹Ÿéœ€è¦å…ˆé€²è¡Œ AWQ é‡åŒ–æ‰èƒ½ä½¿ç”¨çš„é¸é …ã€‚å¯ä»¥åƒè€ƒ [TheBloke æä¾›çš„ AWQ æ¨¡å‹](https://huggingface.co/models?search=thebloke%20awq)ã€‚
    * `eetq`ï¼š 8-Bit é‡åŒ–ï¼Œèˆ‡ `bitsandbytes` ä¸€æ¨£æ˜¯å¯ä»¥ç›´æ¥å¥—ç”¨åœ¨ä¸€èˆ¬æ¨¡å‹ä¸Šçš„é‡åŒ–é¸é …ï¼Œä½†æ˜¯é€Ÿåº¦æ›´å¿«ã€æº–ç¢ºç‡æ›´é«˜ï¼Œç¼ºé»æ˜¯å•Ÿå‹•æ™‚é–“æ¯”è¼ƒä¹…ã€‚
    * `fp8`ï¼š æ˜¯ NVIDIA æœ€æ–°æ”¯æ´çš„æ¨™æº–å…«ä½å…ƒæµ®é»æ•¸è³‡æ–™å‹æ…‹ï¼Œä½†æ˜¯è¦ Hopper æ§‹ä»¥ä¸Šçš„ GPU æ‰èƒ½è·‘ï¼Œåƒæ˜¯ H100 ä¹‹é¡çš„ã€‚
* æœ‰äº†é‡åŒ–ï¼Œæˆ‘å€‘å–®é¡¯å¡å¹³æ°‘å°±å¤©ä¸‹ç„¡æ•µäº†ï¼å…ˆæ‹¿å€‹ [Taiwan LLM 13B](https://huggingface.co/yentinglin/Taiwan-LLaMa-v1.0) ç†±ç†±èº«ï¼š
    ```bash
    docker run --gpus all --shm-size lg \
        -P 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-generation-inference \
        --model-id yentinglin/Taiwan-LLM-138-v2.0-chat \
        --quantize bitsandbytes
    ```
* é †åˆ©è·‘èµ·ä¾†ä¹‹å¾Œï¼Œç°¡å–®æ‹¿å€‹ä¸­æ–‡æ¸¬è©¦ï¼š
    ```bash
    curl -X POST 127.0.0.1:8080/generate \
        -d '{"inputs": "USER: ä»€éº¼æ˜¯èªè¨€æ¨¡å‹? </s>ASSISTANT: "}' \
        -H 'Content-Type: application/json'
    # Output: {"generated_text": "èªè¨€æ¨¡å‹æ˜¯ä¸€ç¨®..."}
    ```
    * å¥½æ¬¸ï¼Œæˆ‘å€‘é †åˆ©åœ¨ä¸€å¼µ 24GB çš„é¡¯å¡ä¸Šé‹è¡Œäº†ä¸€å€‹ 13B çš„æ¨¡å‹å•¦ï¼
    * å› ç‚º TGI åƒæ•¸å¾ˆå¤šå¾ˆè¤‡é›œï¼Œæ–¼æ˜¯ç­†è€…å°‡å¸¸ç”¨çš„åƒæ•¸æ•´ç†æˆä¸€ä»½ Python Script æ”¾åœ¨æ­¤ [GitHub Gist](https://gist.github.com/penut85420/54765b2ed8a3e2e127ffdc0caf2dcaab) ä¸Šæ–¹ä¾¿ä½¿ç”¨ï¼Œæ­¤è…³æœ¬åƒ…ä¾›åƒè€ƒï¼Œè«‹ä»¥è‡ªèº«é‹è¡Œçš„ç’°å¢ƒèˆ‡ TGI ç‰ˆæœ¬ç‚ºä¸»ã€‚
* è£œå……ï¼š
    * é™¤äº†ä¸€èˆ¬çš„ Decoder èªè¨€æ¨¡å‹ä»¥å¤–ï¼Œåƒæ˜¯ [T5](https://huggingface.co/t5-small) æˆ– [M2M](https://huggingface.co/facebook/m2m100_418M) ä¹‹é¡çš„ Encoder-Decoder æ¶æ§‹ä¹Ÿå¯ä»¥ç”¨é€é TGI ä¾†é‹è¡Œã€‚
    * ä½†å¦‚æœä¸æ˜¯ TGI ç‰¹åˆ¥æ”¯æ´çš„æ¨¡å‹ï¼Œå¯èƒ½æœƒå›åˆ°åŸæœ¬ HF Transformers çš„å¯¦åšã€‚
    * ä¸€äº›åƒæ˜¯ Tensor-Parallel æˆ– Flash Attention çš„æ©Ÿåˆ¶å°±ç„¡æ³•ç”¨åˆ°ï¼Œä½†ä¾ç„¶æœ‰ Continuous Batching å’Œ Streaming Outputs é€™äº›åŠŸèƒ½å¯ä»¥ç”¨ã€‚
    * å› æ­¤è‹¥è¦éƒ¨ç½² Seq2Seq æ¨¡å‹å¯ä»¥è€ƒæ…®ä½¿ç”¨ TGI ä¾†æ¶è¨­æœå‹™ã€‚

---

## Token æ•¸é‡è¨­å®š
è‹¥è¦æ›´ç²¾ç´°çš„éƒ¨ç½²æ¨¡å‹ï¼Œå‰‡ Token ç›¸é—œçš„åƒæ•¸è‡³é—œé‡è¦ã€‚å°æ–¼å°é¡¯å¡è€Œè¨€ï¼Œ å¯ä»¥æ¸›å°‘è¨˜æ†¶é«”æ¶ˆè€—ï¼Œå°æ–¼å¤§é¡¯å¡è€Œè¨€ï¼Œæ›´èƒ½å……åˆ†åˆ©ç”¨è¨˜æ†¶é«”ç©ºé–“ã€‚
* æœ€ä¸»è¦çš„ä¸‰å€‹åƒæ•¸å¦‚ä¸‹ï¼š
    * `--max-input-length` ï¼šä»£è¡¨å–®ç­†è¼¸å…¥çš„æœ€å¤§é•·åº¦ã€‚
    * `--max-total-tokens` ï¼šå–®ç­†è¼¸å…¥åŠ è¼¸å‡ºçš„æœ€å¤§ç¸½é•·åº¦ã€‚
    * `--max-batch-prefill-tokens` ï¼šä»£è¡¨æ‰€æœ‰æ‰¹æ¬¡åŠ èµ·ä¾†çš„æœ€å¤§è¼¸å…¥é•·åº¦ã€‚
* å…¶ä¸­ `--max-batch-prefill-tokens` æ˜¯å½±éŸ¿è¨˜æ†¶é«”æ¶ˆè€—æœ€é—œéµçš„åƒæ•¸ï¼Œä¸åƒ…å½±éŸ¿ `Prefill` éšæ®µèƒ½å¤ å®¹ç´å¤šå°‘ Token ç•¶è¼¸å…¥ï¼Œä¹Ÿæœƒå½±éŸ¿å¯¦éš›é‹ä½œæ™‚èƒ½åŒæ™‚è™•ç†å¹¾å€‹éœ€æ±‚ã€‚
* æ±ºå®šé€™äº›åƒæ•¸å€¼çš„æ–¹å‘å¤§æ¦‚åˆ†æˆè¼¸å…¥é•·åº¦ã€è¼¸å‡ºé•·åº¦èˆ‡æ‰¹æ¬¡å¤§å°ã€‚åœ¨ä¸åŒæƒ…å¢ƒä¸‹ï¼Œé€™äº›è¨­å®šæœƒä¸å¤ªä¸€æ¨£ã€‚
* ä»¥Llama 2 13B ç‚ºä¾‹ï¼Œæ­¤æ¨¡å‹æ˜¯ä»¥ 4K Context Window é€²è¡Œè¨“ç·´çš„ï¼Œé‚£å¯ä»¥åˆ†é… 3K çµ¦è¼¸å…¥ï¼Œå‰©ä¸‹ 1K çµ¦è¼¸å‡ºï¼Œå› æ­¤å¾—åˆ°ï¼š
    1. `--max-input-length` ç‚º `3000`
    2. `--max-total-tokens` ç‚º `3000 + 1000 = 4000`
* è£œå……ï¼š
    * ä¸€èˆ¬è€Œè¨€ Context Length æ‰€è¬‚çš„ 2K æˆ– 4K é€šå¸¸æ˜¯ä»¥ 1024 ç‚ºå–®ä½ï¼Œæ‰€ä»¥ 3K/1K çš„åˆ†é…å¯¦éš›ä¸Šæœƒæ˜¯ **3*1024=3072** è·Ÿ **1024** å€‹ Tokensã€‚
* æ¥ä¸‹ä¾†ï¼Œå‡è¨­ç³»çµ±è¦**åŒæ™‚è™•ç† 4 ç­†è¼¸å…¥**ï¼Œå‰‡å¯ä»¥è¨­å®šåƒæ•¸:
    3. `--max-batch-prefill-tokens` ç‚º 3000 * 4 = 12000
    4. `--max-concurrent-requests` ä»£è¡¨ä½‡åˆ—ä¸­æœ€å¤šç­‰å¾…è™•ç†çš„éœ€æ±‚æ˜¯å¹¾ç­†ï¼Œé è¨­ç‚º 128 ç­†ã€‚
* åœ¨é€™æ¨£çš„è¨­å®šä¸‹ï¼Œå¦‚æœä½¿ç”¨è€…æ¯ç­†è¼¸å…¥éƒ½æ˜¯ 3000 å€‹ Tokensï¼Œ TGI æœ€å¤šåªæœƒåŒæ™‚æ¨è«– 4 å€‹è«‹æ±‚ï¼Œå‰©ä¸‹è«‹æ±‚çš„éƒ½æœƒè¢«æ”¾åœ¨ä½‡åˆ—è£¡é¢ï¼Œç­‰å¾…ä»»ä½•ä¸€å€‹ç”ŸæˆçµæŸä¹‹å¾Œï¼Œå†å¾ä½‡åˆ—æ‹¿ä¸€å€‹éœ€æ±‚å‡ºä¾†æ”¾é€²æ‰¹æ¬¡è£¡é¢ç¹¼çºŒç”Ÿæˆã€‚
* å¦å¤–å¯ä»¥å°‡ `--max-best-of` è¨­ç‚º `1` å°±å¥½ï¼Œå› ç‚ºä¸€èˆ¬æ‡‰ç”¨é€šå¸¸åªéœ€è¦ç”Ÿæˆä¸€ç­†çµæœã€‚
* æœ€å¾Œä½¿ç”¨ bitsandbytes-nf4 åš 4-Bit é‡åŒ–ï¼Œæ•´å€‹æŒ‡ä»¤çœ‹èµ·ä¾†æœƒåƒé€™æ¨£ï¼š
    ```bash
    docker run -gpus all --shm-size lg \
        -P 8080:80 -v $PWD/data:/data \
        ghcr.10/huggingface/text-generation-inference:latest \
        --model-id meta-llama/Llama-2-13b-chat-hf \
        --quantize bitsandbytes-nf4 \
        --max-best-of 1 \
        --max-concurrent-requests 128 \
        --max-input-length 3000 \
        --max-total-tokens 4000 \
        --max-batch-prefill-tokens 12000
    ```
* é›–ç„¶è¨­å®šæ‰¹æ¬¡å¤§å°ç‚º 4ï¼Œä¸é TGI å…¶å¯¦æ˜¯å‹•æ…‹æ±ºå®šç•¶ä¸‹è¦åŒæ™‚æ¨è«–çš„æ•¸é‡ã€‚
    * ä¾‹å¦‚ä½¿ç”¨è€…æ¯ç­†è¼¸å…¥éƒ½æ¸›ç‚º 1500 å€‹ Tokens çš„è©±ï¼Œé‚£å¯èƒ½å°±æœƒè®ŠæˆåŒæ™‚æ¨è«– 8 ç­†ã€‚
    * å› æ­¤åœ¨ä¸æœƒè¶…å‡º GPU è¨˜æ†¶é«”çš„æƒ…æ³ä¸‹ï¼ŒæŠŠåƒæ•¸ `--max-batch-prefill-tokens` ç›¡å¯èƒ½é–‹å¤§ï¼Œä¾¿èƒ½ææ˜‡æ•´é«”ç³»çµ±çš„ååé‡ã€‚
* è‡³æ–¼å…·é«”åˆ°åº•è¦èª¿åˆ°å¤šå°‘æ‰ä¸æœƒè¶…å‡ºè¨˜æ†¶é«”å‘¢?
    * æ ¹æ“šç¡¬é«”èˆ‡æ¨¡å‹æ¶æ§‹çš„ä¸åŒï¼Œå„è‡ªéƒ½æœƒæœ‰ä¸åŒçš„ä¸Šé™ï¼Œæ‰€ä»¥åªèƒ½è‡ªå·±æ…¢æ…¢æ¸¬è©¦äº†ã€‚
    * ç­†è€…ç¿’æ…£ä»¥ 2K ç‚ºå–®ä½å¾€ä¸Šéå¢åšæ¸¬è©¦ï¼Œå¦‚æœ 4K ä¸æœƒçˆ†å°±è©¦ 6Kï¼Œå¦‚æœ 6K ä¸æœƒçˆ†å°±è©¦ 8Kï¼Œä¾æ­¤é¡æ¨ã€‚

---

## TGI Client
TGI Client æ˜¯ç”¨ä¾†è·Ÿ TGI Server äº¤æµçš„å®¢æˆ¶ç«¯å¥—ä»¶ï¼Œå› ç‚ºåœ¨ TGI çš„æ–‡ä»¶ä¸­æ²’æœ‰å¤ªå¤§ç¯‡å¹…çš„ä»‹ç´¹ï¼Œæ‰€ä»¥ç›¸ç•¶å®¹æ˜“è¢«äººéºå¿˜ã€‚
* é¦–å…ˆé€é `pip` å®‰è£ `text-generation` å¥—ä»¶:
    ```bash
    pip Install text-generation
    ```
* åŸºæœ¬ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š
    ```python
    from text_generation import Client

    client = Client("http://127.0.0.1:8080", timeout=600)

    resp = client.generate(
        "Hello, ",
        max_new_tokens=16,
        stop_sequences=["."],
        do_sample=False,
        truncate=2048,
    )

    print(resp.generated_text)
    ```
* å…¶å¯¦å°±æ˜¯å°‡ `requests` çš„ç”¨æ³•åšå€‹åŒ…è£ï¼Œä¸¦ä¸”å¹«å›æ‡‰çš„ç‰©ä»¶å®šç¾©äº†æ˜ç¢ºçš„é¡åˆ¥ï¼Œå› æ­¤è¼¸å…¥ `resp.` çš„æ™‚å€™ï¼Œæœƒè·³å‡º `generated_text` çš„æç¤ºï¼Œé–‹ç™¼çš„æ™‚å€™æœƒæ›´æ–¹ä¾¿ä¸€é»ã€‚
* é€éé€™å€‹å¥—ä»¶é€²è¡Œä¸²æµè¼¸å‡ºä¹Ÿæ¯”è¼ƒæ–¹ä¾¿ï¼š
    ```python
    resp = client.generate_stream(
        "Hello",
        max_new_tokens=16,
        stop_sequences=["."],
        do_sample=False,
        truncate=2048,
    )

    for chunk in resp:
        print(end=chunk.generated_text, flush-True)

    print()
    ```
* æ›´å¤šè©³ç´°çš„ç”¨æ³•ï¼Œå¯ä»¥åƒè€ƒå®˜æ–¹åœ¨ PyPl ä¸Šçš„[ä»‹ç´¹é é¢](https://pypi.org/project/text-generation/)ã€‚

---

## é€Ÿåº¦æ¸¬è©¦
* åŒæ¨£ä½¿ç”¨ä¸Šå€‹ç« ç¯€çš„è©•æ¸¬ç¨‹å¼ä¾†æ¸¬é‡ TGI èˆ‡ llama.cpp ä¹‹é–“çš„é€Ÿåº¦å·®ç•°ï¼š
    | æ–¹æ³•             | Prefill (t/s) | Decode (t/s) |
    | ---------------- | ------------- | ------------ |
    | TGI-1T           | 4000          | 49           |
    | TGI-4T           | 4000          | 189          |
    | TGI-8T           | 4200          | 372          |
    | TGI-16T          | 4200          | 686          |
    | llama.cpp        | 4900          | 49           |
    | llama.cpp -np 4  | 4000          | 163          |
    | llama.cpp -np 8  | 3800          | 293          |
    | llama.cpp -np 16 | 4000          | 473          |
    * 1Tã€4T è¡¨ç¤ºåŒæ™‚è·‘ä¸€ç­†æˆ–å››ç­†ï¼Œå¯ä»¥çœ‹åˆ°åœ¨å–®ç­†æ¨è«–æ™‚ï¼Œé€Ÿåº¦èˆ‡ llama.cpp å·®ç•°ä¸¦ä¸å¤§ï¼Œä½†æ˜¯ç•¶åŒæ™‚è·‘çš„ç­†æ•¸è¶Šä¾†è¶Šå¤šçš„æ™‚å€™ï¼Œå…©è€…çš„é€Ÿåº¦å·®è·å°±è¶Šä¾†è¶Šå¤§äº†ã€‚
* æ¥ä¸‹ä¾†æ¸¬è©¦çœ‹çœ‹ TGI æ”¯æ´çš„å„ç¨®é‡åŒ–æ–¹æ³•ï¼š
    | ğŸ’¸         | 1T            | 1T           | 16T           | 16           |
    | ---------- | ------------- | ------------ | ------------- | ------------ |
    | ğŸ’¸         | Prefill (t/s) | Decode (t/s) | Prefill (t/s) | Decode (t/s) |
    | FP16       | 4000          | 49           | 4200          | 686          |
    | EETQ 8-Bit | 4000          | 92           | 5000          | 292          |
    | BNB 8-Bit  | 4000          | 29           | 4100          | 259          |
    | BNB NF4    | 3700          | 80           | 4100          | 1000         |
    | GPTQ 4-Bit | 5700          | 137          | 6500          | 1000         |
    | AWQ 4-Bit  | 2400          | 108          | 2300          | 1346         |
    | FP8        | ğŸ’¸            | ğŸ’¸           | ğŸ’¸            | ğŸ’¸           |
    * BitsAndBytes çš„ Decode é€Ÿåº¦çœŸçš„æ»¿æ…¢çš„ï¼Œç›¸è¼ƒä¹‹ä¸‹ EETQ çš„ç”Ÿæˆé€Ÿåº¦å°±å¿«ä¸Šè¨±å¤šï¼Œä½† EETQ çš„ç†±èº«æ™‚é–“è¼ƒä¹…æ˜¯å€‹å°ç¼ºé»ã€‚
    * åœ¨å–®ç­†æ¨è«–æ™‚ï¼ŒGPTQ çš„ Decode é€Ÿåº¦ç•¥å‹ AWQï¼Œä½†æ˜¯å¤šç­†æ¨è«–æ™‚ï¼ŒAWQ çš„é€Ÿåº¦å°±è´é GPTQ äº†ã€‚
    * å› æ­¤è‹¥æ˜¯ç¶“å¸¸éœ€è¦è¼¸å…¥é•·æ–‡æœ¬çš„æ‡‰ç”¨ï¼Œæ¡ç”¨ GPTQ é‡åŒ–æœƒæ¯”è¼ƒå¥½;ä½†è‹¥æ˜¯çŸ­å•çŸ­ç­”ï¼Œ è¿½æ±‚ç”Ÿæˆé€Ÿåº¦ï¼Œé‚£é¸æ“‡ AWQ è¼ƒä½³ã€‚
    * è‡³æ–¼ FP8ï¼Œç­‰ç­†è€…å‡ºæ›¸å¾Œæš¢éŠ·å¤§è³£è³ºå¤§éŒ¢äº†ï¼Œå†ä¾†è²·å° H100 å¹«å¤§å®¶æ¸¬è©¦çœ‹çœ‹

### ç¿»è­¯æ‡‰ç”¨
ç­†è€…æ›¾ç¶“ä½¿ç”¨ TGI + Taiwan Llama æ›¿äººç¿»è­¯éä¸€ç¯‡ç´„ 13000 Tokens çš„è‹±æ–‡æ–‡ç« ï¼Œä»¥ä¸‹åˆ†äº«é€™å€‹æ‡‰ç”¨çš„å¯¦åšã€‚
* é¦–å…ˆç­†è€…å…ˆå°‡æ–‡ç« å­˜åœ¨ `content.txt` è£¡é¢ï¼Œä¸¦æ‰‹å‹•åˆ†æ®µï¼Œç´„å…©ä¸‰è¡Œçš„å…§å®¹å°±æ”¾å…©å€‹æ›è¡Œã€‚å› ç‚ºæ–‡ç« å…§å®¹æ²’æœ‰å¾ˆé•·ï¼Œæ‰€ä»¥æ‰‹å‹•åˆ†æ®µé‚„ç®—å¯ä»¥ï¼Œä¸»è¦æ˜¯ç‚ºäº†ç¢ºä¿é‚Šç•Œæ­£ç¢ºã€‚ä½†å¦‚æœæ‡‰ç”¨åœ¨æ›´é•·çš„æ–‡ç« ä¸Šï¼Œå¯èƒ½éœ€è¦è€ƒæ…®ä¸€äº›è‡ªå‹•å°‹æ‰¾ Boundary çš„å·¥å…·å”åŠ©äº†ã€‚
* æ¥ä¸‹ä¾†ç”¨ TGI å°‡ Taiwan Llama æ¶èµ·ä¾†ï¼Œä½¿ç”¨ BNB-NF4 é‡åŒ–ï¼Œæ¥è‘—æ’°å¯«ä»¥ä¸‹ç¨‹å¼ç¢¼é€²è¡Œç¿»è­¯ï¼š
    ```python
    import json
    from concurrent.futures import ThreadPoolExecutor

    import requests
    from tqdm import tqdm


    def main():
        # è®€å–æ–‡ç« ä¸¦ä»¥ "\n\n" åˆ‡æˆå¤šå€‹ Chunks
        with open("content.txt", "rt", encoding="UTF-8") as fp:
            text = fp.read().strip()
            text = text.split("\n\n")

        # Taiwan Llama æä¾›çš„ Prompt Template
        template = "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: è«‹å°‡ä»¥ä¸‹å¥å­å¾è‹±æ–‡ç¿»è­¯æˆä¸­æ–‡: {} ASSISTANT:"

        # å®šç¾©æ¯æ®µ Chunk çš„ç¿»è­¯å‡½å¼
        def translate(source):
            prompt = template.format(source)
            target = generate(prompt)
            return {"Original": source, "Translate": target}

        # æœ€å¤šåŒæ™‚ç™¼é€ 128 å€‹ Requests
        results = list()
        with ThreadPoolExecutor(max_workers=128) as executor:
            with tqdm(total=len(text), ncols=80) as progress:
                for res in executor.map(translate, text):
                    results.append(res)
                    progress.update()

        # å°‡çµæœå­˜æˆ JSON æª”
        with open("results.json", "wt", encoding="UTF-8") as fp:
            json.dump(results, fp, ensure_ascii=False, indent=4)


    # å®šç¾©ç™¼é€ HTTP Request çš„å‡½å¼
    def generate(prompt):
        url = "http://localhost:8080/"

        # åƒè€ƒ Taiwan Llama Demo ç¶²é çš„é è¨­åƒæ•¸
        data = {
            "inputs": prompt,
            "parameters": {
                "do_sample": True,
                "best_of": 1,
                "max_new_tokens": 1000,
                "stop": ["\n\n"],
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.90,
            },
        }

        res = requests.post(url, json=data)
        return json.loads(res.text)[0]["generated_text"]


    if __name__ == "__main__":
        main()
    ```
* ä½¿ç”¨ RTX 3090 ç´„å››åˆ†åŠå¯ä»¥å®Œæˆæ•´ä»½ç¿»è­¯ï¼Œçµ¦å„ä½åƒè€ƒçœ‹çœ‹ã€‚

---

## é€£çµ
* [GitHub: Text Generation Inference](https://github.com/huggingface/text-generation-inference)
* [HF Docs: Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)
* [Text Generation Inference API](https://huggingface.github.io/text-generation-inference/)
* [GitHub: Taiwan Llama Issue#18](https://github.com/MiuLab/Taiwan-LLaMa/issues/18)

---

## çµè«–
* æœ¬ç« ç¯€ä»‹ç´¹äº† TGI æ¨è«–æ¡†æ¶ï¼Œé›–ç„¶åœ¨é‡åŒ–é¸é …ä¸Šè¼ƒ llama.cpp ä¾†çš„å°‘ï¼Œä½†æ˜¯ä¸»æµçš„é‡åŒ–æ–¹æ³•éƒ½æœ‰æ”¯æ´ã€‚
* å…¶æ¨è«–çš„é€Ÿåº¦èˆ‡ç©©å®šåº¦ï¼Œä»¥åŠå»£æ³›çš„æ¨¡å‹æ¶æ§‹æ”¯æ´ï¼Œä½¿å®ƒæˆç‚ºç›¸ç•¶é©åˆç”¨åœ¨å¾Œç«¯éƒ¨ç½²æ¨è«–æœå‹™çš„æ–¹æ³•ã€‚
* å…¶å¯¦é™¤äº†é€™å¹¾å¤©ä»‹ç´¹çš„ ONNX, ggml, vLLM, TGI ä»¥å¤–ï¼Œé‚„æœ‰éå¸¸å¤šçš„æ¨è«–æ¡†æ¶ï¼Œä¾‹å¦‚ [Exllama V2](https://github.com/turboderp/exllamav2), [CTranslate 2](https://github.com/OpenNMT/CTranslate2) ç­‰ç­‰ï¼Œé€™äº›æ¨è«–æ¡†æ¶éƒ½æœ‰å…¶ç©©å®šçš„é–‹ç™¼èˆ‡ç”¨æˆ¶ã€‚
    * ä¸éé€™äº›æ¡†æ¶çš„è®ŠåŒ–é€Ÿåº¦ä¹Ÿç›¸ç•¶å¿«ï¼Œå¾ˆæœ‰å¯èƒ½ç­†è€…é€™å¹¾å¤©ä»‹ç´¹çš„æ¡†æ¶ã€åƒæ•¸å’Œç”¨æ³•ç­‰ç­‰ï¼Œéš”å¤©ä¸€å€‹å¤§æ”¹å°±å…¨éƒ¨ä¸èƒ½ç”¨äº†ã€‚

---