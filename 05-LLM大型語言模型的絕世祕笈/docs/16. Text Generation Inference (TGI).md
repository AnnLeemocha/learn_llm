# Text Generation Inference (TGI)
[Text Generation Inference](https://github.com/huggingface/text-generation-inference) 簡稱 TGI，是由 Hugging Face 開發的語言模型推論框架。其中整合了相當多推論技術，例如 Flash Attention、Paged Attention、 Continuous Batching 等等，同時也支援 BitsAndBytes、GPTQ、AWQ 等量化方法，加上 Hugging Face 團隊強大的開發能量與活躍的社群參與，使 TGI 成為部署語言模型服務的最佳選擇之一。

---

## 架設服務
TGI 提供本地安裝與 Docker Image 兩種用法，根據筆者的經驗，本地安裝要花上非常久的時間來編譯與安裝，而且也很容易遇到環境配置的問題，除非要客製化某些功能之類的，不然一律推薦使用 Docker Image 操作。
* 補充：
    * 本章節主要使用 TGI 的 Online Serving 功能，真的需要做 Offline 推論的朋友，推薦使用 Docker Image 搭配 VS Code 的 Container Development 功能，就能在 Docker 裡面操作 Offline 的 TGI 囉！
    * 但因為TGI 主要面向 Online Serving 的關係，Offline 的介面並沒有很好用，相關的說明文件也偏少。
* 可以從官方 [GitHub Packages](https://tinyurl.com/lim-tgi-pkgs) 查看 TGI 的 Docker Image 有哪些版本，或者直接使用最新版本 `latest`：
    ```bash
    docker pull ghcr.io/huggingface/text-generation-inference:latest
    ```
    * 筆者目前使用的是 `sha-f871f11` 版，因為這個 Docker Image 大小約 10 GB 左右，所以下載需要花上一段時間。
* 完成下載之後可以透過--help 查看參數說明：
    ```bash
    docker run --m \
        ghcr.io/huggingface/text-generation-inference:latest --help
    ```
* 可以先用 GPT-2 簡單測試一下：
    ```bash
    docker run --gpus all --shm-size lg \
        -p 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-generation-inference \
        --model-id openai-community/gpt2
    ```
    * 等到訊息紀錄出現 `Connected` 就代表服務啟動完成，因為 TGI 是用 FastAPI 建構的，所以能在`http://localhost:8080/docs/` 底下查看相關的 API Endpoint 與參數。
* 接下來透過 `curl` 做測試：
    ```bash
    curl -X POST 127.0.0.1:8080/generate \
        -d '{"inputs":"Hello, "}' \
        -H 'Content-Type: application/json'
        
    # 得到類似以下的輸出
    # {"generated text": "I'm a new player and ..."}
    ```
* TGI 會透過網路下載模型，並且把模型轉換成 Safetensors 的格式，然後將模型權重存在容器內部的 `/data` 路徑裡面，所以使用參數 `-v $PWD/data:/data` 將此路徑對應出來，這樣就不用每次執行的時候，都要重新下載一遊模型權重。反過來說，也可以先將模型下載好，然後再掛載進容器裡面，例如：
    ```bash
    MODEL_PATH="./models/opt-125m"

    git clone https://huggingface.co/facebook/opt-125m $MODEL_PATH

    docker run --gpus all-shm-size lg \
        -p 8080:80 - SPWD/SMODEL PATH:/SMODEL PATH
        ghcr.io/huggingface/text-generation-inference \
        --model-id /$MODEL_PATH
    ```

---

## 基本用法
TGI 的 API 詳細用法可以參考[官方文件](https://huggingface.github.io/text-generation-inference/)，與 Transformers 一樣有 `do_sample`、`temperature`、`top_k`、`top_p` 的取樣參數可以使用。除此之外，將 `decoder_input_details` 和 `details` 設為 `true` 會回傳一些詳細資訊，例如輸入或輸出的 Token 總數。給 `stop` 參數一個字串列表可以控制輸出停止點。
* TGI 最方便的是可以設定 `truncate` 參數，讓系統幫你把太長的提示截斷，例如設定為 500 的話，就會把提示前面超過 500 個 Tokens 的內容全部切掉，只留最後面的 500 Tokens 當輸入。
* 補充：
    * 這在限制輸入長度很好用，例如做 Retrieval-Based In-Context Learning 時，可以在檢索階段取非常多結果出來，把相似度較低的擺在前面，藉由 `truncate` 參數自然的把相似度太低的範例給切掉，這樣就能在維持輸入長度不會超出系統限制的同時，盡可能的加上更多範例在提示裡面。
* 最後透過 Python 呼叫生成 API 的程式碼大致如下：
    ```python
    import json
    import requests

    url = "http://localhost:8080/generate"
    params = {
        "inputs": "This is a long story maybe, ",
        "parameters": {
            "best_of": 1,
            "details": True,
            "return_full_text": True,
            "decoder_input_details": True,
            "truncate": 4, #只保留最後四個 Tokens
            "max_new_tokens": 128,
            "stop": ["\n", "."],
            "do_sample": True,
            "temperature": 0.5,
            "top_k": 10,
            "top_p": 0.95,
        },
    }

    resp = requests.post(url, json=params)
    result = json.loads(resp.content)
    print(result)
    ```
* 這裡設定 `truncate=4` 來觀察實際效果，因為有打開 `return_full_text` 的選項，所以回傳結果會包含提示與生成，雖然結果裡面看起來提示並沒有被載斷，但若是仔細觀察 `details` 裡面的 `prefill` 就可以看到實際的提示其實是從 `"maybe"` 開始的。
* 除了基本的 `/generate` 以外，TGI 也有提供 `/v1/chat/completions` 這類與 OpenAI API 相容的用法。

---

## 量化選項
TGI 支援的量化方法相當多元，可以透過 `--quantize` 來指定要使用哪一種，目前 TGI 支援 [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) 8-Bit/4-Bit、[GPTQ](https://github.com/PanQiWei/AutoGPTQ)、[AWQ](https://github.com/mit-han-lab/llm-awq)、[EETQ](https://github.com/NetEase-FuXi/EETQ)、FP8 等量化方法：
* 說明：
    * `bitsandbytes`： 8-Bit 量化，雖然推論速度偏慢，但支援相對廣泛與穩定。
    * `bitsandbytes-nf4`： 4-Bit 量化，大部分的模型都可以直接使用此選項，賣料型態為 Normal Float 4.
    * `bitsandbytes-fp4`： 4-Bit 量化，與 `bitsandbytes-nf4` 類似，但使用標準的 4-Bit 浮點數資料型態。
    * `gptq`： 4-Bit 量化，需要先進行 GPTQ 量化才能使用此選項。可以到 HF Hub 上搜尋，例如 [TheBloke 提供的 GPTQ 模型](https://huggingface.co/models?search=thebloke%20gptq)。
    * `awq`： 4-Bit 量化，同樣也需要先進行 AWQ 量化才能使用的選項。可以參考 [TheBloke 提供的 AWQ 模型](https://huggingface.co/models?search=thebloke%20awq)。
    * `eetq`： 8-Bit 量化，與 `bitsandbytes` 一樣是可以直接套用在一般模型上的量化選項，但是速度更快、準確率更高，缺點是啟動時間比較久。
    * `fp8`： 是 NVIDIA 最新支援的標準八位元浮點數資料型態，但是要 Hopper 構以上的 GPU 才能跑，像是 H100 之類的。
* 有了量化，我們單顯卡平民就天下無敵了！先拿個 [Taiwan LLM 13B](https://huggingface.co/yentinglin/Taiwan-LLaMa-v1.0) 熱熱身：
    ```bash
    docker run --gpus all --shm-size lg \
        -P 8080:80 -v $PWD/data:/data \
        ghcr.io/huggingface/text-generation-inference \
        --model-id yentinglin/Taiwan-LLM-138-v2.0-chat \
        --quantize bitsandbytes
    ```
* 順利跑起來之後，簡單拿個中文測試：
    ```bash
    curl -X POST 127.0.0.1:8080/generate \
        -d '{"inputs": "USER: 什麼是語言模型? </s>ASSISTANT: "}' \
        -H 'Content-Type: application/json'
    # Output: {"generated_text": "語言模型是一種..."}
    ```
    * 好欸，我們順利在一張 24GB 的顯卡上運行了一個 13B 的模型啦！
    * 因為 TGI 參數很多很複雜，於是筆者將常用的參數整理成一份 Python Script 放在此 [GitHub Gist](https://gist.github.com/penut85420/54765b2ed8a3e2e127ffdc0caf2dcaab) 上方便使用，此腳本僅供參考，請以自身運行的環境與 TGI 版本為主。
* 補充：
    * 除了一般的 Decoder 語言模型以外，像是 [T5](https://huggingface.co/t5-small) 或 [M2M](https://huggingface.co/facebook/m2m100_418M) 之類的 Encoder-Decoder 架構也可以用透過 TGI 來運行。
    * 但如果不是 TGI 特別支援的模型，可能會回到原本 HF Transformers 的實做。
    * 一些像是 Tensor-Parallel 或 Flash Attention 的機制就無法用到，但依然有 Continuous Batching 和 Streaming Outputs 這些功能可以用。
    * 因此若要部署 Seq2Seq 模型可以考慮使用 TGI 來架設服務。

---

## Token 數量設定
若要更精細的部署模型，則 Token 相關的參數至關重要。對於小顯卡而言， 可以減少記憶體消耗，對於大顯卡而言，更能充分利用記憶體空間。
* 最主要的三個參數如下：
    * `--max-input-length` ：代表單筆輸入的最大長度。
    * `--max-total-tokens` ：單筆輸入加輸出的最大總長度。
    * `--max-batch-prefill-tokens` ：代表所有批次加起來的最大輸入長度。
* 其中 `--max-batch-prefill-tokens` 是影響記憶體消耗最關鍵的參數，不僅影響 `Prefill` 階段能夠容納多少 Token 當輸入，也會影響實際運作時能同時處理幾個需求。
* 決定這些參數值的方向大概分成輸入長度、輸出長度與批次大小。在不同情境下，這些設定會不太一樣。
* 以Llama 2 13B 為例，此模型是以 4K Context Window 進行訓練的，那可以分配 3K 給輸入，剩下 1K 給輸出，因此得到：
    1. `--max-input-length` 為 `3000`
    2. `--max-total-tokens` 為 `3000 + 1000 = 4000`
* 補充：
    * 一般而言 Context Length 所謂的 2K 或 4K 通常是以 1024 為單位，所以 3K/1K 的分配實際上會是 **3*1024=3072** 跟 **1024** 個 Tokens。
* 接下來，假設系統要**同時處理 4 筆輸入**，則可以設定參數:
    3. `--max-batch-prefill-tokens` 為 3000 * 4 = 12000
    4. `--max-concurrent-requests` 代表佇列中最多等待處理的需求是幾筆，預設為 128 筆。
* 在這樣的設定下，如果使用者每筆輸入都是 3000 個 Tokens， TGI 最多只會同時推論 4 個請求，剩下請求的都會被放在佇列裡面，等待任何一個生成結束之後，再從佇列拿一個需求出來放進批次裡面繼續生成。
* 另外可以將 `--max-best-of` 設為 `1` 就好，因為一般應用通常只需要生成一筆結果。
* 最後使用 bitsandbytes-nf4 做 4-Bit 量化，整個指令看起來會像這樣：
    ```bash
    docker run -gpus all --shm-size lg \
        -P 8080:80 -v $PWD/data:/data \
        ghcr.10/huggingface/text-generation-inference:latest \
        --model-id meta-llama/Llama-2-13b-chat-hf \
        --quantize bitsandbytes-nf4 \
        --max-best-of 1 \
        --max-concurrent-requests 128 \
        --max-input-length 3000 \
        --max-total-tokens 4000 \
        --max-batch-prefill-tokens 12000
    ```
* 雖然設定批次大小為 4，不過 TGI 其實是動態決定當下要同時推論的數量。
    * 例如使用者每筆輸入都減為 1500 個 Tokens 的話，那可能就會變成同時推論 8 筆。
    * 因此在不會超出 GPU 記憶體的情況下，把參數 `--max-batch-prefill-tokens` 盡可能開大，便能提昇整體系統的吞吐量。
* 至於具體到底要調到多少才不會超出記憶體呢?
    * 根據硬體與模型架構的不同，各自都會有不同的上限，所以只能自己慢慢測試了。
    * 筆者習慣以 2K 為單位往上遞增做測試，如果 4K 不會爆就試 6K，如果 6K 不會爆就試 8K，依此類推。

---

## TGI Client
TGI Client 是用來跟 TGI Server 交流的客戶端套件，因為在 TGI 的文件中沒有太大篇幅的介紹，所以相當容易被人遺忘。
* 首先透過 `pip` 安裝 `text-generation` 套件:
    ```bash
    pip Install text-generation
    ```
* 基本使用方法如下：
    ```python
    from text_generation import Client

    client = Client("http://127.0.0.1:8080", timeout=600)

    resp = client.generate(
        "Hello, ",
        max_new_tokens=16,
        stop_sequences=["."],
        do_sample=False,
        truncate=2048,
    )

    print(resp.generated_text)
    ```
* 其實就是將 `requests` 的用法做個包裝，並且幫回應的物件定義了明確的類別，因此輸入 `resp.` 的時候，會跳出 `generated_text` 的提示，開發的時候會更方便一點。
* 透過這個套件進行串流輸出也比較方便：
    ```python
    resp = client.generate_stream(
        "Hello",
        max_new_tokens=16,
        stop_sequences=["."],
        do_sample=False,
        truncate=2048,
    )

    for chunk in resp:
        print(end=chunk.generated_text, flush-True)

    print()
    ```
* 更多詳細的用法，可以參考官方在 PyPl 上的[介紹頁面](https://pypi.org/project/text-generation/)。

---

## 速度測試
* 同樣使用上個章節的評測程式來測量 TGI 與 llama.cpp 之間的速度差異：
    | 方法             | Prefill (t/s) | Decode (t/s) |
    | ---------------- | ------------- | ------------ |
    | TGI-1T           | 4000          | 49           |
    | TGI-4T           | 4000          | 189          |
    | TGI-8T           | 4200          | 372          |
    | TGI-16T          | 4200          | 686          |
    | llama.cpp        | 4900          | 49           |
    | llama.cpp -np 4  | 4000          | 163          |
    | llama.cpp -np 8  | 3800          | 293          |
    | llama.cpp -np 16 | 4000          | 473          |
    * 1T、4T 表示同時跑一筆或四筆，可以看到在單筆推論時，速度與 llama.cpp 差異並不大，但是當同時跑的筆數越來越多的時候，兩者的速度差距就越來越大了。
* 接下來測試看看 TGI 支援的各種量化方法：
    | 💸         | 1T            | 1T           | 16T           | 16           |
    | ---------- | ------------- | ------------ | ------------- | ------------ |
    | 💸         | Prefill (t/s) | Decode (t/s) | Prefill (t/s) | Decode (t/s) |
    | FP16       | 4000          | 49           | 4200          | 686          |
    | EETQ 8-Bit | 4000          | 92           | 5000          | 292          |
    | BNB 8-Bit  | 4000          | 29           | 4100          | 259          |
    | BNB NF4    | 3700          | 80           | 4100          | 1000         |
    | GPTQ 4-Bit | 5700          | 137          | 6500          | 1000         |
    | AWQ 4-Bit  | 2400          | 108          | 2300          | 1346         |
    | FP8        | 💸            | 💸           | 💸            | 💸           |
    * BitsAndBytes 的 Decode 速度真的滿慢的，相較之下 EETQ 的生成速度就快上許多，但 EETQ 的熱身時間較久是個小缺點。
    * 在單筆推論時，GPTQ 的 Decode 速度略勝 AWQ，但是多筆推論時，AWQ 的速度就贏過 GPTQ 了。
    * 因此若是經常需要輸入長文本的應用，採用 GPTQ 量化會比較好;但若是短問短答， 追求生成速度，那選擇 AWQ 較佳。
    * 至於 FP8，等筆者出書後暢銷大賣賺大錢了，再來買台 H100 幫大家測試看看

### 翻譯應用
筆者曾經使用 TGI + Taiwan Llama 替人翻譯過一篇約 13000 Tokens 的英文文章，以下分享這個應用的實做。
* 首先筆者先將文章存在 `content.txt` 裡面，並手動分段，約兩三行的內容就放兩個換行。因為文章內容沒有很長，所以手動分段還算可以，主要是為了確保邊界正確。但如果應用在更長的文章上，可能需要考慮一些自動尋找 Boundary 的工具協助了。
* 接下來用 TGI 將 Taiwan Llama 架起來，使用 BNB-NF4 量化，接著撰寫以下程式碼進行翻譯：
    ```python
    import json
    from concurrent.futures import ThreadPoolExecutor

    import requests
    from tqdm import tqdm


    def main():
        # 讀取文章並以 "\n\n" 切成多個 Chunks
        with open("content.txt", "rt", encoding="UTF-8") as fp:
            text = fp.read().strip()
            text = text.split("\n\n")

        # Taiwan Llama 提供的 Prompt Template
        template = "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: 請將以下句子從英文翻譯成中文: {} ASSISTANT:"

        # 定義每段 Chunk 的翻譯函式
        def translate(source):
            prompt = template.format(source)
            target = generate(prompt)
            return {"Original": source, "Translate": target}

        # 最多同時發送 128 個 Requests
        results = list()
        with ThreadPoolExecutor(max_workers=128) as executor:
            with tqdm(total=len(text), ncols=80) as progress:
                for res in executor.map(translate, text):
                    results.append(res)
                    progress.update()

        # 將結果存成 JSON 檔
        with open("results.json", "wt", encoding="UTF-8") as fp:
            json.dump(results, fp, ensure_ascii=False, indent=4)


    # 定義發送 HTTP Request 的函式
    def generate(prompt):
        url = "http://localhost:8080/"

        # 參考 Taiwan Llama Demo 網頁的預設參數
        data = {
            "inputs": prompt,
            "parameters": {
                "do_sample": True,
                "best_of": 1,
                "max_new_tokens": 1000,
                "stop": ["\n\n"],
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.90,
            },
        }

        res = requests.post(url, json=data)
        return json.loads(res.text)[0]["generated_text"]


    if __name__ == "__main__":
        main()
    ```
* 使用 RTX 3090 約四分半可以完成整份翻譯，給各位參考看看。

---

## 連結
* [GitHub: Text Generation Inference](https://github.com/huggingface/text-generation-inference)
* [HF Docs: Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)
* [Text Generation Inference API](https://huggingface.github.io/text-generation-inference/)
* [GitHub: Taiwan Llama Issue#18](https://github.com/MiuLab/Taiwan-LLaMa/issues/18)

---

## 結論
* 本章節介紹了 TGI 推論框架，雖然在量化選項上較 llama.cpp 來的少，但是主流的量化方法都有支援。
* 其推論的速度與穩定度，以及廣泛的模型架構支援，使它成為相當適合用在後端部署推論服務的方法。
* 其實除了這幾天介紹的 ONNX, ggml, vLLM, TGI 以外，還有非常多的推論框架，例如 [Exllama V2](https://github.com/turboderp/exllamav2), [CTranslate 2](https://github.com/OpenNMT/CTranslate2) 等等，這些推論框架都有其穩定的開發與用戶。
    * 不過這些框架的變化速度也相當快，很有可能筆者這幾天介紹的框架、參數和用法等等，隔天一個大改就全部不能用了。

---