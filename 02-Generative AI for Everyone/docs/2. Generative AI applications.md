# Generative AI applications


## 寫作
* 集思廣益產品名稱 (創意頭腦風暴)
    * LLM可以作為創意合作夥伴，幫助提出創意點子
* 制定銷售策略
* 撰寫新聞稿
    * LLM還可以用來撰寫各類文案
    * 如果沒有提供具體信息，模型會生成一個非常通用的新聞稿。
    * 但如果提供更多背景資料，則可以生成更加具體和有深度的新聞稿。這顯示了在編寫提示時，提供更多的上下文可以得到更好的結果。
* 翻譯
    * LLM在某些情況下可用於翻譯，並且在某些語言中甚至比專門的機器翻譯引擎表現更好。
    * 譯可能會有小瑕疵，不過，通過調整提示或請教母語者，這些小錯誤可以輕鬆修正。
      (翻譯成正式的XX語 → 翻譯成正式的XX口語)
    * 通常判斷語言翻譯的情況，為了瞭解發生了什麼，會將其翻譯成海盜英語進行測試


---


## 閱讀
* 內容校對 
    * 識別拼寫和語法錯誤，以及不夠流暢的句子
    * 有時即使在多次檢查後，LLM仍能發現一些錯誤
* 總結文章
    * LLM可以對長文章進行摘要，幫助他快速了解文章的要點，節省了閱讀整篇文章的時間
    * web-base
* 客服通話摘要
    * 在商業場景中，LLM可以總結長時間的客服通話紀錄，幫助管理者更有效率地回顧通話內容，快速提取關鍵細節
    * 對話錄製音檔 → 轉文檔 → APP → 總結對話 → 總結統計表格
    * app-base
* 客戶郵件路由
    * LLM可以分析客戶郵件，判斷應該將郵件路由到哪個部門
    * app-base
    * 寫一個提示，告訴 LLM 閱讀這封電子郵件，並決定將其路由到哪個部門。若 LLM 未獲提供足夠的情境，讓它不知道應該從您公司中哪些實際部門的名稱中進行選擇，就需要更新提示，給定您想要選擇的選項集，將其導到正確的路由。
* 聲譽監測
    * 使用儀錶板來追蹤一段時間內客戶情緒變化
    * LLM可以分析顧客的評論或郵件，進行情感分類，將其歸類為正面或負面。
    * app-base
    * 這有助於企業追蹤顧客的情緒變化，並在情感趨勢負面時及時作出反應。


---


## 聊天
* 客戶服務聊天機器人
* 專業聊天機器人
    * **能夠提供專業領域的知識，並給予建議**。
      例如，協助旅行規劃、提供職業建議或烹飪建議的機器人
* IT 服務聊天機器人
    * 經常收到大量的且重複的請求，這類請求可以由聊天機器人自動處理，減輕負擔。
    * 能夠提供專業領域的知識，**並與公司的其他軟件系統進行交互，甚至執行操作**。(處理訂單或重置密碼)

### 聊天機器人在客戶服務中的興起 
![The rise of chatbots in customer service](images/The%20rise%20of%20chatbots%20in%20customer%20service.png)

* 只有人
    * 重複打字
* 機器人支援人類
    * 機器人會生成建議消息，由人工客服審核和修改後再發送給客戶。  
      這樣的設計可以避免聊天機器人犯錯，這是一種減少錯誤風險的設計方式。
* 機器人對人類進行分類
    * 機器人可以回答簡單的訊息，但對於還未準備好處理的事情，就會升級給人工處理。
    * 可以幫助您的人工服務人員節省時間，並只需專注於較難的個案。
* 只有機器人
    * 軟體直接回應客戶
一些聊天機器人可以處理簡單的問題，將較為複雜的問題升級給人工客服。

### 部署聊天機器人的建議
* 從面向內部的聊天機器人開始 
    * 與員工合作評估聊天機器人的行為 (發現和修正問題)
    * 避免公開錯誤
* 使用 Human-in-the-loop (人工參與)進行部署以檢查錯誤
    * 讓人工客服檢查機器人發送的消息
* 只有在被認為安全后，才允許機器人直接與客戶通信
    * 直到表現穩定且可靠才能進行


---


## LLM 可以做什麼和不能做什麼
理解LLM的限制對於避免誤用其進行不適合的任務非常重要。
### 提示 LLM 可以做什麼
#### 思維框架:
當試圖理解LLM能做什麼時，可以用一個問題來思考...

**假設一位剛畢業的大學生，僅根據指示，能否完成所需的任務？**
前提假設： 
* 無法存取網路或其他資源 
* 沒有針對您的公司/業務的培訓 
* 不記得以前完成的任務 
    * 您每次都會得到不同的應屆大學畢業生

結論：
* 如果大學生能做的事情，LLM通常也能做。
* 但如果給予的背景信息不足，像是讓大學生或LLM寫一篇不知情的新聞稿，結果就會非常泛泛和不具體。

### 知識截止 (Knowledge cutoffs)
LLM 對世界的瞭解在接受培訓時被凍結。LLM的知識是基於訓練時的數據，這意味著它對訓練時間之後的事件一無所知。

例如，使用 2022 年 1 月從互聯網上抓取的數據訓練的模型，若詢問它 2022 年最高票房電影，它無法回答，因為它的知識庫只涵蓋了訓練前的信息。

### 編造：幻覺（Hallucinations）
LLM有時會「編造」信息，這些「幻覺」看起來可能很真實，但實際上並不正確，可能會產生嚴重的後果。

例如，當你請求 LLM 提供莎士比亞(Shakespeare)關於碧昂絲(Beyonce)的名言時，它可能會創造出不存在的引述。這種情況會讓LLM看起來很有權威性，卻導致錯誤信息的傳播。

### 輸入 （和輸出） 長度是有限的

**技術限制**  
LLM對輸入的字數有上限，大多數 LLM 可以接受最多只有幾千個單詞的提示。
* 您可以為其提供的上下文總量是有限的
    * 因此處理非常長的文本（如整篇文章或報告）時，會受到限制。
    * 此時，可以將長文本分割成多個部分，逐一處理。
* 找到一些上下文限制更長的 LLM 
    * 最多 100,000 個單詞
* LLM 的上下文長度是總輸入 + 輸出大小的限制

### 生成式 AI 不適用於結構化（表格）數據
LLM對於結構化數據（例如表格數據，如Excel或Google Sheets中的數據）表現不佳。

例如，要求LLM基於某些特徵（如房屋的面積）預測價格時，這不是LLM的強項，這類問題應該使用監督學習等其他技術。

### 生成式 AI 最適合處理非結構化數據
非結構化數據是指文本、圖像、音訊、視頻等。

### 偏見和有害的言論
LLM 可以反映它從中學習的文本中存在的偏見。

這樣的偏見可能會在某些應用中造成問題，必須謹慎使用LLM來避免不當影響。

一些 LLM 可以輸出有害或其他有害的語音，但隨著時間的推移，大多數模型已經逐漸變得更加安全。

雖然大部分主流LLM提供者都在努力提升模型的安全性，但仍然存在一些模型可能生成有害的語言，甚至教導如何做不當或非法的事情。因此，在使用這些模型時需要格外小心，並且選擇安全性較高的接口。


---


## 提示技巧
* 詳細具體
* 引導模型思考其答案
* 試驗和反覆運算

### 詳細具體
* 為 LLM 提供足夠的上下文(背景訊息)以完成任務
    * 越具體的描述你的需求會越有幫助
* 詳細描述所需的任務

例如，如果你只簡單地說「幫我寫一封電子郵件，要求分配到法律文檔項目。」，LLM可能無法知道該如何說服別人選擇你。

但越具體的描述你的需求會越有幫助。比方說，改成「寫一段文本，解釋為什麼我的背景使我成為這個項目的強有力候選人」會讓LLM理解你希望它做的事更清楚，並產生更符合要求的結果。

### 引導模型思考其答案
如果你有具體的流程或思路，可以讓LLM逐步按照你的指示來解決問題。

例如，如果你想讓它為一個新款貓玩具命名，並希望名字押韻且附帶相關的表情符號，你可以將提示分步指導，這樣LLM就能更容易理解並按照步驟進行。

1. 首先讓它想出五個與貓有關的快樂詞彙，
2. 然後為每個詞提供一個押韻的名字，
3. 再為每個名字加上一個有趣的表情符號。

這樣的具體步驟指導會讓LLM的結果更符合你的預期。

### 試驗和反覆運算
* 沒有適合每個人或情況的完美提示！
* 相反，最好透過迭代來開發改進提示的流程
    * 不斷評估輸出並改進提示

當你開始提示LLM時，可能不會馬上得到完美的結果。
這時候，重要的是要知道，提示過程通常是反覆修改的。

例如:
1. 你可以先嘗試「幫我改寫這段文字」，
2. 如果結果不理想，再加上一些具體要求，如「糾正語法和拼寫錯誤」，
3. 然後如果還不符合預期，再進一步要求「用專業簡歷的語氣改寫」。

這種反覆的過程有助於你逐步調整提示，直到得到你想要的結果。

不要過於糾結於第一次輸入的提示，很多時候，起步時用一個較簡單的提示，然後根據結果再進行調整，是更有效的方式。

記住，最重要的是不斷地實驗、調整，直到找到最適合的提示。

### 反覆運算改進提示
![Iteratively improving your prompt](images/Iteratively%20improving%20your%20prompt.png)

提示過程
* 在提示中保持清晰和具體
* 思考為什麼結果沒有給出期望的輸出
* 優化您的提示
* 重複

**注意**：
* 不要過度考慮初始提示。只需快速嘗試一些東西並隨著時間的推移改進它！

**注意事項：小心**
1. 機密資訊 
    * 如果你在提示LLM時處理的是敏感或機密的資料，請務必確認LLM提供者如何處理這些資料，以確保你的資料安全。
2. 您是否信任 LLM 輸出 (檢查結果的可靠性)
    * 在使用LLM的結果時，特別是在涉及重要事宜的情況下，請檢查並確保你能夠信任這些輸出的準確性。


---


## 圖像生成
### 影像生成 (擴散模型, diffusion models)
擴散模型透過從大量圖像中學習來創建圖像。
這項技術不僅能用來生成現有的圖像，也能創造出全新的、前所未見的圖像，並且在藝術、設計等領域有著巨大的應用潛力。

這些模型的工作原理是：
1. 加入噪音：
    * 首先將清晰的圖像逐漸加入更多噪音，最終變成純粹的隨機噪音。
2. 學習去噪音：
    * 模型使用大量標註好的圖像數據集進行訓練，學會如何將帶有噪音的圖像恢復成較為清晰的版本。
    * 訓練好的模型可以從純隨機噪音開始生成新圖像。每一步，模型都會逐漸去除噪音，直到最終產生出清晰的圖像。
    * 輸入純粹的隨機噪點圖片 → 輸出噪點略少的圖片 
      (圖片越來越清晰)
    * 對於擴散模型通常為 ~100 個步驟才會讓圖片變清晰
3. 加入文字 (提示)
    * 告訴它希望生成什麼 (文生圖)
    * 輸入純粹的隨機噪點圖片+標題 → 輸出噪點略少的圖片 
    * 每一張噪音圖像都會與對應的文字描述一起訓練，這樣模型學會根據文本生成與描述相符的圖像。
    * 模型可以根據給定的文本提示來生成特定的圖像。
    * 訓練過程中，模型不僅學會如何去噪，還學會如何根據文本描述來生成特定的圖像。
4. 訓練完畢
    * 給它一個文本提示，它會從噪音圖像開始，並根據提示逐漸生成符合要求的圖像。
    * 實際範例：
        生成綠香蕉的圖像：
        * 從純噪音圖像開始。
        * 給模型提供文本提示「綠香蕉」。
        * 模型根據提示進行逐步去噪，直到生成一張清晰的綠香蕉圖像。



### 添加文字
### 從文字生成圖像


---


## 課程資源
### 值得嘗試的 Web UI 聊天機器人
如果您想嘗試提示大型語言模型（LLM），您可以訪問下面連結的聊天機器人之一：
* [ChatGPT](https://chat.openai.com/) 來自OpenAI
* [Gemini](https://gemini.google.com/app)（原名 Bard）來自 Google
* [Bing Chat](https://www.bing.com/search?q=Bing+AI&showconv=1&FORM=hpcodx) 來自 Microsoft

### 生成式 AI 與經濟
您可以透過閱讀以下報告和文章來了解生成式人工智慧對經濟的影響：

* [McKinsey:生成式人工智慧的經濟潛力：下一個生產力前沿](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#introduction)
    > *McKinsey: 
The economic potential of generative AI: The next productivity frontier
, McKinsey Digital report, June 2023*
* [GPT 就是 GPT：大型語言模型對勞動市場影響潛力的初步觀察](https://arxiv.org/pdf/2303.10130.pdf)
    > *GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models
, Tyna Eloundou, Sam Manning, Pamela Miskin, and Daniel Rock, March 2023 (arXiv:2303.10130)*
* [Goldman Sachs:人工智慧對經濟成長的潛在巨大影響](https://www.gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html)
    > *Goldman Sachs: 
The Potentially Large Effects of Artificial Intelligence on Economic Growth
, Joseph Briggs and Devesh Kodnani, March 2023*



