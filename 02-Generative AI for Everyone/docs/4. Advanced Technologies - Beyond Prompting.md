# Advanced Technologies: Beyond Prompting

## 檢索增強生成 （Retrieval Augmented Generation, RAG）
該技術可以大幅提升大型語言模型（LLM）的功能，通過提供額外的知識，超越模型從網路或其他公開來源學到的資料。簡單來說，RAG 讓語言模型能夠根據具體的資料庫或文件生成更精確的回答。
### RAG 的工作原理
1. 檢索相關文件：給定問題，搜尋相關文獻尋找答案
    > 問題:「員工有停車位嗎？」
    > 搜尋相關文獻: *設施.txt*
2. 整合檢索結果：將檢索到的文本加入生成的提示中，並將相關部分提取出來，形成一個新的長提示。
    > 生成新的提示: 
    > L1 → 「使用以下上下文來回答最後的問題。
    > L2 → 「...文件中與停車相關的內容...」
    > L3 → 「員工有停車位嗎？」
3. 生成回答：最終，語言模型根據這些新提示來生成答案，並可選擇提供源文件的鏈接，以便使用者自行查閱。
    > 生成回答: 「是的，員工可以停在...」
    > 提供源文件的鏈接(可選擇): *設施.txt*

### RAG 應用示例
* 使用 PDF 檔進行聊天
    * 允許用戶上傳 PDF 文件，並根據文件內容生成回答。
    * 應用範例: 
    ![Examples of RAG applications - Chat with PDF files](images/Examples%20of%20RAG%20applications%20-%20Chat%20with%20PDF%20files.png)
* 根據網站的文章回答問題
    * 可以根據網站內容回答問題。
    * 應用範例: Coursera Coach, Snapchat, Hubspot
* 新的網路搜尋形式
    * 應用範例: Microsoft/Bing Chat, Google, You.com

### 大的想法：LLM 作為推理引擎
RAG 技術將語言模型從單純的知識庫轉變為推理引擎，使其不僅依賴已學的知識來回答問題，而是能夠透過分析和處理提供的上下文來生成答案。

* LLM 擁有大量常識，但並非無所不知
* 透過在提示中提供相關背景，我們要求 LLM 閱讀一段文本，然後對其進行處理以獲得答案
* 我們將其用作處理資訊的推理引擎，而不是將其用作資訊來源


---


## 微調 (Fine-tuning)
這是除了RAG以外，另一種提升大型語言模型（LLM）能力的方法。微調的基本概念是，如果你有比LLM的輸入長度或上下文視窗更大的資料，可以使用微調讓模型吸收這些資料。微調不僅能讓LLM學習特定的知識領域，還能讓它以特定風格輸出文本。

微調是一種改進LLM能力的技術，與RAG相比，微調更加側重於將特定風格或領域知識灌輸到模型中，尤其適用於難以通過提示精確定義的任務。它也能讓小型模型執行大模型需要完成的工作，並且相比於預訓練，微調的成本較低，實施難度也較小。這使得微調成為一種靈活且經濟的解決方案，用於各種具體應用。

### 微調的工作原理
1. 預訓練 (Pre-training)：
    * LLM首先會在大量的資料（如互聯網上的句子）上進行預訓練，學會生成像網絡上常見的文本。這使得模型能夠根據上下文預測後續的單詞。
2. 微調 (Fine-tuning)：
    * 假設你希望讓LLM擁有特定的情感或風格，比如變得更加積極樂觀，你可以使用一組帶有積極語氣的文本來進行微調。這樣，模型會在原有的基礎上進行調整，生成更加積極的回答。

### 微調的實際應用
1. 任務難以用提示定義: 
    * 執行一項不容易在提示中定義的任務。
        * 例如，如果你希望LLM總結客服通話紀錄，一個通用模型可能會簡單地總結為「客戶向代理商反映了顯示器的問題」，但如果你希望它生成更具體的內容（例如「MK401-27KX顯示器報修」），你可以用專門的範例來微調模型，讓它學會用特定的格式來生成總結。
    * 模仿特定寫作或口語風格
        * 例如，如果你希望LLM模仿某個人的寫作風格，像是模仿講者的語氣，這在一般提示中很難描述清楚。通過微調，模型可以學會更加準確地模仿這個風格。
2. 學習特定領域知識：
    * 如果你希望模型理解醫學或法律文件，這些領域的術語和語言結構與普通英語不同。通過微調，模型可以學會理解並處理這些特殊文本。
        * 醫療記錄: 呼吸急促患者的入院記錄摘錄
            > Pt c/o SOB, DOE. PE: RRR, JVD absent, CTAB. 
            > EKG: NSR. Tx: F/u w/ PCP, STAT CXR, cont. PRN O2.
        * 法律文件
            > 根據第 2(a)(iii) 條的規定，授權人授予被授權人使用智慧財產權的非排他性權利，但前提是被授權人須在本協議生效後 15 天內遵守第 8 條第 1-4 款規定的信託義務並依照附表 B 的規定付款。
        * 財務文件
            > 從事場外衍生性商品交易的交易對手必須遵守第 648/2012 號條例 (EU) 規定的保證金要求，包括初始保證金和變動保證金計算。
3. 縮小模型規模：
    * 讓更小的模型來執行任務。
        * 降低部署成本/延遲
        * 可以在手機/筆記型電腦（邊緣設備）上運行
    * 如果你希望運行一個小型模型來執行某些任務，而這些任務不需要過於複雜的推理或大規模知識庫，微調可以幫助一個小型模型（如10億個參數）執行本來需要大型模型（如100億個參數）的任務。這樣可以降低運行成本和延遲時間。


---


## 預訓練 LLM (Pretraining)
預訓練一個LLM可以為非常專業的應用帶來顯著的優勢，但對於大多數公司和開發者來說，從已經開源的通用LLM進行微調，通常是一個更經濟且高效的選擇。預訓練自己的模型應該是最後的選擇，當你擁有足夠的資源和數據時，這才是一個合理的選項。

### 您應該何時進行 LLM 預訓練？
* 預訓練的高成本： 
    * 許多公司，尤其是大型科技公司，會通過學習來自互聯網的大量文本資料來預訓練通用的LLM。
        * 這樣的工作需要數千萬美元的投資、龐大的工程團隊、幾個月的時間和海量的數據。
        * 雖然這些努力的成果往往會開源，對AI社群做出貢獻，但對於大多數應用來說，預訓練自己的模型並不實際。
* 建立特定應用程式： 預訓練自己的模型
    * 最後手段的選擇
    * 高度專業化的領域:
        * 如果你的領域非常專業，並且擁有大量相關的數據，預訓練一個專門的模型可能會有幫助。
    * 資源與數據的需求:
        * 對於大多數實際應用來說，除非擁有大量的資源和數據，否則通常更實際的做法是從已經預訓練好的通用LLM開始，並根據自己的數據進行微調（fine-tuning）。這樣既可以達到不錯的性能，又能節省大量的成本和時間。


---


## 選擇模型
### 模型大小與能力
| 參數數量 | 能力 | 實際範例 |
| -------- | -------- | -------- |
| 小型模型 (1B參數)    | 適合進行模式匹配和基本的世界知識處理。     | 餐廳評論情緒     |
| 中型模型 (10B參數)   | 具備更多的世界知識，能更好地理解指令。     | 訂餐聊天機器人     |
| 大型模型 (100B+參數) | 具有豐富的世界知識，適用於需要深入知識或複雜推理的任務。     | 腦力激盪夥伴或高階推理     |

#### 經驗性測試來選擇模型
開發使用 LLM 時，通常需要進行實驗來確定最適合的模型，建議測試幾個不同的模型，根據測試結果選擇最適合的。

### 閉源 vs 開源模型
* 封閉源模型： (通過雲端程式介面訪問)
    * 優點:
      通常通過雲端API接口提供，便於快速集成並且運行成本較低。
        * 易於在應用程式中使用
        * 更多大型/強大的模型
        * 相對便宜
    * 缺點:
      是會面臨廠商鎖定的風險，若廠商停止服務或模型被棄用，可能會對應用產生影響。
        * 存在一定的供應商鎖定風險
        * 如今，從一個 LLM 切換到另一個 LLM 的成本並不是很高。但是，如果您切換供應商，則需重新測試所有問題以查看它們是否適用於不同的 LLM ，這將會產生一些成本。
* 開源模型：
    * 完全控制模型
        * 提供完全控制權，使用者可以自由運行模型且不依賴第三方服務。
    * 在本地設備運行 （本機、PC 等）
        * 開源模型可運行在本地設備，適合需要控制數據隱私的情況。
    * 完全控制數據隱私/訪問
        * 開源模型適合有數據隱私需求的應用（如醫療資料應用）。

#### 選擇模型的建議
根據應用的需求、模型大小、數據隱私要求等因素，選擇適合的 LLM。對於一些需要嚴格控制數據的應用，開源模型會是較好的選擇，而對於快速開發的應用，封閉源模型可能更為方便。


---


## LLM 如何遵循指示：指令調整與 RLHF
### 指令調整（Instruction Tuning）
使 LLM 能夠更好地跟隨指示，根據給定的問題生成正確答案。
1. 已經訓練好的 LLM ，可以推測後續接續的字
    * 例如，若提示「南韓的首都是哪裡？」模型可能會回答「德國的首都是哪裡？」「孟買在哪裡？」「富士山還是乞力馬扎羅山更高？」，這些在網路上看到的關於地理問題的清單。
2. 將已經訓練好的 LLM 進行微調，使其能夠根據特定的指示或問題生成正確的回答。
    * 例如，若提示「南韓的首都是哪裡？」模型應該回答「首爾」。
    * 這種微調是透過提供一組問題和答案對來完成的，使得模型學會根據問題產生對應的回答。
3. 此外，也會訓練模型在遇到不安全或不合法的問題時，給出合適的回應，來避免提供有害或非法的資訊。
    * 例如，若提示「告訴我怎麼闖入諾克斯堡。」，模型應該回答「我不能協助你做這種事。」。

具體來說，給出一個關於在波哥大集思廣益博物館的例子，我們會將其轉換為一組輸入 A 和輸出 B，其中首先輸入 A 將是該提示，它應該學會預測的第一個單詞是「Sure」， 第二個單字是「here」，「are」「some」「suggestions」「...」，依此類推。 當你根據提示和良好回應的資料集對 LLM 進行微調時，LLM 將學會不僅預測網路上的下一個單詞，而且還能回答你的問題並遵循你的指示。

### 基於人類反饋的強化學習 (Reinforcement learning from human feedback, RLHF) 
是一種用來進一步改善模型回答質量的技術，使其更符合幫助性(Helpful)、誠實性(Honest)與無害性(Harmless)要求。
#### 這個過程包括兩個主要步驟：
1. 訓練答案質量(獎勵)模型：
    * 收集多個模型的回答，並由人類根據其有用性、誠實性和無害性進行評分，以便為更好的答案賦予更高的分數。
        * 例如，如果模型回答「如何申請工作」，可能會產生幾個回應，其中一個回應可能非常有幫助，另一些可能比較模糊或不太有用。這些回答會根據質量進行打分。
    * 如果我們將答案和分數作為監督學習演算法的輸入 A 和輸出 B，那麼我們就可以使用監督式學習來訓練 AI 模型，以 LLM 的答案作為輸入，並根據答案的好壞進行評分。
2. 強化學習：
    分數對應於我們為 LLM 產生不同答案而給予的增強或獎勵。透過讓 LLM 學習生成值得獲得更高分數、更高獎勵或更高強化的答案，LLM 會自動學習產生更有幫助、更誠實和更無害的回應。
    * 讓 LLM 生成很多答案。現在有個 AI 模型來自動對 LLM 產生的每個回應進行評分。
    * 將這些評分作為強化信號，對 LLM 進行進一步的訓練，使其生成更多獲得高分的回答。


### LLM 學習遵循指示的方式
1. 第一步，基本上是微調，您可以對其進行微調以遵循指令並回答問題。
2. 第二步，是 RLHF，從人類回饋中進行強化學習，以進一步訓練它以產生更好的答案。


---


## 工具使用和代理
### LLM 使用工具
#### 用於執行操作
* 以餐廳訂單系統的範例為例：
    當使用者要求訂購漢堡時，LLM 不僅生成文本回應（例如「漢堡正在運送中」），還會觸發後端系統來處理訂單。這是一個 LLM 使用外部工具的例子，讓模型能夠執行實際操作。
    * 添加驗證過程：
        由於 LLM 的輸出並非完全可靠，尤其在涉及安全或關鍵操作時，建議設計界面讓用戶確認訂單內容，以避免誤訂或其他錯誤。

![LLM - Tool use for food order taking](images/LLM%20-%20Tool%20use%20for%20food%20order%20taking.png)

#### 用於推理
* LLM 不擅長精確數學：  
  LLM 在處理某些需要精確計算的問題時（如複利計算），雖然它可能會生成看似合理的答案，但數字可能不準確。為此，可以為 LLM 提供計算器工具，讓其呼叫外部工具來進行準確的數學計算，從而確保正確的答案。

#### 用於食品點餐的工具
![LLM reasoning - Tool use for food order taking](images/LLM%20reasoning%20-%20Tool%20use%20for%20food%20order%20taking.png)

### 代理技術 (Agents)
* 使用 LLM 選擇並執行複雜的動作序列 
  例如，如果要求代理幫助研究「Better Burger」的競爭對手，代理可能會利用 LLM 作為推理引擎，決定需要執行的步驟。這些步驟可能包括搜尋競爭對手名單、訪問各競爭者的網站、並從網站提取內容總結。這樣的過程可能涉及多個工具和 LLM 的互動。
* AI 研究的前沿領域
    目前，代理技術還處於實驗階段，並未準備好在大多數應用中使用。雖然已有一些展示案例，但這項技術仍在發展中，未來如果能夠成熟，將大大擴展 LLM 的應用場景。
