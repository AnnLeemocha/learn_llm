# Language Models, the Chat Format and Tokens 語言模型、聊天格式和令牌
要有效使用大型語言模型，了解其運作方式、訓練流程以及像「tokenizer（分詞器）」這類細節是非常重要的，這些都會直接影響你撰寫提示詞（prompts）時，模型的回應品質與行為。

也會學習 LLM 的聊天格式（chat format），這是指定系統訊息和使用者訊息的一種方式，幫助你更靈活地控制模型的行為。

---

## 語言模型
語言模型（Language Model, LM）可以透過監督學習（Supervised Learning）的方式訓練，其目標是預測下一個單字，即完成從輸入 𝑥 到輸出 𝑦 的對應。
(文字生成)
* 舉例說明
  當你輸入提示語句：「我喜歡吃」，大型語言模型（LLM）會試圖補全接下來最有可能出現的內容。例如，模型可能會回答：
  1. 「貝果」
  2. 「配」
  3. 「起司」
  4. ...

### 現在主流的大型語言模型大致可分為兩種：(Base LLM & Instruction Tuned LLM)
1. 基礎 LLM（Base LLM）
   根據文字訓練資料「預測下一個單字」
2. 指令微調 LLM（Instruction Tuned LLM）
    * 會試圖「照著指令做事」
    * 越來越常被使用

### 從基礎 LLM 到指令微調 LLM
* 使用大量資料訓練基礎 LLM
    * 用大量資料
    * 需要幾個月
* 進一步訓練模型
    1. 根據輸出與遵循輸入指令的範例進行微調 (較小的資料)
        * 這些資料集是含有「指令與良好回應」的範例
    2. 取得不同 LLM 輸出品質的人工評級，標準包括是否有用、誠實和無害
        * 依據回應是否具有幫助、誠實、安全等標準進行評分。
    3. 調整 LLM 以增加其產生更高評級輸出的機率（使用 RLHF：從人類回饋中進行強化學習）
        * 依評分微調，提升模型的回應品質
* 基礎模型轉成指令微調模型只需幾天，資料和算力需求也較低。但這方法目前不太適用於像 Excel 表格這類結構化資料。

---

## Tokens (令牌)
模型預測的不是「下一個詞」，而是「下一個 token（分詞單位）」。
* LLM 處理文字的單位是 token，而不是字母。
* 用 tokenizer （分詞器），把文字拆解成 token。
* Token 是將一段文字切成常見的字詞或字元組合。
    * 常見詞通常是一個 token，不常見詞則會被拆成多個 token。
    * 例如：
        ![token sample](images/token%20sample.png)
        * `Learning new things is fun!` 
          → 每個詞或標點符號是 1 個 token
        * `Prompting as powerful developer tool.`
          → `Prompting` 被拆成 `prom`, `pt`, `ing` 共三個 token
        * `Lollipop` 
          → 被拆成 `l`, `oll`, `ipop`

### 限制與計算
* 對於英語輸入，1 個 token 大約代表 4 個字符，或 3/4 個單字。
* 英文裡，一個 token 約等於 4 字元或 3/4 個詞。
* 不同的模型對輸入(context)+輸出(completion)中的 token 數量有不同的限制。
    * 以 GPT-3.5 Turbo 為例，context 上限約 4,000 token，超出將觸發 API 錯誤。
* 在開發中，除非擔心使用者輸入過長而超出限制，否則不必特別處理 token 數量。若有此疑慮，再檢查長度並截斷。

---

## 聊天格式（Chat Format）
可以透過「系統訊息（system）」、「使用者訊息（user）」、「助手訊息（assistant）」來更靈活控制 LLM 的行為。

---

## 提示正在徹底改變人工智慧應用程式的開發

### 監督式學習
整體可能要幾個月
1. 收集標記數據
    * 幾百筆，可能要幾週
2. 用這些資料來訓練 AI 模型 + 評估
    * 數天到數週
3. 上雲部署和呼叫模型 (測試)
    * 數天到數週

### 基於提示的人工智慧 (開發文字相關的應用)
可能幾小時內就可以部署並實際使用
1. 指定提示
    * 幾分鐘或幾小時
2. 呼叫模型 (透過 API 開始測試)
    * 幾小時內

---

## 說明範例

### 設定
載入 API 金鑰和相關的 Python 函式庫。
在本課程中，我們為您提供了一些載入 OpenAI API 金鑰的程式碼。
```python
import os
import openai
import tiktoken
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
​
openai.api_key  = os.environ['OPENAI_API_KEY']
```

### 輔助函數
在整個課程中，我們將使用 OpenAI 的 gpt-3.5-turbo 模型和[聊天完成端點](https://platform.openai.com/docs/guides/chat)。

此輔助功能將使使用提示和查看產生的輸出變得更加容易。

註：2023 年 6 月，OpenAI 更新了 gpt-3.5-turbo。您在筆記本中看到的結果可能與影片中的結果略有不同。一些提示也經過了輕微修改以產生所需的結果。

```python
def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output 
    )
    return response.choices[0].message["content"]
```

註： 本課程的本實驗筆記本和所有其他實驗筆記本均使用 OpenAI 庫版本 0.27.0。

為了使用 OpenAI 函式庫版本 1.0.0，以下是您可以用來取代 get_completion 函數的程式碼：

```python
client = openai.OpenAI()

def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content
```

### 提示模型並取得完成
```python
response = get_completion("What is the capital of France?")
```
```python
print(response)
```

### 令牌 (Tokens)
模型預測的是「下一個 token（分詞單位）」。模型用 tokenizer （分詞器），把文字拆解成 token。
```python
response = get_completion("Take the letters in lollipop \
and reverse them")
print(response)
```
“lollipop”反過來應該是“popillol”
```python
response = get_completion("""Take the letters in \
l-o-l-l-i-p-o-p and reverse them""")
```
```python
response
```

### 聊天格式
可多輪對話的輔助函數
```python
def get_completion_from_messages(messages, 
                                 model="gpt-3.5-turbo", 
                                 temperature=0, 
                                 max_tokens=500):
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, # this is the degree of randomness of the model's output
        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut 
    )
    return response.choices[0].message["content"]
```
```python
messages =  [  
{'role':'system', 
 'content':"""You are an assistant who\
 responds in the style of Dr Seuss."""},    
{'role':'user', 
 'content':"""write me a very short poem\
 about a happy carrot"""},  
] 
response = get_completion_from_messages(messages, temperature=1)
print(response)
```
限制輸出只回一句話。
```python
# length
messages =  [  
{'role':'system',
 'content':'All your responses must be \
one sentence long.'},    
{'role':'user',
 'content':'write me a story about a happy carrot'},  
] 
response = get_completion_from_messages(messages, temperature =1)
print(response)
```
結合多種輸出要求。(風格+一句話)
```python
# combined
messages =  [  
{'role':'system',
 'content':"""You are an assistant who \
responds in the style of Dr Seuss. \
All your responses must be one sentence long."""},    
{'role':'user',
 'content':"""write me a story about a happy carrot"""},
] 
response = get_completion_from_messages(messages, 
                                        temperature =1)
print(response)
```
輔助函數: (取得 token 的數量)
* 提示（prompt）用了多少 token
* 回覆（completion）用了多少 token
* 總共用了多少 token
```python
def get_completion_and_token_count(messages, 
                                   model="gpt-3.5-turbo", 
                                   temperature=0, 
                                   max_tokens=500):
    
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, 
        max_tokens=max_tokens,
    )
    
    content = response.choices[0].message["content"]
    
    token_dict = {
'prompt_tokens':response['usage']['prompt_tokens'],
'completion_tokens':response['usage']['completion_tokens'],
'total_tokens':response['usage']['total_tokens'],
    }

    return content, token_dict
```
```python
messages = [
{'role':'system', 
 'content':"""You are an assistant who responds\
 in the style of Dr Seuss."""},    
{'role':'user',
 'content':"""write me a very short poem \ 
 about a happy carrot"""},  
] 
response, token_dict = get_completion_and_token_count(messages)
```
```python
print(response)
```
```python
print(token_dict)
```

---

## 注意事項
關於在課堂之外使用 OpenAI API 的注意事項
要安裝 OpenAI Python 庫：
```python
!pip install openai
```
該庫需要配置你的帳戶金鑰，該金鑰可以在[網站](https://platform.openai.com/account/api-keys)上取得。
您可以在使用該庫之前將其設定為 OPENAI_API_KEY 環境變數：
```python
 !export OPENAI_API_KEY='sk-...'
```
或者，將 openai.api_key 設定為其值：
```python
import openai
openai.api_key = "sk-..."
```

### API 金鑰管理的小技巧
不要在程式中直接寫明你的 OpenAI API 金鑰，這很危險。

更安全的做法是使用 .env 檔案儲存你的 API 金鑰，然後用 dotenv 套件載入它：
```python
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
os.getenv("OPENAI_API_KEY")
```

### 關於反斜線的說明
在課程中，我們使用反斜線 `\` 來使文字適合螢幕，而無需插入換行符 `\n`。
無論是否插入換行符，GPT-3 都不會受到太大影響。但是，在使用 LLM 時，您可能會考慮提示中的換行符號是否會影響模型的效能。

### 分隔符號小技巧
`###` 作為分隔符，是一個不錯的選擇，因為它在 tokenizer 中只會被視為一個 token。


---

