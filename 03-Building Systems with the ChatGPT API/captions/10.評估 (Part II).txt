在上一支影片中，你已經看到如何在 LLM 輸出正確答案的情況下進行評估。

因此，我們可以寫一個函式，明確地告訴我們 LLM 是否輸出了正確的分類與產品列表。

但如果 LLM 是用來生成文字，而這些文字並不存在唯一正確的版本呢？我們來看看如何評估這種類型的 LLM 輸出。

這是我平常會用的一些輔助函式。假設有一個客戶訊息：「告訴我關於 smartx pro phone 和 fotosnap 相機的資訊。」等等。這裡有一些工具可以幫我取得助理的回覆。這基本上就是 Isa 在前面影片中說明的流程。

以下是助理的回答：「當然，我很樂意幫忙！」接著列出了智慧型手機、smartx pro phone 等等。

那麼，我們要如何評估這是不是一個好的回答呢？看起來可能有很多種合理的好答案。

一種評估方式是撰寫一份評分標準（rubric），也就是一系列的準則，從不同面向來評估這個回答，然後根據這些準則來決定你是否對這個回答感到滿意。讓我來示範怎麼做。

我會先建立一個小的資料結構來儲存客戶訊息以及產品資訊。

然後我會指定一個 prompt，使用所謂的 rubric 來評估助理的回答。我等一下會說明這是什麼意思。

這個 prompt 在系統訊息中寫道：「你是一位評估客服人員回答品質的助理，透過檢視客服人員產出回應所使用的上下文內容來進行評估。」

這個回應就是我們先前在筆記本中產生的助理回答。我們會在 prompt 中提供資料，包括客戶訊息、上下文（也就是提供的產品與分類資訊），以及 LLM 的輸出。

接下來，這就是 rubric。我們希望 LLM 去「比較提交的答案與上下文的事實內容是否一致。忽略文體、文法或標點的差異。」

我們還希望檢查幾項指標，例如：「助理的回答是否只根據提供的上下文？回答中是否包含上下文中沒有的資訊？回答是否與上下文有任何矛盾？」

這就是所謂的 rubric，它說明了我們認為一個好回答應該具備哪些要素。

最後，我們希望能印出「是」或「否」等等的結論。

現在，如果我們執行這項評估，會得到這樣的結果：

它指出：「助理的回答是僅根據提供的上下文內容。」在這個案例中，它似乎沒有捏造新資訊，也沒有出現矛盾。使用者問了兩個問題，也都有回答。因此兩個問題都被回覆了。

所以我們可以看這個結果，並可能會認為這是一個相當不錯的回答。

補充一點，這裡我使用的是 ChatGPT 3.5 Turbo 模型來進行評估。

如果要更嚴謹地評估，考慮使用 GPT-4 可能會更妥當。即使你在正式環境中使用的是 3.5 Turbo 來產生大量文字，如果評估只是在開發中偶爾進行，那麼付費使用稍貴的 GPT-4 API 以獲得更嚴謹的評估結果，是值得的。

希望你能從這裡學到一個設計模式：當你可以撰寫 rubric，也就是一份評估 LLM 輸出的準則清單，那你就可以使用另一個 API 呼叫來評估第一次 LLM 的輸出。

還有另一個設計模式也可能對某些應用很有幫助，就是如果你可以提供一份理想的回應。這裡我會示範一個測試範例，客戶訊息是：「請告訴我關於 smartx pro phone 的資訊。」等等。

以下是理想答案，也就是由一位專業客服人員撰寫的非常好的回答。這位專家說這會是一個很棒的答案：「當然！SmartX ProPhone 是一款……」

接著給出了許多有幫助的資訊。

當然，我們不能期望任何 LLM 能逐字輸出這個理想答案。

而在傳統的自然語言處理技術中，有一些指標用來評估 LLM 的輸出與人類寫的答案是否相似，例如 BLEU 分數。你可以上網搜尋 BLEU 來了解更多，它可以衡量一段文字與另一段文字的相似程度。但其實還有更好的方式，我們可以使用 prompt，讓 LLM 自行比較自動產生的客服回應與上面提到的理想人類回答的相符程度。

這是我們可以使用的 prompt，我們要讓 LLM 成為一位助理，評估客服人員的回覆是否良好，方法是比較自動產生的回答與理想（由專家撰寫）的回答。

我們會提供這些資料：客戶的請求、專家撰寫的理想答案，以及 LLM 實際產生的回應。

這份 rubric 來自 OpenAI 的開源評估框架，這是一個很棒的框架，其中包含了 OpenAI 開發者與開源社群貢獻的多種評估方法。

事實上，你也可以自行貢獻一份評估方法到這個框架中，幫助其他人評估他們的 LLM 輸出。

在這份 rubric 中，我們告訴 LLM：「比較提交的回答與專家回答的事實內容是否一致。忽略風格、文法或標點的差異。」

你可以暫停影片，詳細閱讀這些內容，但重點是我們要求 LLM 根據比較結果，從 A 到 E 評分，依據下列標準：

- A：提交的回答是專家回答的子集，且完全一致。
- B：提交的回答是專家回答的超集，但完全一致，可能代表它捏造了一些額外資訊。
- C：提交的回答包含與專家回答相同的所有細節。
- D：兩者之間存在矛盾。
- E：兩者回答不同，但這些差異在事實層面並不重要。

LLM 會選出最合適的描述。所以這是我們剛剛的助理回答。我覺得它不錯，但現在我們來看看它與測試集中的理想回答比較後的結果。看起來它拿到了 A，也就是「提交的答案是專家答案的子集，且完全一致」，我覺得這評價是合理的。

這個助理回答比上面的專家回答簡短很多，但它基本上是內容一致的。

再次說明，我這裡用的是 GPT-3.5 Turbo，但如果你想更嚴謹地評估自己應用中的模型，使用 GPT-4 可能會更好。

現在我們來試個完全不同的例子。我讓助理回答：「人生就像一盒巧克力」，這是一句來自電影《阿甘正傳》的台詞。

如果我們評估這段回答，會得到 D，並指出「提交的答案與專家答案之間有矛盾。」

所以它正確地判斷這是一個非常糟糕的回答。

總結一下，我希望你從這支影片中學會了兩個設計模式：

第一，就算沒有專家提供的理想答案，只要你能撰寫一份評分標準（rubric），就能用一個 LLM 來評估另一個 LLM 的輸出。

第二，如果你能提供專家撰寫的理想答案，那麼你可以讓 LLM 更好地判斷某個助理回答是否與該理想答案相似。

希望這對你評估 LLM 系統的輸出有所幫助。

這樣不論是在開發階段還是系統運行中，你都可以持續監控其表現，並利用這些工具持續地進行評估與改進，讓系統表現越來越好。