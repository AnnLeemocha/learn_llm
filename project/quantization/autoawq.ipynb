{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684facfa",
   "metadata": {},
   "source": [
    "### AWQ\n",
    "[Activation-Aware Weight Quantization (AWQ)](https://github.com/mit-han-lab/llm-awq) 也是一種 Post Training Optimization 的量化方法，同樣也是透過少量的校準資料集來進行更精準的量化。\n",
    "* 相較於 GPTQ 而言，作者宣稱 AWQ 更不容易發生校準資料集的過度擬合問題，而且 AWQ 量化的模型在使用時可以有更快的推論速度。\n",
    "    * 作者指出，像是 GPTQ 這種透過**反向傳播 (Backward)** 來進行量化的方法，可能會使模型在校準資料集上發生過度擬合的情形。\n",
    "    * 因此AWQ並不仰賴反向傳播，只透過**前向運算 (Forward)** 來觀察**輸出分佈 (Activation Distribution)** 並藉此尋找**重要的模型權重 (Salient Weights)**。\n",
    "    * 作者指出，只要將其中 1% 的重要權重保留為 FP16 的資料型態，就能大幅降低量化帶來的誤差。\n",
    "* 但如果為了保護這些分散在不同地方的重要權重，而使得整份權重量化完後是混精度的，則會使計算效率降低。\n",
    "    * 因此作者引入 **Activation-Aware Scaling** 的技巧來對每組權重做量化，使得重要權重能夠獲得保護的同時，也能夠避免混精度權重以提昇硬體友善度，降低運算瓶頸，在實際推論上可以有相當快的速度。\n",
    "* 在社群上也有類似 AutoGPTQ 的 [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) 這種方便的工具可用，AWQ 量化的過程是一層一層的量化，所以 AutoAWQ 可以將模型權重本身放在 CPU 裡面，只有在該層需要量化時，才把它的權重拿出來放在 GPU 記憶體裡面，因此記憶體的使用非常有效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26722d",
   "metadata": {},
   "source": [
    "#### 1. 若要使用 AWQ 量化，首先需要安裝 AutoAWQ 套件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8347218",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install autoawq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402513f7",
   "metadata": {},
   "source": [
    "#### 2. 可以使用 `AutoAWQForCausalLM` 類別讀取原始模型：\n",
    "* 這裡將模型放在 CPU 記憶體裡面，避免模型權重佔用太多 GPU 記憶體導致 AutoAWQ 沒有空間進行量化運算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",   # 權重可以放在 CPU 就好\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd04f",
   "metadata": {},
   "source": [
    "#### 3. 接下來會需要準備一份校準資料集\n",
    "* 預設會使用原作者提供的 [`mit-han-lab/pile-val-backup` 這份資料集](https://huggingface.co/datasets/mit-han-lab/pile-val-backup) 進行校準。\n",
    "* 但我們也可以自行準備一份字串列表當作資料集：\n",
    "* 注意：\n",
    "    * 如果給的文本太短太少的話， 進行量化時就會發生 `RuntimeError: torch.cat() expected a non-empty list of Tensors` 的錯誤。\n",
    "    * 這是因為在套件裡面的 `awq/utils/calib_data.py`，作者放了一個 Token 數量必須大於 512 才會拿來用的條件，可能是因為[原本 AWQ 的實做](https://github.com/casper-hansen/AutoAWQ/issues/191)就是如此的關係。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此為示範用的校準資料集，請根據自身應用替換成適當資料集\n",
    "# 只要是 list[str] 的型態且 Token 數量夠多就能量化\n",
    "\n",
    "calib_data = [\"Hello, World! \"  *  8] * 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb996730",
   "metadata": {},
   "source": [
    "#### 4. 接下來放上相關設定，並開始進行量化：\n",
    "* 筆者實測使用 RTX 3090 24 GB 量化一個 7B 模型，大約也是七八分鐘左右，速度上與 GPTQ 是差不多的，而且過程中所使用的 GPU 記憶體非常少量。 \n",
    "* 如果 CPU 記憶體夠大，也能拿來量化 70B 的模型，大概要花費 100 分鐘左右。 \n",
    "* 不過 70B 的模型即便量化到 4-Bit 也放不進一張 24GB GPU 裡面就是了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "tokenizer = TkCls.from_pretrained(model_dir)\n",
    "\n",
    "# 相關設定\n",
    "quant_config = dict(\n",
    "    zero_point=True,\n",
    "    q_group_size=128,\n",
    "    w_bit=4,\n",
    "    version=\"GEMM\",\n",
    ")\n",
    "\n",
    "# 進行量化\n",
    "model.quantize(\n",
    "    tokenizer,\n",
    "    quant_config=quant_config,\n",
    "    calib_data=calib_data,\n",
    ")\n",
    "\n",
    "# 儲存量化結果\n",
    "output_dir = \"Models/Llama3-8B-Inst-AWQ\"\n",
    "model.save_quantized(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d97da",
   "metadata": {},
   "source": [
    "#### 5. 這些量化完的模型，可以直接讀取進來使用：\n",
    "* 不需要任何額外的設定就能讀取，且 AWQ 模型在讀取速度上快滿多的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d319ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM as ModelCls\n",
    "\n",
    "model_dir = \"Models/Llama3-8B-Inst-AWQ\"\n",
    "\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")   # 約 5.8 GiB"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
