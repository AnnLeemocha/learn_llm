{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得模型\n",
    "\n",
    "這邊讀取 `meta-llama/Meta-Llama-3-8B-Instruct` 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 .from_pretrained 讀取： \n",
    "* 使用**量化 (Quantization)** 技術來減少 GPU 記憶體的消耗\n",
    "* 透過 `BitsAndBytesConfig` 可以輕鬆地將模型量化 8 到 4 位元。\n",
    "* 將 `device_map` 設定為 `\"auto\"`，以確保模型權重都放在 GPU 裡面。\n",
    "* 如果 CPU 記憶體也不是很夠的話，那就需要將 `low_cpu_mem_usage` 設定為 `True` 才能順利讀取模型。\n",
    "* 以上步驟實際上會從 [Hugging Face Hub](https://huggingface.co/models) (HF Hub)下載模型的權重，預設會放在 `~/.cache/huggingface/` 裡面，也可以透過`cache_dir` 參數指定這些檔案要存放在哪裡："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# 需要 CUDA 與 GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,    # 約需 8 GiB GPU 記憶體\n",
    "    # load_in_4bit=True,  # 約需 6 GiB GPU 記憶體\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 需要 CUDA 與 GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有存取限制的模型\n",
    "* 先去 [HF Hub](https://huggingface.co/) 註冊帳號\n",
    "* 進入該模型介紹頁面，找到 \"This repository is gated\" 區塊，填寫存取表單，並等待核准通知。\n",
    "* 前往 HF 帳號設定的 Access Tokens 頁面，並建立一個新的 Token。\n",
    "    * 如果只要下載模型的話，權限只要設定 Read 就好。\n",
    "    * 如果你想讓 Git 也記得這個 token，避免每次 push 都輸入密碼，可以執行：\n",
    "        ```\n",
    "        git config --global credential.helper store\n",
    "        ```\n",
    "        這會讓 Git 把憑證儲存在本地 .git-credentials 中（純文字，請小心安全性）。\n",
    "        * 詳細說明：https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage\n",
    "* 使用建立好的 Access Token 登入後下載模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(方式一) 設定 SSH 金鑰，然後透過 SSH 下載："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone git@hf.co:meta-llama/Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(方式二) 透過 `huggingface-cli` 登入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "huggingface-cli login\n",
    "# 貼上 建立好的 Access Token\n",
    "\n",
    "# 預設會放在 `~/.cache/huggingface/` 裡面，也可以透過 `--local-dir` 參數指定這些檔案要存放在哪裡\n",
    "huggingface-cli download google/gemma-2b-it \\\n",
    "    --local-dir Models/Meta-Llama-3-8B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
