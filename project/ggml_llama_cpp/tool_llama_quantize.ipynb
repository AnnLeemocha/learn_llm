{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef82b2f2",
   "metadata": {},
   "source": [
    "### 量化工具 llama-quantize\n",
    "量化是 llama.cpp 最受歡迎的功能，llama.cpp 支援的量化方法相當廣泛，從 8-Bit、6-Bit 甚至到 2-Bit、1-Bit 都有，同時也支援混精度、線性、非線性等量化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193138da",
   "metadata": {},
   "source": [
    "#### 1. 基本的量化方法只要使用 `llama-quantize` 這個工具便可以進行操作：\n",
    "* 以下指令會將模型量化為 4-Bit 格式，這裡的 `Q4_K_M` 代表「以 `Q4_K` 為基本型態的 Medium 大小」，意思是大部分的模型權重都是 4-Bit 的 `Q4_K` 型態，但是有一部分的權重會用 6-Bit 的 `Q6_K` 型態，因此最後的**平均權重位元 (Bits Per Weight, BPW)** 其實會將近 5.0。\n",
    "    * 另外也有 `Q4_K_S` 是混合 `Q4K` 與 `Q5K` 的格式，其 BPW 會更貼近 4.0 一些。\n",
    "* 透過 `./build/bin/llama-quantize --help` 可以看到，還有許多 `IQ` 開頭的量化方法。\n",
    "    * 例如極低位元數的 `IQ2_XXS` 與 `IQ1_S` 等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12da1d5",
   "metadata": {},
   "source": [
    "參數說明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59682241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-quantize --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb78086",
   "metadata": {},
   "source": [
    "將模型量化為 4-Bit 格式 (`Q4_K_M`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb260862",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-quantize \\\n",
    "    ./llama-3-8b-inst.gguf \\\n",
    "    ./llama-3-8b-inst.Q4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c01fe5",
   "metadata": {},
   "source": [
    "#### 2. 透過另外一個程式 `llama-imatrix` 的協助來使用這些量化方法。\n",
    "* `llama-imatrix` 指的是**重要性矩陣 (Importance Matrix)** ，這個程式會透過一份校準資料集來評估哪些模型權重是重要的，這些重要的權重需要保護起來，才能在極低位元的量化下維持模型的表現。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072253cd",
   "metadata": {},
   "source": [
    "安裝套件\n",
    "* 這個模組是 Hugging Face 的 datasets 庫，常用於載入和處理各種資料集，特別是在 NLP 任務中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2614bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (2.0.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading aiohttp-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━��━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━0m\u001b[\u001b[0m\u001b[━━━━\u001b[0m \u001b[32m0.0/42.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/42.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/42.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/42.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/42.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/42.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/42.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m34.3/42.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m39.1/42.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m42.2/42.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pyarrow]━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/17\u001b[0m [dill]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [pandas]━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [aiohttp]━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m16/17\u001b[0m [datasets]━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.2 aiosignal-1.3.2 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 multidict-6.4.4 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1b8a8",
   "metadata": {},
   "source": [
    "需要先蒐集一份純文字資料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8422ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_path = \"bigscience-data/roots_zh-tw_wikipedia\"\n",
    "ds = load_dataset(ds_path, split=\"train\", streaming=True)\n",
    "text = [item[\"text\"] for _, item in zip(range(128), ds)]\n",
    "text = \"\\n\".join(text)\n",
    "with open(\"zh-wiki.txt\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367d7df",
   "metadata": {},
   "source": [
    "接著丟入 `llama-imatrix` 裡面：\n",
    "* 大約兩三分鐘的時間就能跑完，並產生一份 `imatrix.dat` 檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aee2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 1 (a3c3084) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from llama-3-8b-inst.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.96 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors:   CPU_Mapped model buffer size = 15317.02 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context:        CPU compute buffer size =   560.01 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192\n",
      "\n",
      "system_info: n_threads = 8 (n_threads_batch = 8) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "compute_imatrix: tokenizing the input ..\n",
      "compute_imatrix: tokenization took 141.648 ms\n",
      "compute_imatrix: computing over 20 chunks with batch_size 2048\n",
      "compute_imatrix: 332.24 seconds per pass - ETA "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hours 50.73 minutes\n",
      "[1]15.2444,[2]12.2086,[3]11.5078,[4]11.8649,[5]13.2535,[6]14.3845,[7]14.3393,[8]13.8441,[9]13.3747,[10]13.6807,"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-imatrix \\\n",
    "    -ngl 99 -c 8192 -f ../zh-wiki.txt \\\n",
    "    -m llama-3-8b-inst.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c08e2",
   "metadata": {},
   "source": [
    "#### 3. 將這份檔案用在量化工具上，便能產生極低位元的量化模型了：\n",
    "* 以 Breeze-7B 實測，原本 FP16 為 14 GB，量化成 `Q4_KM` 只剩下 4.3 GB，到了 `IQ2_XXS` 只剩下 2 GB，最極端的 `IQ1_S` 更是只剩下 1.7 GB！但 7B 的模型量化到這個程度，其實已經接近無法使用的狀態了。\n",
    "* 根據筆者的經驗，8-Bit 和 6-Bit 幾乎不會有損失的感覺，到了 4-Bit 以下才會有明顯感受到不同，而參數量越大的機型，受到量化的影響則越小。\n",
    "    * 不過這是個人的主觀感受，最後選擇使用多少位元的量化，還是要根據實際應用的狀況進行過充分的評測後再來決定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e2573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 1 (a3c3084)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'llama-3-8b-inst.gguf' to 'llama-3-8b-inst.IQ2_XXS.gguf' as IQ2_XXS\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from llama-3-8b-inst.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 5f0b02c75b57c5855da9ae460ce51323ea669d8a\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "================================ Have weights data with 224 entries\n",
      "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, \n",
      "====== llama_model_quantize_impl: did not find weights for output.weight\n",
      "converting to q5_K .. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_imatrix: imatrix dataset='../zh-wiki.txt'\n",
      "load_imatrix: loaded 224 importance matrix entries from imatrix.dat computed on 320 chunks\n",
      "prepare_imatrix: have 224 importance matrix entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size =  1002.00 MiB ->   344.44 MiB\n",
      "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, \n",
      "====== llama_model_quantize_impl: did not find weights for token_embd.weight\n",
      "converting to q2_K .. size =  1002.00 MiB ->   164.39 MiB\n",
      "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
      "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
      "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
      "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q2_K .. size =   112.00 MiB ->    18.38 MiB\n",
      "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to iq2_xxs .. size =     8.00 MiB ->     1.03 MiB\n",
      "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =    32.00 MiB ->     4.12 MiB\n",
      "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to iq2_xxs .. size =   112.00 MiB ->    14.44 MiB\n",
      "llama_model_quantize_impl: model size  = 15317.02 MB\n",
      "llama_model_quantize_impl: quant size  =  2280.59 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 115021.70 ms\n",
      "main:    total time = 115021.70 ms\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd llama.cpp\n",
    "\n",
    "./build/bin/llama-quantize \\\n",
    "    --imatrix imatrix.dat \\\n",
    "    llama-3-8b-inst.gguf \\\n",
    "    llama-3-8b-inst.IQ2_XXS.gguf IQ2_XXS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
