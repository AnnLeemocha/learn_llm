{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取模型\n",
    "\n",
    "這邊讀取 Llama-2 7B Chat 模型\n",
    "* 因為原本的 Llama-2 需要權限並設定 SSH 公鑰才能直接下載，因此這邊使用 TheBloke 大神上傳的 FP16 版本做示範\n",
    "    * [TheBloke](https://huggingface.co/TheBloke) 是 HuggingFace 社群上非常活躍的用戶，經常幫各個模型做 Quantization 並重新上傳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 .from_pretrained 讀取： \n",
    "* *TheBloke 大神提供的 [Llama-2 7B](https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16) Chat 為例：\n",
    "    * 使用**量化 (Quantization)** 技術來減少 GPU 記憶體的消耗\n",
    "    * 透過 `BitsAndBytesConfig` 可以輕鬆地將模型量化 8 到 4 位元。\n",
    "    * 將 `device_map` 設定為 `\"auto\"`，以確保模型權重都放在 GPU 裡面。\n",
    "    * 如果 CPU 記憶體也不是很夠的話，那就需要將 `low_cpu_mem_usage` 設定為 `True` 才能順利讀取模型。\n",
    "* 以上步驟實際上會從 [Hugging Face Hub](https://huggingface.co/models) (HF Hub)下載模型的權重，預設會放在 `~/.cache/huggingface/` 裡面，也可以透過`cache_dir` 參數指定這些檔案要存放在哪裡："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 2 files: 100%|██████████| 2/2 [03:33<00:00, 106.55s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "\n",
    "# 需要 CUDA 與 GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,    # 約需 8 GiB GPU 記憶體\n",
    "    # load_in_4bit=True,  # 約需 6 GiB GPU 記憶體\n",
    ")\n",
    "\n",
    "# 需要 CUDA 與 GPU\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# # 無 GPU 指定用 CPU\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "\n",
    "#     # device_map=\"auto\", 在無 GPU 環境中沒意義。\n",
    "#     device_map={\"\": \"cpu\"},\n",
    "\n",
    "#     # 這是 Hugging Face 為大型模型設計的「省 RAM 載入技巧」，需要 PyTorch 2.0+ 和 GPU。\n",
    "#     # 在無 GPU 時常會導致奇怪的錯誤或佔用更多 CPU 資源。\n",
    "#     # low_cpu_mem_usage=True,\n",
    "\n",
    "#     # 完全不能用，因為 bnb_config 是屬於 bitsandbytes 的，該套件需要 GPU + CUDA。\n",
    "#     # quantization_config=bnb_config,\n",
    "# )\n",
    "\n",
    "# # 指定下載的模型權重檔案會放在哪\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     cache_dir=\"./cache_models\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看本地模型的 commit id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4594bf11fd5c66a9021d20f5ae5f3a246ce388a1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ~/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-chat-fp16/snapshots/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看目前的 generation_config\n",
    "* 如果出現 `do_sample` 的警告訊息，那是因為預設的 `do_sample` 為 `False`，與模型內建的 `generation_config.json` 有衝突設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generation_config.json`\n",
    "* 位置通常在：\n",
    "    * `~/.cache/huggingface/hub/models--<model-name>/snapshots/<commit-id>/generation_config.json`\n",
    "* 可以使用 `find ~/.cache/huggingface/hub -name generation_config.json` 尋找，並進行修改。\n",
    "    * 加上 `\"do_sample\": true,`，或是刪除 `temperature` 與 `top_p` 等參數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai-x/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-chat-fp16/snapshots/4594bf11fd5c66a9021d20f5ae5f3a246ce388a1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find ~/.cache/huggingface/hub -name generation_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看安裝位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.cache/huggingface:\n",
      "total 24\n",
      "drwxrwxr-x  4 ai-x docker 4096  四   7 17:14 .\n",
      "drwx------ 20 ai-x ai-x   4096  五  15 16:58 ..\n",
      "drwxrwxr-x  2 ai-x ai-x   4096  四   7 17:14 accelerate\n",
      "drwxrwxr-x  9 ai-x docker 4096  五  16 09:13 hub\n",
      "-rw-rw-r--  1 ai-x ai-x     68  四   9 09:58 stored_tokens\n",
      "-rw-rw-r--  1 ai-x ai-x     37  四   9 09:58 token\n",
      "~/.cache/huggingface/hub:\n",
      "total 40\n",
      "drwxrwxr-x 9 ai-x docker 4096  五  16 09:13 .\n",
      "drwxrwxr-x 4 ai-x docker 4096  四   7 17:14 ..\n",
      "drwxrwxr-x 9 ai-x docker 4096  五  16 09:13 .locks\n",
      "drwxrwxr-x 6 ai-x docker 4096  三  31 18:13 models--BAAI--bge-reranker-v2-m3\n",
      "drwxrwxr-x 6 ai-x ai-x   4096  五  15 11:59 models--intfloat--multilingual-e5-base\n",
      "drwxrwxr-x 6 ai-x docker 4096  三  31 18:09 models--sentence-transformers--all-MiniLM-L6-v2\n",
      "drwxrwxr-x 6 ai-x ai-x   4096  四  30 11:39 models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2\n",
      "drwxrwxr-x 6 ai-x ai-x   4096  五  16 09:16 models--TheBloke--Llama-2-7b-chat-fp16\n",
      "drwxrwxr-x 3 ai-x ai-x   4096  四   1 12:07 models--xai-org--grok-1\n",
      "-rw-rw-r-- 1 ai-x ai-x      1  四   8 14:34 version.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"~/.cache/huggingface:\"\n",
    "ls -al ~/.cache/huggingface\n",
    "\n",
    "echo \"~/.cache/huggingface/hub:\"\n",
    "ls -al ~/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 Git 取得 [Hugging Face Hub](https://huggingface.co/models) (HF Hub) 中的模型的權重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 安裝 Git LFS 模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt install -y git git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 啟動 Git LFS 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 下載模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 直接複製\n",
    "git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16\n",
    "\n",
    "# # 如果想看下載的網速圖紙紙條\n",
    "# # 1. 跳過所有 LFS 檔案的下載\n",
    "# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/TheBloke/Llama-2-7b-chat-fp16\n",
    "# cd Llama-2-7b-chat-fp16\n",
    "# # 2. 透過 Git LFS 進行 Fetch\n",
    "# git lfs fetch\n",
    "# git lfs checkout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 讀取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-chat-fp16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe52fbd9d2cd4b8db9f2bd6efd749c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "# 直接指定資料夾路徑名稱\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"TheBloke/Llama-2-7b-chat-fp16\",\n",
    "    cache_dir=\"./cache_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有存取限制的模型\n",
    "* 先去 [HF Hub](https://huggingface.co/) 註冊帳號\n",
    "* 進入該模型介紹頁面，找到 \"This repository is gated\" 區塊，填寫存取表單，並等待核准通知。\n",
    "* 前往 HF 帳號設定的 Access Tokens 頁面，並建立一個新的 Token。\n",
    "    * 如果只要下載模型的話，權限只要設定 Read 就好。\n",
    "    * 如果你想讓 Git 也記得這個 token，避免每次 push 都輸入密碼，可以執行：\n",
    "        ```\n",
    "        git config --global credential.helper store\n",
    "        ```\n",
    "        這會讓 Git 把憑證儲存在本地 .git-credentials 中（純文字，請小心安全性）。\n",
    "        * 詳細說明：https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage\n",
    "* 使用建立好的 Access Token 登入後下載模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(方式一) 設定 SSH 金鑰，然後透過 SSH 下載："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone git@hf.co:meta-llama/Meta-Llama-3-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(方式二) 透過 `huggingface-cli` 登入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "huggingface-cli login\n",
    "# 貼上 建立好的 Access Token\n",
    "\n",
    "# 預設會放在 `~/.cache/huggingface/` 裡面，也可以透過 `--local-dir` 參數指定這些檔案要存放在哪裡\n",
    "huggingface-cli download google/gemma-2b-it \\\n",
    "    --local-dir Models/Meta-Llama-3-8B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
