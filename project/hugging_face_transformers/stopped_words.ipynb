{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定停止點\n",
    "\n",
    "停止點是用來告訴系統在生成的過程中，除了遇到 EOS (End-of-Sentence) Token 以外，還有遇到哪些 Token 應該停止輸出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最基本的方法是設定 `GenerationConfig` 的 `eos_token_id`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>[INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "from transformers import TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelCls\n",
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "model_path = \"google/gemma-2b-it\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_path)\n",
    "\n",
    "prompt = \"[INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] \"\n",
    "\n",
    "inputs = tk(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ts = TextStreamer(tk)\n",
    "\n",
    "config = GenerationConfig(\n",
    "    eos_token_id=[\n",
    "        tk.eos_token_id,       # 留著原本的 EOS\n",
    "        tk.encode(\".\")[-1],    # 遇到句點停下\n",
    "        tk.encode(\"\\n\")[-1],   # 遇到換行停下\n",
    "        tk.encode(\"\\n\\n\")[-1], # 遇到雙換行停下\n",
    "    ],\n",
    ")\n",
    "\n",
    "output = model.generate(**inputs, generation_config=config, streamer=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自己實作 StoppingCriteria 類別：\n",
    "* 接著將 `StopWords` 物件放進一個 `StoppingCriteriaList` 裡面，然後傳入 `model.generate` 裡面即可：\n",
    "    * 每次系統檢查的時候，將整份輸出 Decode 回純文字，並且檢查文字的結尾是否符合使用者設定的停止點。\n",
    "    * 此範例為單一輸入進行文本生成的情況，批次推論時情況會複雜許多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.StopWords object at 0x771414223750>]\n",
      "<bos>[INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] \n",
      "\n",
      "大型語言模型（LLM）是一種 AI 模型，它能夠像人類一樣使用語言。\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopWords(StoppingCriteria):\n",
    "    def __init__(self, tk: TkCls, stop_words: list[str]):\n",
    "        self.tk = tk\n",
    "        self.stop_tokens = stop_words\n",
    "\n",
    "    def __call__(self, input_ids, *_) -> bool:\n",
    "        s = self.tk.batch_decode(input_ids)[0]\n",
    "        for t in self.stop_tokens:\n",
    "            if s.endswith(t):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "sw = StopWords(tk, [\"。\", \"！\", \"？\"])\n",
    "scl = StoppingCriteriaList([sw])\n",
    "print(scl)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    streamer=TextStreamer(tk),\n",
    "    stopping_criteria=scl,\n",
    ")\n",
    "# 模型在輸出遇到 \"。！？\" 時就會停下來了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers 在 `GenerationConfig` 中新增了 `stop_strings` 的參數：\n",
    "* 可以更輕鬆的設定停止點："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i'm looking for a way to make my website more accessible to people with disabilities.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "config = GenerationConfig(\n",
    "    stop_strings=[\".\", \"\\n\"],\n",
    ")\n",
    "\n",
    "inputs = tk([\"hello,\", \"goodbye, \"], padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "output = model.generate(**inputs, generation_config=config, tokenizer=tk)\n",
    "\n",
    "print(tk.decode(output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
