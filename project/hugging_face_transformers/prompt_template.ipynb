{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提示樣板\n",
    "\n",
    "* 提示樣本其實就是把使用者與模型之間一問一答的過程，寫成一個像是**聊天紀錄的格式**，用來「欺騙」語言模型生成下個聊天片段。\n",
    "* 這個樣板會根據每個語言模型訓練的方式而有所不同，一般而言遵守模型開發者的建議會比較好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama2 提示樣板範例\n",
    "* [Llama2 的官方樣板](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3#64b71f7588b86014d7e2dd71)：\n",
    "    ```\n",
    "    [INST] <使用者訊息> [/INST] <模型的回覆從這開始>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great! Here's my answer in Traditional Chinese:\n",
      "\n",
      "大型語言模型 (大型语言模型) refers to a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. These models are designed to mimic the language abilities of humans, such as understanding context, using grammar and syntax, and producing text that is grammatically correct and contextually appropriate.\n",
      "大型語言模型 can be used for a variety of applications, such as:\n",
      "1. Language Translation\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelCls\n",
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_path)\n",
    "\n",
    "prompt = \"[INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] \"\n",
    "\n",
    "inputs = tk(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    streamer=TextStreamer(tk),\n",
    ")\n",
    "# Output: <s> [INST] 使用繁體中文回答，請問什麼是大型語言模型？ [/INST] 大型語言模型（Large Language Model，LLM）是一種 ...</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接調用行開發者設定的模板： (樣板隨模型而變化)\n",
    "* 使用 Transformer 套件內建的**聊天樣板 (Chat Template)** 功能，只要呼叫分詞器的 `apply_chat_template` 方法即可。\n",
    "* Google 的 Gemma 提示樣板範例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. 呼叫分詞器的 `apply_chat_template` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "使用繁體中文回答，請問什麼是大型語言模型？<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tk = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "query = \" 使用繁體中文回答，請問什麼是大型語言模型？ \"\n",
    "conversation = [{\"role\": \"user\", \"content\": query}]\n",
    "prompt = tk.apply_chat_template(conversation, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 附加上可以把模型開始回答前的格式，讓模型比較不容易生成失敗。(透過 `add_generation_prompt` 參數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "使用繁體中文回答，請問什麼是大型語言模型？<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "# tk = AutoTokenizer.from_pretrained(model_path)\n",
    "# query = \" 使用繁體中文回答，請問什麼是大型語言模型？ \"\n",
    "# conversation = [{\"role\": \"user\", \"content\": query}]\n",
    "prompt = tk.apply_chat_template(\n",
    "    conversation, \n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 格式確認完畢後，來取得模型的輸入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    106,   1645,    108,   7060, 238249, 236969,  50039,  33226,\n",
      "         235365, 160287,  24437, 235427,  66240, 144955,  43936, 235544,    107,\n",
      "            108,    106,   2516,    108]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = tk.apply_chat_template(\n",
    "    conversation, \n",
    "    # tokenize=False,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    ").to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Output: tensor([[2, 106, ..., 2516, 108]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 輸入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "使用繁體中文回答，請問什麼是大型語言模型？<end_of_turn>\n",
      "<start_of_turn>model\n",
      "大型語言模型（LLM）是一種 AI 模型，它具有超過 1 公億個字的語言資料，並能夠根據這些資料進行語言處理。這使得它能夠完成各種任務，包括：\n",
      "\n",
      "* 翻譯文字\n",
      "* 產生文字\n",
      "* 識別語言\n",
      "* 進行語言推理\n",
      "* 識別情緒\n",
      "\n",
      "LLM 的出現改變了 AI 的發展方向，因為它們可以被用來解決各種問題，包括：\n",
      "\n",
      "* 減少語言溝通中的歧視\n",
      "* 提高語言模型的解釋性\n",
      "* 建立更可靠的語言模型\n",
      "\n",
      "目前，LLM 是各種語言模型的基礎，包括：\n",
      "\n",
      "* **BERT**\n",
      "* **GPT**\n",
      "* **XLNet**\n",
      "* **T5**\n",
      "\n",
      "LLM 的發展是一個不斷的過程，隨著新的模型被開發出來，它們變得越來越精確和高效。<eos>\n"
     ]
    }
   ],
   "source": [
    "# output = model.generate(**inputs) # 原本\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    streamer=TextStreamer(tk),\n",
    ")  # 現在"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
