{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªå‹•é¡åˆ¥ \n",
    "\n",
    "æœ‰äº›æ¨¡å‹èˆ‡ Tokenizer ç¬¬ä¸€æ™‚é–“å¯èƒ½ç„¡æ³•åˆ¤åˆ¥çš„ä½¿ç”¨ä»€éº¼æ¶æ§‹ï¼Œæˆ–æ˜¯æƒ³å¯«å‡ºç›¸å®¹æ€§æ›´å»£æ³›çš„æ–¹æ¡ˆç¢¼ï¼Œé‚£å°±å¯ä»¥ä½¿ç”¨ Transformers çš„ Auto Class ä¾†é€²è¡Œè®€å–ã€‚\n",
    "\n",
    "* æ¯”è¼ƒå·®ç•°\n",
    "\n",
    "| é …ç›®       | `LlamaTokenizer` | `LlamaTokenizerFast`                   |\n",
    "| -------- | ---------------- | -------------------------------------- |\n",
    "| ğŸ“¦ å¯¦ä½œæ–¹å¼  | Python ç´”å¯¦ä½œ       | Rust èƒŒå¾Œå¯¦ä½œï¼ˆé€é `tokenizers` å¥—ä»¶ï¼‰          |\n",
    "| âš¡ æ•ˆèƒ½     | æ…¢ï¼ˆå°¤å…¶è™•ç†å¤§é‡è³‡æ–™æ™‚ï¼‰     | å¿«å¾ˆå¤šï¼ˆæ•¸å€æ–¼æ…¢é€Ÿç‰ˆæœ¬ï¼‰                           |\n",
    "| âœ… åŠŸèƒ½å®Œæ•´æ€§  | å…¨åŠŸèƒ½ï¼Œæ˜“é™¤éŒ¯          | å¹¾ä¹å®Œå…¨ç›¸å®¹ï¼Œé€Ÿåº¦å¿«                             |\n",
    "| ğŸ§© æ‰€éœ€ä¾è³´  | `transformers`   | `transformers` + `tokenizers`ï¼ˆRust ç·¨è­¯ï¼‰ |\n",
    "| ğŸ“‰ è¨˜æ†¶é«”æ•ˆç‡ | é€šå¸¸ç•¥é«˜             | æ›´å„ªï¼Œæ•ˆèƒ½æ›´ä½³                                |\n",
    "| â— é©ç”¨æƒ…å¢ƒ   | é™¤éŒ¯ã€è¼•é‡ä»»å‹™ã€å°å‹æ‰¹æ¬¡     | æ¨è«–éƒ¨ç½²ã€è¨“ç·´å¤§é‡è³‡æ–™                            |\n",
    "* é«˜é€Ÿæ¨è«–æˆ–è¨“ç·´ --> `LlamaTokenizerFast`\n",
    "* é™¤éŒ¯ç‰¹æ®Š tokenizer è¡Œç‚º --> `LlamaTokenizer`ï¼ˆæ…¢é€Ÿï¼‰\n",
    "* åªæƒ³ã€Œå®ƒèƒ½è·‘ã€ --> `AutoTokenizer`ï¼ˆè‡ªå‹•æŒ‘å¿«çš„ï¼‰ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. åˆ¤åˆ¥çš„ä½¿ç”¨æ¶æ§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.07s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelCls\n",
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_path)\n",
    "\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "print(type(model))  # <class 'transformers...LlamaForCausalLM'>\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä¸æƒ³ç”¨å¿«é€Ÿç‰ˆæœ¬çš„ Tokenizer å¯ä»¥æŠŠ use_fast åƒæ•¸è¨­å®šæˆ Falseï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "tk: TkCls = TkCls.from_pretrained(model_path, use_fast=False)\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizer'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å¤§éƒ¨åˆ†çš„æ–¹æ³•éƒ½ä¾†è‡ª `PreTrainedModel` èˆ‡ `PreTrainedTokenizer` é€™å…©å€‹å‹åˆ¥ï¼Œå› æ­¤å¯ä»¥çµ¦å‡ºå‹åˆ¥è¨»è¨˜ï¼š(åˆ¥å)\n",
    "    * è‡ªå‹•å®Œæˆå›ä¾†äº†ã€‚\n",
    "    * æœ€æ–°ç‰ˆçš„ Transformers å¥—ä»¶ä¸­ï¼Œ`AutoTokenizer.from_pretrained()` å·²ç¶“æœƒè‡ªå‹•æŠŠå›å‚³ç‰©ä»¶æ¨™è¨˜ç‚º `PreTrainedTokenizer` å‹åˆ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.07s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelImp\n",
    "from transformers import AutoTokenizer as TkImp\n",
    "from transformers import PreTrainedModel as ModelCls\n",
    "from transformers import PreTrainedTokenizer as TkCls\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "model: ModelCls = ModelImp.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkImp.from_pretrained(model_path)\n",
    "\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "print(type(model))  # <class 'transformers...LlamaForCausalLM'>\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
