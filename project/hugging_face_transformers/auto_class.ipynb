{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動類別 \n",
    "\n",
    "有些模型與 Tokenizer 第一時間可能無法判別的使用什麼架構，或是想寫出相容性更廣泛的方案碼，那就可以使用 Transformers 的 Auto Class 來進行讀取。\n",
    "\n",
    "* 比較差異\n",
    "\n",
    "| 項目       | `LlamaTokenizer` | `LlamaTokenizerFast`                   |\n",
    "| -------- | ---------------- | -------------------------------------- |\n",
    "| 📦 實作方式  | Python 純實作       | Rust 背後實作（透過 `tokenizers` 套件）          |\n",
    "| ⚡ 效能     | 慢（尤其處理大量資料時）     | 快很多（數倍於慢速版本）                           |\n",
    "| ✅ 功能完整性  | 全功能，易除錯          | 幾乎完全相容，速度快                             |\n",
    "| 🧩 所需依賴  | `transformers`   | `transformers` + `tokenizers`（Rust 編譯） |\n",
    "| 📉 記憶體效率 | 通常略高             | 更優，效能更佳                                |\n",
    "| ❗ 適用情境   | 除錯、輕量任務、小型批次     | 推論部署、訓練大量資料                            |\n",
    "* 高速推論或訓練 --> `LlamaTokenizerFast`\n",
    "* 除錯特殊 tokenizer 行為 --> `LlamaTokenizer`（慢速）\n",
    "* 只想「它能跑」 --> `AutoTokenizer`（自動挑快的） \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 判別的使用架構："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelCls\n",
    "from transformers import AutoTokenizer as TkCls\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "model: ModelCls = ModelCls.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkCls.from_pretrained(model_path)\n",
    "\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "print(type(model))  # <class 'transformers...LlamaForCausalLM'>\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不想用快速版本的 Tokenizer 可以把 use_fast 參數設定成 False："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "tk: TkCls = TkCls.from_pretrained(model_path, use_fast=False)\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizer'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 大部分的方法都來自 `PreTrainedModel` 與 `PreTrainedTokenizer` 這兩個型別，因此可以給出型別註記：(別名)\n",
    "    * 自動完成回來了。\n",
    "    * 最新版的 Transformers 套件中，`AutoTokenizer.from_pretrained()` 已經會自動把回傳物件標記為 `PreTrainedTokenizer` 型別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ai-x/miniconda3/envs/ann_py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM as ModelImp\n",
    "from transformers import AutoTokenizer as TkImp\n",
    "from transformers import PreTrainedModel as ModelCls\n",
    "from transformers import PreTrainedTokenizer as TkCls\n",
    "\n",
    "model_path = \"TheBloke/Llama-2-7b-chat-fp16\"\n",
    "model: ModelCls = ModelImp.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    ")\n",
    "tk: TkCls = TkImp.from_pretrained(model_path)\n",
    "\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "print(type(model))  # <class 'transformers...LlamaForCausalLM'>\n",
    "print(type(tk))     # <class 'transformers...LlamaTokenizerFast'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
